{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Text, Union\n",
    "import copy\n",
    "import math\n",
    "from qlib.utils import get_or_create_path\n",
    "from qlib.log import get_module_logger\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from qlib.model.base import Model\n",
    "from qlib.data.dataset import DatasetH\n",
    "from qlib.data.dataset.handler import DataHandlerLP\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [T, N, F]\n",
    "        return x + self.pe[: x.size(0), :]\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_feat=6, d_model=8, nhead=4, num_layers=2, dropout=0.5, device=None):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.feature_layer = nn.Linear(d_feat, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.decoder_layer = nn.Linear(d_model, 1)\n",
    "        self.device = device\n",
    "        self.d_feat = d_feat\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src [N, F*T] --> [N, T, F]\n",
    "        src = src.reshape(len(src), self.d_feat, -1).permute(0, 2, 1)\n",
    "        src = self.feature_layer(src)\n",
    "\n",
    "        # src [N, T, F] --> [T, N, F], [60, 512, 8]\n",
    "        src = src.transpose(1, 0)  # not batch first\n",
    "\n",
    "        mask = None\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, mask)  # [60, 512, 8]\n",
    "\n",
    "        # [T, N, F] --> [N, T*F]\n",
    "        output = self.decoder_layer(output.transpose(1, 0)[:, -1, :])  # [512, 1]\n",
    "\n",
    "        return output.squeeze()\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=d_feat,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.d_feat = d_feat\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [N, F*T]\n",
    "        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]\n",
    "        x = x.permute(0, 2, 1)  # [N, T, F]\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.fc_out(out[:, -1, :]).squeeze()\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=d_feat,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.d_feat = d_feat\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [N, F*T]\n",
    "        x = x.reshape(len(x), self.d_feat, -1)  # [N, F, T]\n",
    "        x = x.permute(0, 2, 1)  # [N, T, F]\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.fc_out(out[:, -1, :]).squeeze()\n",
    "class ALSTMModel(nn.Module):\n",
    "    def __init__(self, d_feat=6, hidden_size=64, num_layers=2, dropout=0.0, rnn_type=\"GRU\"):\n",
    "        super().__init__()\n",
    "        self.hid_size = hidden_size\n",
    "        self.input_size = d_feat\n",
    "        self.dropout = dropout\n",
    "        self.rnn_type = rnn_type\n",
    "        self.rnn_layer = num_layers\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        try:\n",
    "            klass = getattr(nn, self.rnn_type.upper())\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"unknown rnn_type `%s`\" % self.rnn_type) from e\n",
    "        self.net = nn.Sequential()\n",
    "        self.net.add_module(\"fc_in\", nn.Linear(in_features=self.input_size, out_features=self.hid_size))\n",
    "        self.net.add_module(\"act\", nn.Tanh())\n",
    "        self.rnn = klass(\n",
    "            input_size=self.hid_size,\n",
    "            hidden_size=self.hid_size,\n",
    "            num_layers=self.rnn_layer,\n",
    "            batch_first=True,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "        self.fc_out = nn.Linear(in_features=self.hid_size * 2, out_features=1)\n",
    "        self.att_net = nn.Sequential()\n",
    "        self.att_net.add_module(\n",
    "            \"att_fc_in\",\n",
    "            nn.Linear(in_features=self.hid_size, out_features=int(self.hid_size / 2)),\n",
    "        )\n",
    "        self.att_net.add_module(\"att_dropout\", torch.nn.Dropout(self.dropout))\n",
    "        self.att_net.add_module(\"att_act\", nn.Tanh())\n",
    "        self.att_net.add_module(\n",
    "            \"att_fc_out\",\n",
    "            nn.Linear(in_features=int(self.hid_size / 2), out_features=1, bias=False),\n",
    "        )\n",
    "        self.att_net.add_module(\"att_softmax\", nn.Softmax(dim=1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: [batch_size, input_size*input_day]\n",
    "        inputs = inputs.view(len(inputs), self.input_size, -1)\n",
    "        inputs = inputs.permute(0, 2, 1)  # [batch, input_size, seq_len] -> [batch, seq_len, input_size]\n",
    "        rnn_out, _ = self.rnn(self.net(inputs))  # [batch, seq_len, num_directions * hidden_size]\n",
    "        attention_score = self.att_net(rnn_out)  # [batch, seq_len, 1]\n",
    "        out_att = torch.mul(rnn_out, attention_score)\n",
    "        out_att = torch.sum(out_att, dim=1)\n",
    "        out = self.fc_out(\n",
    "            torch.cat((rnn_out[:, -1, :], out_att), dim=1)\n",
    "        )  # [batch, seq_len, num_directions * hidden_size] -> [batch, 1]\n",
    "        return out[..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "class Trader:\n",
    "    def __init__(self,factors,d_feat,batch_size,lr=0.001,optimizer=\"adam\"):\n",
    "        import random\n",
    "        self.inputfactorsnums=random.sample(range(len(factors)), 6)#factors=['KLEN', 'KLOW', 'ROC60', 'STD5', 'RSQR5', 'RSQR10', 'RSQR20', 'RSQR60', 'RESI5', 'RESI10', 'CORR5', 'CORR10', 'CORR20', 'CORR60', 'CORD5','CORD10', 'CORD60', 'VSTD5', 'WVMA5', 'WVMA60']\n",
    "        self.inputfactors=[]\n",
    "        for i in self.inputfactorsnums:\n",
    "            self.inputfactors.append(factors[i])\n",
    "        self.nmodel= random.randint(0, 3)\n",
    "        self.train_loss=0\n",
    "        self.stop_steps=0\n",
    "        self.train_score = 0\n",
    "        self.best_score=-np.inf\n",
    "        self.train_loss=0\n",
    "        self.train_score =-np.inf\n",
    "        self.val_loss=0\n",
    "        self.val_score=-np.inf\n",
    "        self.predicted_data = np.array([])\n",
    "        self.actual_data=np.array([])\n",
    "        self.overall_accuracy=-1\n",
    "        self.lr=lr\n",
    "        self.perstockacc= np.array([])\n",
    "        if self.nmodel==0:\n",
    "            self.Model =Transformer(d_feat=d_feat,num_layers=2)\n",
    "        elif self.nmodel == 1:\n",
    "            self.Model = ALSTMModel(d_feat,num_layers =  2)\n",
    "        elif self.nmodel== 2:\n",
    "            self.Model =GRUModel(d_feat =d_feat, num_layers = 2)\n",
    "        elif self.nmodel == 3:\n",
    "            self.Model = LSTMModel(d_feat =d_feat, num_layers = 2)\n",
    "        if optimizer.lower() == \"adam\":\n",
    "            self.train_optimizer = optim.Adam(self.Model.parameters(), lr=self.lr)\n",
    "        elif optimizer.lower() == \"gd\":\n",
    "            self.train_optimizer = optim.SGD(self.Model.parameters(), lr=self.lr)\n",
    "    def acc(self,lookback,nstcok):\n",
    "        mae_scores = []\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "        for i in range(10):\n",
    "            actual_values = self.actual_data.reshape(lookback,nstcok)[:,i]\n",
    "            predicted_values = self.predicted_data.reshape(lookback,nstcok)[:,i]\n",
    "            mae = mean_absolute_error(actual_values, predicted_values)\n",
    "            mae_scores.append(mae)\n",
    "        overall_accuracy = np.mean(mae_scores)\n",
    "        self.perstockacc=mae_scores\n",
    "        self.overall_accuracy= overall_accuracy\n",
    "    def mse(self, pred, label):\n",
    "        loss = (pred - label) ** 2\n",
    "        return torch.mean(loss)\n",
    "    def train_item(self, x,y):\n",
    "        self.Model.train()\n",
    "        pred = self.Model(x)\n",
    "        loss = self.mse(pred,torch.tensor(y,requires_grad=True))\n",
    "        self.train_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(self.Model.parameters(), 3.0)\n",
    "        self.train_optimizer.step()\n",
    "        return pred.detach().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "class CompanyModel(nn.Module):\n",
    "    def __init__(self,models):\n",
    "        super(CompanyModel, self).__init__()#name='FactorVAE')\n",
    "        self.models =models\n",
    "        self.fc_out = nn.Linear(64, 1)\n",
    "    def forward(self, x_train,y_train,train=False):\n",
    "\n",
    "        # x.shape == (Ns, T, C)\n",
    "        # if training:\n",
    "        if train==True:\n",
    "            scores=[]\n",
    "            for (i,model) in enumerate( self.models):\n",
    "                yi_pred=c.traders[i].train_item(torch.from_numpy(x_train[c.traders[i].inputfactors].values).float(),y_train)\n",
    "                #yi_pred=model(torch.from_numpy(x_train[c.traders[i].inputfactors].values).float()).detach().numpy()\n",
    "                if(len(c.traders[i].predicted_data)<c.lookback*len(yi_pred)):\n",
    "                    c.traders[i].predicted_data=np.append(c.traders[i].predicted_data,yi_pred)\n",
    "                    c.traders[i].actual_data=np.append(c.traders[i].predicted_data,y_train)\n",
    "                    scores.append(0)\n",
    "                else:\n",
    "                    c.traders[i].predicted_data=np.append(c.traders[i].predicted_data,yi_pred)\n",
    "                    c.traders[i].actual_data=np.append(c.traders[i].predicted_data,y_train)\n",
    "                    c.traders[i].predicted_data = c.traders[i].predicted_data[-200:]\n",
    "                    c.traders[i].actual_data=c.traders[i].actual_data[-200:]\n",
    "                    c.traders[i].acc(c.lookback,len(yi_pred))\n",
    "                    scores.append(c.traders[i].overall_accuracy)\n",
    "\n",
    "            sorted_list = sorted(scores)\n",
    "            smallest_indices = [scores.index(sorted_list[i]) for i in range(c.aggnum)]\n",
    "            ans=[c.traders[i].predicted_data[-len(yi_pred):] for i in smallest_indices]\n",
    "            return sum(ans)/c.aggnum\n",
    "        else:\n",
    "            scores=[]\n",
    "            for (i,model) in enumerate( self.models):\n",
    "                yi_pred=model(torch.from_numpy(x_train[c.traders[i].inputfactors].values).float()).detach().numpy()\n",
    "                scores.append(c.traders[i].overall_accuracy)\n",
    "            sorted_list = sorted(scores)\n",
    "            smallest_indices = [scores.index(sorted_list[i]) for i in range(c.aggnum)]\n",
    "            ans=[c.traders[i].predicted_data[-len(yi_pred):] for i in smallest_indices]\n",
    "            for (i,model) in enumerate( self.models):\n",
    "                c.traders[i].predicted_data=np.append(c.traders[i].predicted_data,yi_pred)\n",
    "                c.traders[i].actual_data=np.append(c.traders[i].predicted_data,y_train)\n",
    "                c.traders[i].predicted_data = c.traders[i].predicted_data[-200:]\n",
    "                c.traders[i].actual_data=c.traders[i].actual_data[-200:]\n",
    "                c.traders[i].acc(c.lookback,len(yi_pred))\n",
    "            return sum(ans)/c.aggnum\n",
    "\n",
    "\n",
    "class Company:\n",
    "    def __init__(self,\n",
    "                 factors:[],\n",
    "                 n_traders: int,\n",
    "                 d_feat: int,\n",
    "                 lookback=20,\n",
    "                 batch_size=1,\n",
    "                 optimizer=\"adam\",\n",
    "                 aggnum=3,\n",
    "                 lr=0.001,\n",
    "                 seed = 1\n",
    "                 ):\n",
    "        self.aggnum=aggnum\n",
    "        self.factors=factors\n",
    "        self.lookback=lookback\n",
    "        self.n_traders = n_traders\n",
    "        self.d_feat =  d_feat\n",
    "        self.batch_size =  batch_size\n",
    "        self.lr = lr\n",
    "        self.logger = get_module_logger(\"TransformerModel\")\n",
    "        self.traders= [Trader(self.factors,self.d_feat,self.batch_size) for n in range(n_traders)]\n",
    "        modells=[i.Model for i in self.traders]\n",
    "        self.Model=CompanyModel(modells)\n",
    "        self.device = torch.device(\"cuda:%d\" % 0if torch.cuda.is_available() and 0 >= 0 else \"cpu\")\n",
    "        if optimizer.lower() == \"adam\":\n",
    "            self.train_optimizer = optim.Adam(self.Model.parameters(), lr=self.lr)\n",
    "        elif optimizer.lower() == \"gd\":\n",
    "            self.train_optimizer = optim.SGD(self.Model.parameters(), lr=self.lr)\n",
    "\n",
    "    def mse(self, pred, label):\n",
    "        loss = (pred - label) ** 2\n",
    "        return torch.mean(loss)\n",
    "\n",
    "    def train_epoch(self, h,nstock):\n",
    "        self.Model.train()\n",
    "        l=int(len(h.fetch(col_set=\"label\").values)/nstock)\n",
    "        indices = np.arange(l)\n",
    "        np.random.shuffle(indices)\n",
    "        losses=[]\n",
    "        for i in range(l):\n",
    "            x=h.fetch(col_set=\"feature\").iloc[10*i:10*i+10]\n",
    "            y=h.fetch(col_set=\"label\").iloc[10*i:10*i+10].values\n",
    "            pred = self.Model(x,y,True)\n",
    "            loss = self.mse(torch.tensor(pred,requires_grad=True),torch.tensor(y,requires_grad=True))\n",
    "            self.train_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(self.Model.parameters(), 3.0)\n",
    "            self.train_optimizer.step()\n",
    "            losses.append(float(loss))\n",
    "        print(\"trainloss:\"+str(sum(losses) / len(losses)))\n",
    "    def test_epoch(self, h,nstock):\n",
    "        l=int(len(h.fetch(col_set=\"label\").values)/nstock)\n",
    "        indices = np.arange(l)\n",
    "        losses=[]\n",
    "        rankic=[]\n",
    "        for i in range(l):\n",
    "            x=h.fetch(col_set=\"feature\").iloc[10*i:10*i+10]\n",
    "            y=h.fetch(col_set=\"label\").iloc[10*i:10*i+10].values\n",
    "            pred = self.Model(x,y)\n",
    "            loss = self.mse(torch.tensor(pred,requires_grad=True),torch.tensor(y,requires_grad=True))\n",
    "            losses.append(float(loss))\n",
    "\n",
    "        print(\"testloss:\"+str(sum(losses) / len(losses)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[94573:MainThread](2023-04-03 19:33:12,348) INFO - qlib.Initialization - [config.py:402] - default_conf: client.\n",
      "[94573:MainThread](2023-04-03 19:33:12,350) INFO - qlib.Initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\n",
      "[94573:MainThread](2023-04-03 19:33:12,351) INFO - qlib.Initialization - [__init__.py:76] - data_path={'__DEFAULT_FREQ': PosixPath('/Users/linweiqiang/.qlib/qlib_data/cn_data')}\n",
      "[94573:MainThread](2023-04-03 19:33:21,451) INFO - qlib.timer - [log.py:117] - Time cost: 9.099s | Loading data Done\n",
      "/Users/linweiqiang/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/dataset/processor.py:242: RuntimeWarning: Mean of empty slice\n",
      "  self.mean_train = np.nanmean(df[cols].values, axis=0)\n",
      "/Users/linweiqiang/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1664: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "[94573:MainThread](2023-04-03 19:33:21,562) INFO - qlib.timer - [log.py:117] - Time cost: 0.106s | ZScoreNorm Done\n",
      "[94573:MainThread](2023-04-03 19:33:21,570) INFO - qlib.timer - [log.py:117] - Time cost: 0.006s | Fillna Done\n",
      "[94573:MainThread](2023-04-03 19:33:21,577) INFO - qlib.timer - [log.py:117] - Time cost: 0.004s | DropnaLabel Done\n",
      "[94573:MainThread](2023-04-03 19:33:21,934) INFO - qlib.timer - [log.py:117] - Time cost: 0.356s | CSZScoreNorm Done\n",
      "[94573:MainThread](2023-04-03 19:33:21,935) INFO - qlib.timer - [log.py:117] - Time cost: 0.482s | fit & process data Done\n",
      "[94573:MainThread](2023-04-03 19:33:21,935) INFO - qlib.timer - [log.py:117] - Time cost: 9.583s | Init data Done\n"
     ]
    }
   ],
   "source": [
    "import qlib\n",
    "from qlib.config import REG_CN\n",
    "from qlib.data.dataset.loader import QlibDataLoader\n",
    "from qlib.data.dataset.processor import ZScoreNorm, Fillna\n",
    "from qlib.contrib.data.handler import Alpha158\n",
    "\n",
    "qlib.init()\n",
    "\n",
    "data_handler_config = {\n",
    "    \"start_time\": \"2017-01-01\",\n",
    "    \"end_time\": \"2018-11-10\",\n",
    "    \"instruments\": ['SH000923','SH600015','SH600019','SH600028','SH600030','SH600036','SH600048','SH600050','SH600104','SH600519'],\n",
    "     \"infer_processors\":[ZScoreNorm(fit_start_time='2017-01-01', fit_end_time=\"2018-11-10\"), Fillna()],\n",
    "}\n",
    "h = Alpha158(**data_handler_config)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[94573:MainThread](2023-04-03 19:33:30,258) INFO - qlib.timer - [log.py:117] - Time cost: 8.317s | Loading data Done\n",
      "/Users/linweiqiang/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/dataset/processor.py:242: RuntimeWarning: Mean of empty slice\n",
      "  self.mean_train = np.nanmean(df[cols].values, axis=0)\n",
      "/Users/linweiqiang/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1664: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "[94573:MainThread](2023-04-03 19:33:30,286) INFO - qlib.timer - [log.py:117] - Time cost: 0.026s | ZScoreNorm Done\n",
      "[94573:MainThread](2023-04-03 19:33:30,288) INFO - qlib.timer - [log.py:117] - Time cost: 0.002s | Fillna Done\n",
      "[94573:MainThread](2023-04-03 19:33:30,291) INFO - qlib.timer - [log.py:117] - Time cost: 0.002s | DropnaLabel Done\n",
      "[94573:MainThread](2023-04-03 19:33:30,316) INFO - qlib.timer - [log.py:117] - Time cost: 0.024s | CSZScoreNorm Done\n",
      "[94573:MainThread](2023-04-03 19:33:30,316) INFO - qlib.timer - [log.py:117] - Time cost: 0.057s | fit & process data Done\n",
      "[94573:MainThread](2023-04-03 19:33:30,317) INFO - qlib.timer - [log.py:117] - Time cost: 8.376s | Init data Done\n"
     ]
    }
   ],
   "source": [
    "data_handler_config = {\n",
    "    \"start_time\": \"2018-11-11\",\n",
    "    \"end_time\": \"2018-12-10\",\n",
    "    \"instruments\": ['SH000923','SH600015','SH600019','SH600028','SH600030','SH600036','SH600048','SH600050','SH600104','SH600519'],\n",
    "     \"infer_processors\":[ZScoreNorm(fit_start_time= \"2018-11-11\", fit_end_time=\"2018-12-10\"), Fillna()],\n",
    "}\n",
    "h2= Alpha158(**data_handler_config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "c=Company(h.get_cols()[0:158],20,6,20,1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainloss:0.9783354238846674\n",
      "testloss:1.069811001252227\n",
      "trainloss:0.9718581991213814\n",
      "testloss:1.0332065823278809\n",
      "trainloss:0.9714907155999852\n",
      "testloss:1.0436734918328916\n",
      "trainloss:0.9714499230822994\n",
      "testloss:1.0388451660062732\n",
      "trainloss:0.9714825566447042\n",
      "testloss:1.0389929604532215\n",
      "trainloss:0.9710013990401198\n",
      "testloss:1.0431691949955364\n",
      "trainloss:0.9707470149457661\n",
      "testloss:1.0432182162212245\n",
      "trainloss:0.9708983006196886\n",
      "testloss:1.045039912732901\n",
      "trainloss:0.9707088618531623\n",
      "testloss:1.0368773081577443\n",
      "trainloss:0.970724549969598\n",
      "testloss:1.0432713643009222\n",
      "trainloss:0.9706960021617184\n",
      "testloss:1.0427402771746104\n",
      "trainloss:0.9708352251405841\n",
      "testloss:1.0449235157584906\n",
      "trainloss:0.9708239143556926\n",
      "testloss:1.0428149743169628\n",
      "trainloss:0.9706458291601165\n",
      "testloss:1.044329292853873\n",
      "trainloss:0.9706213266952444\n",
      "testloss:1.0395804561626842\n",
      "trainloss:0.9701536585458793\n",
      "testloss:1.0446493318471881\n",
      "trainloss:0.9704813976933913\n",
      "testloss:1.0391007735660474\n",
      "trainloss:0.9703203772824748\n",
      "testloss:1.0420697595616173\n",
      "trainloss:0.9703309079426761\n",
      "testloss:1.0402380393065895\n",
      "trainloss:0.970271740865516\n",
      "testloss:1.0473552761751361\n",
      "trainloss:0.9698910670172687\n",
      "testloss:1.0408108408236727\n",
      "trainloss:0.9702656180774534\n",
      "testloss:1.0436829128692109\n",
      "trainloss:0.9701625359674431\n",
      "testloss:1.041856501564892\n",
      "trainloss:0.9698952694390374\n",
      "testloss:1.0393219299238061\n",
      "trainloss:0.9699846661932994\n",
      "testloss:1.0373461436206066\n",
      "trainloss:0.9696829293096547\n",
      "testloss:1.042773955742521\n",
      "trainloss:0.9705387338826634\n",
      "testloss:1.0358313027146167\n",
      "trainloss:0.9698433833198976\n",
      "testloss:1.0421182066656596\n",
      "trainloss:0.9699789131128687\n",
      "testloss:1.0429553796531588\n",
      "trainloss:0.9702078881821747\n",
      "testloss:1.0427771047035241\n",
      "trainloss:0.9708208872256592\n",
      "testloss:1.0414900916125556\n",
      "trainloss:0.970164331708946\n",
      "testloss:1.0400680106181452\n",
      "trainloss:0.9710168421730623\n",
      "testloss:1.0397267093666693\n",
      "trainloss:0.9711205845846876\n",
      "testloss:1.0368747743325075\n",
      "trainloss:0.9705250430345309\n",
      "testloss:1.037260552894976\n",
      "trainloss:0.9708018202823981\n",
      "testloss:1.0419555680680068\n",
      "trainloss:0.970689861944329\n",
      "testloss:1.038305271807175\n",
      "trainloss:0.9709582236331697\n",
      "testloss:1.041838092901373\n",
      "trainloss:0.9708550174017228\n",
      "testloss:1.04358417688855\n",
      "trainloss:0.9706065586693227\n",
      "testloss:1.0457415737193108\n",
      "trainloss:0.9708481450418779\n",
      "testloss:1.0418003769198778\n",
      "trainloss:0.9710521405637745\n",
      "testloss:1.0418772183842344\n",
      "trainloss:0.9704538264433386\n",
      "testloss:1.0447992701124362\n",
      "trainloss:0.9705201154635165\n",
      "testloss:1.039873449075033\n",
      "trainloss:0.9707892934966607\n",
      "testloss:1.042164800781725\n",
      "trainloss:0.9702676784706145\n",
      "testloss:1.0338587981151994\n",
      "trainloss:0.9709767898482474\n",
      "testloss:1.041225497757055\n",
      "trainloss:0.9706776577970081\n",
      "testloss:1.0470486066049873\n",
      "trainloss:0.9703883693348329\n",
      "testloss:1.0437749585476377\n",
      "trainloss:0.9706321571135662\n",
      "testloss:1.035855135897799\n"
     ]
    }
   ],
   "source": [
    "for i in range(5000):\n",
    "    c.train_epoch(h,10)\n",
    "    c.test_epoch(h2,10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}