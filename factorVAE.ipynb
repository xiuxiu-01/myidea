{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, proj_dim=20, H: int=25):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.proj = nn.Linear(in_features=C, out_features=proj_dim)\n",
    "        self.proj2 = nn.Linear(in_features=proj_dim*H, out_features=H)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.gru = nn.GRU(input_size=proj_dim, hidden_size=H)\n",
    "    def forward(self, x):\n",
    "        x=x.float()\n",
    "        x = self.proj(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        #print(x)\n",
    "        # x.shape == (Ns, T, proj_dim)\n",
    "        if len(x.shape) == 4:\n",
    "            B, Ns, T, C = x.shape\n",
    "            x = torch.reshape(x, (B * Ns, T, C))\n",
    "            x,_ = self.gru(x)\n",
    "            x = torch.reshape(x, (B, Ns, -1))\n",
    "            x=self.proj2(x)\n",
    "        else:\n",
    "            x = self.gru(x)\n",
    "        # x.shape == (Ns, rnn_unit)\n",
    "        return x\n",
    "\n",
    "batch_size = 32\n",
    "Ns, T, C = 10, 1, 158\n",
    "fe = FeatureExtractor(T, C)\n",
    "x = torch.randn((batch_size, Ns, T, C))\n",
    "e = fe(x)\n",
    "e = e.detach().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'torch.FloatTensor'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.type()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(32, 10, 158)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class FactorEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, M: int=30, K: int=25):\n",
    "        \"\"\"\n",
    "        M: 投资组合的数量\n",
    "        K: 因子的维度\n",
    "        \"\"\"\n",
    "        super(FactorEncoder, self).__init__()\n",
    "        self.portfolio_layer = nn.Linear(in_features=K, out_features=M)\n",
    "        self.mapping_layer_mu = nn.Linear(in_features=M, out_features=K)\n",
    "        self.mapping_layer_sigma = nn.Linear(in_features=M, out_features=K)\n",
    "        self.softplus = nn.Softplus()\n",
    "    def forward(self, y, e):\n",
    "        e = torch.from_numpy(e)\n",
    "        ap = self.portfolio_layer(e)\n",
    "        # ap.shape == (Ns, M)\n",
    "        if len(e.shape) == 3:\n",
    "            y=y.float()\n",
    "            yp = torch.einsum(\"bn,bnm->bm\", y, ap)\n",
    "        else:\n",
    "            yp = torch.einsum(\"n,nm->m\", y, ap)[torch.newaxis, ...]\n",
    "        # yp.shape == (M, )\n",
    "        mu_post = torch.squeeze(self.mapping_layer_mu(yp))\n",
    "        sigma_post = self.softplus(torch.squeeze(self.mapping_layer_sigma(yp)))\n",
    "        # mu_post.shape, sigma_post.shape == (K, )\n",
    "        return mu_post, sigma_post\n",
    "\n",
    "y = torch.randn((batch_size, Ns))\n",
    "factor_encoder = FactorEncoder(158,158)\n",
    "mu_post, sigma_post = factor_encoder(y, e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class AlphaLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, H: int=158):\n",
    "        \"\"\"\n",
    "        H: Hidden state\n",
    "        \"\"\"\n",
    "        super(AlphaLayer, self).__init__()\n",
    "        self.proj = nn.Linear(in_features= H, out_features=H)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "        self.mapping_layer_mu = nn.Linear(in_features=H, out_features=1)\n",
    "        self.mapping_layer_sigma = nn.Linear(in_features=H, out_features=1)\n",
    "        self.softplus = nn.Softplus()\n",
    "    def forward(self, e):\n",
    "        h_alpha = self.proj(torch.from_numpy(e))\n",
    "        h_alpha = self.leakyrelu(h_alpha)\n",
    "        # h_alpha.shape == (Ns, H)\n",
    "        mu_alpha = torch.squeeze(self.mapping_layer_mu(h_alpha))\n",
    "        sigma_alpha = self.softplus(torch.squeeze(self.mapping_layer_sigma(h_alpha)))\n",
    "        # mu_alpha.shape, sigma_alpha.shape == (Ns, )\n",
    "        return mu_alpha, sigma_alpha"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(32, 10, 158)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, H: int=25, K: int=25):\n",
    "        \"\"\"\n",
    "        H: Hidden state\n",
    "        K: Num of Factors\n",
    "        \"\"\"\n",
    "        super(FactorDecoder, self).__init__()\n",
    "        self.alpha_layer = AlphaLayer(H)\n",
    "        self.beta_layer = nn.Linear(in_features=K, out_features=K)\n",
    "    def forward(self, z, e):\n",
    "        mu_z, sigma_z = z[0], z[1]\n",
    "        # mu_z.shape, sigma_z.shape == (K, )\n",
    "        mu_alpha, sigma_alpha = self.alpha_layer(e)\n",
    "        mu_alpha=mu_alpha.unsqueeze(-1)\n",
    "        sigma_alpha=sigma_alpha.unsqueeze(-1)\n",
    "        # mu_alpha.shape, sigma_alpha.shape == (Ns, )\n",
    "        beta = self.beta_layer(torch.from_numpy(e))\n",
    "        # beta.shape == (Ns, K)\n",
    "        mu_y = torch.squeeze(mu_alpha + torch.matmul(beta, mu_z.unsqueeze(-1)), dim=-1)\n",
    "        sigma_y = torch.squeeze(torch.sqrt(torch.square(sigma_alpha) + torch.matmul(torch.square(beta), torch.square(sigma_z.unsqueeze(-1)))), dim=-1)\n",
    "        # mu_y.shape, sigma_y.shape = (Ns, )\n",
    "        return mu_y, sigma_y\n",
    "\n",
    "fd = FactorDecoder(C,C)\n",
    "mu_y, sigma_y = fd((mu_post, sigma_post), e)\n",
    "# mu_y.shape, sigma_y.shape == (TensorShape([32, 100]), TensorShape([32, 100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class FactorPredictor(nn.Module):\n",
    "\n",
    "    def __init__(self, H: int=25, K: int=25):\n",
    "        \"\"\"\n",
    "        H: Hidden state\n",
    "        K: Num of factor/attn head\n",
    "        \"\"\"\n",
    "        super(FactorPredictor, self).__init__()\n",
    "        self.w_key = nn.Parameter(torch.randn(K, 1))\n",
    "        self.w_value = nn.Parameter(torch.randn(K, 1))\n",
    "        self.q = nn.Parameter(torch.randn(H))\n",
    "\n",
    "        self.mapping_layer_mu = nn.Linear(K, 1)\n",
    "        self.mapping_layer_sigma = nn.Linear(K, 1)\n",
    "        self.softplus = nn.Softplus()\n",
    "    def forward(self, e):\n",
    "        if len(e.shape) == 2:\n",
    "            e_ = torch.unsqueeze(torch.from_numpy(e), dim=0)\n",
    "            # e.shape == (1, Ns, H)\n",
    "            k = torch.einsum(\"kd,dnh->knh\", self.w_key, e_)\n",
    "            v = torch.einsum(\"kd,dnh->knh\", self.w_value, e_)\n",
    "            # k.shape, v.shape == (K, Ns, H)\n",
    "            a_att = torch.einsum(\"h,knh->kn\", self.q, k) / (torch.norm(self.q, p=2) * torch.norm(k, p=2, dim=-1, keepdim=False))\n",
    "            a_att = torch.clamp(a_att, min=0.)\n",
    "            a_att = a_att / torch.clamp(torch.sum(a_att, dim=-1, keepdim=True), min=1e-6)\n",
    "            # a_att.shape == (K, Ns)\n",
    "            h_muti = torch.einsum(\"kn,knh->kh\", a_att, v)\n",
    "            # h_muti.shape == (K, H)\n",
    "            mu_prior = torch.squeeze(self.mapping_layer_mu(h_muti), dim=-1)\n",
    "            sigma_prior = self.softplus(torch.squeeze(self.mapping_layer_sigma(h_muti), dim=-1))\n",
    "            # mu_prior.shape, sigma_prior.shape == (K, )\n",
    "        else:\n",
    "            e_ = torch.unsqueeze(torch.from_numpy(e), dim=1)\n",
    "            # e.shape == (b, 1, Ns, H)\n",
    "            k = torch.einsum(\"kd,bdnh->bknh\", self.w_key, e_)\n",
    "            v = torch.einsum(\"kd,bdnh->bknh\", self.w_value, e_)\n",
    "            # k.shape, v.shape == (b, K, Ns, H)\n",
    "\n",
    "            q_norm = torch.norm(self.q, p=2, dim=-1, keepdim=True)\n",
    "            # q_norm.shape == (1, K, 1)\n",
    "            k_norm = torch.norm(k, p=2, dim=-1, keepdim=False)\n",
    "            # k_norm.shape = (b, K, Ns)\n",
    "            a_att = torch.einsum(\"h,bknh->bkn\", self.q, k) / (q_norm * k_norm)\n",
    "\n",
    "            a_att = torch.clamp(a_att, min=0.)\n",
    "            a_att = a_att / torch.clamp(torch.sum(a_att, dim=-1, keepdim=True), min=1e-6)\n",
    "            # print(torch.clamp(torch.sum(a_att, dim=-1, keepdim=True), min=0.0001))\n",
    "            # a_att.shape == (b, K, Ns)\n",
    "            h_muti = torch.einsum(\"bkn,bknh->bkh\", a_att, v)\n",
    "            # h_muti.shape == (b, K, H)\n",
    "            mu_prior = torch.squeeze(self.mapping_layer_mu(h_muti), dim=-1)\n",
    "            sigma_prior = self.softplus(torch.squeeze(self.mapping_layer_sigma(h_muti), dim=-1))\n",
    "            # mu_prior.shape, sigma_prior.shape == (b, K, )\n",
    "        return mu_prior, sigma_prior\n",
    "fp = FactorPredictor(158,158)\n",
    "mu_prior, sigma_prior = fp(e)\n",
    "# mu_prior.shape, sigma_prior.shape == (TensorShape([32, 25]), TensorShape([32, 25]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorVAE(nn.Module):\n",
    "    def __init__(self, M: int=30, H: int=158, K: int=158, proj_dim: int=20):\n",
    "        super(FactorVAE, self).__init__()#name='FactorVAE')\n",
    "        self.feature_extractor = FeatureExtractor(proj_dim=proj_dim, H=H)\n",
    "        self.factor_encoder = FactorEncoder(H,K)\n",
    "        self.factor_predictor = FactorPredictor(H,K)\n",
    "        self.factor_decoder = FactorDecoder(H,K)\n",
    "    def forward(self, x, y=None, training=False):\n",
    "\n",
    "        # x.shape == (Ns, T, C)\n",
    "        if training:\n",
    "            if y is None:\n",
    "                raise ValueError(\"`y` must be stock future return!\")\n",
    "            e = self.feature_extractor(x)\n",
    "            # e.shape == (Ns, H)\n",
    "            z_post = self.factor_encoder(y, e.detach().numpy())\n",
    "            z_prior = self.factor_predictor(e.detach().numpy())\n",
    "            # z_post.shape, z_prior.shape == (K, ) 这里包含均值和方差的维度\n",
    "            y_rec = self.factor_decoder(z_post, e.detach().numpy())\n",
    "            y_pred = self.factor_decoder(z_prior, e.detach().numpy())\n",
    "            # y.shape = (Ns, )\n",
    "            return z_post, z_prior, y_rec, y_pred\n",
    "        else:\n",
    "            e = self.feature_extractor(x)\n",
    "            z_prior = self.factor_predictor(e.detach().numpy())\n",
    "            y_pred = self.factor_decoder(z_prior, e.detach().numpy())\n",
    "            return y_pred\n",
    "\n",
    "batch_size = 4\n",
    "model = FactorVAE(H=C,K=C,proj_dim=T)\n",
    "x = torch.randn((batch_size, Ns, T, C))\n",
    "y = torch.randn((batch_size, Ns))\n",
    "z_post, z_prior, y_rec, y_pred = model(x, y, training=True)\n",
    "y_pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "y_pred = model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.0721, -0.0724, -0.0736, -0.0734, -0.0742, -0.0742, -0.0745, -0.0749,\n         -0.0744, -0.0749],\n        [-0.0828, -0.0828, -0.0829, -0.0834, -0.0839, -0.0831, -0.0835, -0.0830,\n         -0.0829, -0.0836],\n        [-0.0829, -0.0829, -0.0839, -0.0831, -0.0827, -0.0827, -0.0828, -0.0829,\n         -0.0830, -0.0830],\n        [-0.0831, -0.0838, -0.0831, -0.0829, -0.0839, -0.0830, -0.0832, -0.0837,\n         -0.0835, -0.0827]], grad_fn=<SqueezeBackward1>)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl(u1,sigma1,u2,sigma2):\n",
    "    \"\"\"\n",
    "    计算两个多元高斯分布之间KL散度KL(N1||N2)；\n",
    "    \n",
    "    所有的shape均为(B1,B2,...,dim),表示协方差为0的多元高斯分布\n",
    "    这里我们假设加上Batch_size，即形状为(B,dim)\n",
    "    \n",
    "    dim:特征的维度\n",
    "    \"\"\"\n",
    "\n",
    "    sigma1_matrix = torch.diag_embed(sigma1) # (B,dim,dim)\n",
    "    sigma1_matrix_det = torch.det(sigma1_matrix) # (B,)\n",
    "\n",
    "    sigma2_matrix = torch.diag_embed(sigma2) # (B,dim,dim)\n",
    "    sigma2_matrix_det = torch.det(sigma2_matrix) # (B,)\n",
    "    sigma2_matrix_inv = torch.diag_embed(1./sigma2) # (B,dim,dim)\n",
    "\n",
    "    delta_u = torch.unsqueeze((u2-u1),dim=-1) # (B,dim,1)\n",
    "    delta_u_transpose = torch.transpose(delta_u, -1, -2) # (B,1,dim)\n",
    "\n",
    "    term1 = torch.sum((1./sigma2)*sigma1,dim=-1) # (B,) represent trace term\n",
    "    term2 = delta_u_transpose @ sigma2_matrix_inv @ delta_u  # (B,)\n",
    "    term3 = - u1.shape[-1]\n",
    "    term4 = torch.log(sigma2_matrix_det) - torch.log(sigma1_matrix_det)\n",
    "\n",
    "    KL = 0.5 * (term1 + term2 + term3 + term4)\n",
    "\n",
    "    # if you want to compute the mean of a batch,then,\n",
    "    KL_mean = torch.mean(KL)\n",
    "\n",
    "    return KL_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def loss_function(z_post, z_prior, y, y_rec, gamma=10):\n",
    "    mu, sigma = y_rec\n",
    "    f = lambda x: torch.exp(-torch.square(x - mu)/(2*torch.square(sigma))) / (sigma*torch.sqrt(torch.tensor(2*np.pi)))\n",
    "    term1 = - torch.mean(torch.log(f(y)))\n",
    "    term2 = compute_kl(z_post[0], z_post[1], z_prior[0], z_prior[1])\n",
    "    return term1 + gamma * term2\n",
    "\n",
    "#loss_function(z_post, z_prior, y, y_rec)\n",
    "# <tf.Tensor: shape=(), dtype=float32, numpy=38.76919>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def compute_rank_ic(y_pred, y_true):\n",
    "    # y_pred.shape, y_true.shape == (b, Ns)\n",
    "    _, Ns = y_true.shape\n",
    "    ic = 1/Ns*torch.einsum(\"bn,bn->b\", y_pred - torch.mean(y_pred, dim=-1, keepdim=True), y_true - torch.mean(y_true, dim=-1, keepdim=True))\n",
    "    ic = ic / (torch.sqrt(torch.std(y_pred, dim=-1))*torch.sqrt(torch.std(y_true, dim=-1)))\n",
    "    return torch.mean(ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "def train_step(x_batch, y_batch):\n",
    "    optimizer.zero_grad()\n",
    "    z_post, z_prior, y_rec, y_pred = model(x_batch.double(), y_batch.double(), training=True)\n",
    "    loss = loss_function(z_post, z_prior, y_batch, y_rec)\n",
    "    rank_ic = compute_rank_ic(y_pred[0], y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, rank_ic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6878:MainThread](2023-04-02 16:45:48,814) INFO - qlib.Initialization - [config.py:402] - default_conf: client.\n",
      "[6878:MainThread](2023-04-02 16:45:49,194) INFO - qlib.Initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\n",
      "[6878:MainThread](2023-04-02 16:45:49,195) INFO - qlib.Initialization - [__init__.py:76] - data_path={'__DEFAULT_FREQ': PosixPath('/Users/linweiqiang/.qlib/qlib_data/cn_data')}\n",
      "[6878:MainThread](2023-04-02 16:45:57,886) INFO - qlib.timer - [log.py:117] - Time cost: 8.690s | Loading data Done\n",
      "/Users/linweiqiang/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/dataset/processor.py:242: RuntimeWarning: Mean of empty slice\n",
      "  self.mean_train = np.nanmean(df[cols].values, axis=0)\n",
      "/Users/linweiqiang/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1664: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "[6878:MainThread](2023-04-02 16:45:58,160) INFO - qlib.timer - [log.py:117] - Time cost: 0.265s | ZScoreNorm Done\n",
      "[6878:MainThread](2023-04-02 16:45:58,169) INFO - qlib.timer - [log.py:117] - Time cost: 0.008s | Fillna Done\n",
      "[6878:MainThread](2023-04-02 16:45:58,179) INFO - qlib.timer - [log.py:117] - Time cost: 0.008s | DropnaLabel Done\n",
      "[6878:MainThread](2023-04-02 16:45:59,120) INFO - qlib.timer - [log.py:117] - Time cost: 0.940s | CSZScoreNorm Done\n",
      "[6878:MainThread](2023-04-02 16:45:59,121) INFO - qlib.timer - [log.py:117] - Time cost: 1.233s | fit & process data Done\n",
      "[6878:MainThread](2023-04-02 16:45:59,121) INFO - qlib.timer - [log.py:117] - Time cost: 9.925s | Init data Done\n"
     ]
    }
   ],
   "source": [
    "import qlib\n",
    "from qlib.config import REG_CN\n",
    "from qlib.data.dataset.loader import QlibDataLoader\n",
    "from qlib.data.dataset.processor import ZScoreNorm, Fillna\n",
    "from qlib.contrib.data.handler import Alpha158\n",
    "\n",
    "qlib.init()\n",
    "\n",
    "data_handler_config = {\n",
    "    \"start_time\": \"2013-01-01\",\n",
    "    \"end_time\": \"2017-12-10\",\n",
    "    \"instruments\": ['SH000923','SH600015','SH600019','SH600028','SH600030','SH600036','SH600048','SH600050','SH600104','SH600519'],\n",
    "     \"infer_processors\":[ZScoreNorm(fit_start_time='2013-01-01', fit_end_time=\"2017-12-10\"), Fillna()],\n",
    "}\n",
    "h = Alpha158(**data_handler_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "Epoch 1, step 0, loss 13.438911437988281, rank IC -0.00015846197493374348\n",
      "Epoch 1, step 50, loss 6.8710408210754395, rank IC 0.0038746988866478205\n",
      "Epoch 1, step 100, loss 3.6537559032440186, rank IC -0.0013625550782307982\n",
      "Epoch 1, step 150, loss 4.9384894371032715, rank IC -0.0021817819215357304\n",
      "Epoch 1, step 200, loss 3.6534605026245117, rank IC -0.006984233390539885\n",
      "Epoch 1, step 250, loss 2.7783010005950928, rank IC -0.006748193874955177\n",
      "Epoch 1, step 300, loss 2.5676867961883545, rank IC -0.0017780187772586942\n",
      "Epoch 1, step 350, loss 2.8439111709594727, rank IC 0.0021678556222468615\n",
      "Epoch 2, step 0, loss 2.171469211578369, rank IC 0.0037812478840351105\n",
      "Epoch 2, step 50, loss 2.2188162803649902, rank IC -0.0030063530430197716\n",
      "Epoch 2, step 100, loss 1.6173110008239746, rank IC -0.002598225837573409\n",
      "Epoch 2, step 150, loss 2.5132956504821777, rank IC -0.001801606616936624\n",
      "Epoch 2, step 200, loss 1.9300047159194946, rank IC -0.005751485005021095\n",
      "Epoch 2, step 250, loss 1.5153441429138184, rank IC -0.005880980286747217\n",
      "Epoch 2, step 300, loss 1.5193331241607666, rank IC -0.0016831861576065421\n",
      "Epoch 2, step 350, loss 1.964418888092041, rank IC 0.001145698712207377\n",
      "Epoch 3, step 0, loss 1.3138833045959473, rank IC 0.004777117166668177\n",
      "Epoch 3, step 50, loss 1.5747275352478027, rank IC -0.003504592925310135\n",
      "Epoch 3, step 100, loss 1.0870246887207031, rank IC -0.00013603696424979717\n",
      "Epoch 3, step 150, loss 1.9447968006134033, rank IC -0.001967982156202197\n",
      "Epoch 3, step 200, loss 1.4926687479019165, rank IC -0.004722366575151682\n",
      "Epoch 3, step 250, loss 1.196552038192749, rank IC -0.0064153210259974\n",
      "Epoch 3, step 300, loss 1.2620322704315186, rank IC -0.002073767362162471\n",
      "Epoch 3, step 350, loss 1.7611687183380127, rank IC 0.002695334143936634\n",
      "Epoch 4, step 0, loss 1.1025723218917847, rank IC 0.003841768018901348\n",
      "Epoch 4, step 50, loss 1.423292875289917, rank IC -0.005577108357101679\n",
      "Epoch 4, step 100, loss 0.9602816700935364, rank IC 0.00012181940110167488\n",
      "Epoch 4, step 150, loss 1.8117142915725708, rank IC -0.0026745025534182787\n",
      "Epoch 4, step 200, loss 1.3944011926651, rank IC -0.00449552945792675\n",
      "Epoch 4, step 250, loss 1.1245394945144653, rank IC -0.0059564183466136456\n",
      "Epoch 4, step 300, loss 1.2066634893417358, rank IC -0.0017846118425950408\n",
      "Epoch 4, step 350, loss 1.7241302728652954, rank IC 0.002653111470863223\n",
      "Epoch 5, step 0, loss 1.0550557374954224, rank IC 0.0033431723713874817\n",
      "Epoch 5, step 50, loss 1.3934123516082764, rank IC -0.005318294744938612\n",
      "Epoch 5, step 100, loss 0.9318860769271851, rank IC 0.0005683861090801656\n",
      "Epoch 5, step 150, loss 1.7800335884094238, rank IC -0.0028839129954576492\n",
      "Epoch 5, step 200, loss 1.3699345588684082, rank IC -0.004545832518488169\n",
      "Epoch 5, step 250, loss 1.1072988510131836, rank IC -0.005669583100825548\n",
      "Epoch 5, step 300, loss 1.1950256824493408, rank IC -0.0014053367776796222\n",
      "Epoch 5, step 350, loss 1.7226157188415527, rank IC 0.002271457342430949\n",
      "Epoch 6, step 0, loss 1.042844533920288, rank IC 0.0025551116559654474\n",
      "Epoch 6, step 50, loss 1.3885682821273804, rank IC -0.004584231413900852\n",
      "Epoch 6, step 100, loss 0.9239816069602966, rank IC 0.0012700186343863606\n",
      "Epoch 6, step 150, loss 1.7691283226013184, rank IC -0.0029526520520448685\n",
      "Epoch 6, step 200, loss 1.3509938716888428, rank IC -0.005719196051359177\n",
      "Epoch 6, step 250, loss 1.100134015083313, rank IC -0.0053412276320159435\n",
      "Epoch 6, step 300, loss 1.1917928457260132, rank IC -0.0011950860498473048\n",
      "Epoch 6, step 350, loss 1.7262299060821533, rank IC 0.0017765540396794677\n",
      "Epoch 7, step 0, loss 1.038231611251831, rank IC 0.002430426888167858\n",
      "Epoch 7, step 50, loss 1.3882321119308472, rank IC -0.004681945778429508\n",
      "Epoch 7, step 100, loss 0.9204275608062744, rank IC 0.0008929658215492964\n",
      "Epoch 7, step 150, loss 1.7634761333465576, rank IC -0.002699753502383828\n",
      "Epoch 7, step 200, loss 1.364028811454773, rank IC -0.005365891382098198\n",
      "Epoch 7, step 250, loss 1.0958592891693115, rank IC -0.005910155829042196\n",
      "Epoch 7, step 300, loss 1.190001368522644, rank IC -0.00034640883677639067\n",
      "Epoch 7, step 350, loss 1.7313495874404907, rank IC 0.001507317298091948\n",
      "Epoch 8, step 0, loss 1.0348156690597534, rank IC 0.0017872732132673264\n",
      "Epoch 8, step 50, loss 1.3888334035873413, rank IC -0.006446473766118288\n",
      "Epoch 8, step 100, loss 0.9180349111557007, rank IC 0.002708552638068795\n",
      "Epoch 8, step 150, loss 1.7591564655303955, rank IC -0.0023797128815203905\n",
      "Epoch 8, step 200, loss 1.3421066999435425, rank IC -0.005545653402805328\n",
      "Epoch 8, step 250, loss 1.0910730361938477, rank IC -0.00509977200999856\n",
      "Epoch 8, step 300, loss 1.1884636878967285, rank IC -0.0001875520101748407\n",
      "Epoch 8, step 350, loss 1.7358161211013794, rank IC 0.0014562122523784637\n",
      "Epoch 9, step 0, loss 1.0322670936584473, rank IC 0.0021645575761795044\n",
      "Epoch 9, step 50, loss 1.3889206647872925, rank IC -0.005910640116780996\n",
      "Epoch 9, step 100, loss 0.9159636497497559, rank IC 0.002429726766422391\n",
      "Epoch 9, step 150, loss 1.755807638168335, rank IC -0.00037780930870212615\n",
      "Epoch 9, step 200, loss 1.3794176578521729, rank IC -0.005892585963010788\n",
      "Epoch 9, step 250, loss 1.0864403247833252, rank IC -0.004396222531795502\n",
      "Epoch 9, step 300, loss 1.187362551689148, rank IC -0.00012712774332612753\n",
      "Epoch 9, step 350, loss 1.7397392988204956, rank IC 0.0017876500496640801\n",
      "Epoch 10, step 0, loss 1.0303157567977905, rank IC 0.002425283892080188\n",
      "Epoch 10, step 50, loss 1.3894983530044556, rank IC -0.005486129317432642\n",
      "Epoch 10, step 100, loss 0.9146459102630615, rank IC 0.0019007482333108783\n",
      "Epoch 10, step 150, loss 1.752398133277893, rank IC 0.0013780960580334067\n",
      "Epoch 10, step 200, loss 1.3734928369522095, rank IC -0.005923312623053789\n",
      "Epoch 10, step 250, loss 1.0818558931350708, rank IC -0.0036819896195083857\n",
      "Epoch 10, step 300, loss 1.1864228248596191, rank IC -0.0007175170467235148\n",
      "Epoch 10, step 350, loss 1.7429440021514893, rank IC 0.002014421159401536\n",
      "Epoch 11, step 0, loss 1.029013752937317, rank IC 0.0010653859935700893\n",
      "Epoch 11, step 50, loss 1.3894083499908447, rank IC -0.0041275243274867535\n",
      "Epoch 11, step 100, loss 0.9132772088050842, rank IC 0.002573668723925948\n",
      "Epoch 11, step 150, loss 1.749964714050293, rank IC 0.0015355813084170222\n",
      "Epoch 11, step 200, loss 1.569169282913208, rank IC -0.005734833423048258\n",
      "Epoch 11, step 250, loss 1.0782378911972046, rank IC -0.0021263277158141136\n",
      "Epoch 11, step 300, loss 1.1857067346572876, rank IC -0.0013687083264812827\n",
      "Epoch 11, step 350, loss 1.7462750673294067, rank IC 0.0017030833987519145\n",
      "Epoch 12, step 0, loss 1.0277656316757202, rank IC 0.00041730026714503765\n",
      "Epoch 12, step 50, loss 1.389573097229004, rank IC -0.0026382727082818747\n",
      "Epoch 12, step 100, loss 0.9120243787765503, rank IC 0.0023063865955919027\n",
      "Epoch 12, step 150, loss 1.748110294342041, rank IC 0.0018327435245737433\n",
      "Epoch 12, step 200, loss 1.409933090209961, rank IC -0.008492215536534786\n",
      "Epoch 12, step 250, loss 1.073562741279602, rank IC -0.001170700998045504\n",
      "Epoch 12, step 300, loss 1.1848286390304565, rank IC -0.0016127167036756873\n",
      "Epoch 12, step 350, loss 1.748806357383728, rank IC 0.0010136235505342484\n",
      "Epoch 13, step 0, loss 1.026518702507019, rank IC 1.4118850231170654e-06\n",
      "Epoch 13, step 50, loss 1.389430046081543, rank IC -0.001699968590401113\n",
      "Epoch 13, step 100, loss 0.9109805822372437, rank IC 0.0020194381941109896\n",
      "Epoch 13, step 150, loss 1.7461059093475342, rank IC 0.0022321881260722876\n",
      "Epoch 13, step 200, loss 1.4018536806106567, rank IC -0.007635950576514006\n",
      "Epoch 13, step 250, loss 1.0697072744369507, rank IC -0.0006056432612240314\n",
      "Epoch 13, step 300, loss 1.1842130422592163, rank IC -0.0013883099891245365\n",
      "Epoch 13, step 350, loss 1.751492977142334, rank IC 0.0008646072237752378\n",
      "Epoch 14, step 0, loss 1.0255208015441895, rank IC -0.00029237676062621176\n",
      "Epoch 14, step 50, loss 1.389769196510315, rank IC -0.0008183688041754067\n",
      "Epoch 14, step 100, loss 0.9100979566574097, rank IC 0.0017987083410844207\n",
      "Epoch 14, step 150, loss 1.7440450191497803, rank IC 0.0025339832063764334\n",
      "Epoch 14, step 200, loss 1.403563380241394, rank IC -0.00948905386030674\n",
      "Epoch 14, step 250, loss 1.0661189556121826, rank IC -9.013339877128601e-05\n",
      "Epoch 14, step 300, loss 1.1837581396102905, rank IC -0.0013724915916100144\n",
      "Epoch 14, step 350, loss 1.7533679008483887, rank IC 5.220784805715084e-05\n",
      "Epoch 15, step 0, loss 1.0248292684555054, rank IC -0.0008096922538243234\n",
      "Epoch 15, step 50, loss 1.3895344734191895, rank IC -0.00017715121794026345\n",
      "Epoch 15, step 100, loss 0.90937739610672, rank IC 0.001089236349798739\n",
      "Epoch 15, step 150, loss 1.7430795431137085, rank IC 0.0026135428342968225\n",
      "Epoch 15, step 200, loss 1.4168791770935059, rank IC -0.00740807643160224\n",
      "Epoch 15, step 250, loss 1.0626609325408936, rank IC 0.0012011100770905614\n",
      "Epoch 15, step 300, loss 1.1831119060516357, rank IC -0.0010581344831734896\n",
      "Epoch 15, step 350, loss 1.755378007888794, rank IC -0.00018539046868681908\n",
      "Epoch 16, step 0, loss 1.024156928062439, rank IC -0.0008968701586127281\n",
      "Epoch 16, step 50, loss 1.389693260192871, rank IC -0.00017258618026971817\n",
      "Epoch 16, step 100, loss 0.9089120626449585, rank IC 0.0009133249404840171\n",
      "Epoch 16, step 150, loss 1.7412586212158203, rank IC 0.002892509801313281\n",
      "Epoch 16, step 200, loss 1.392059326171875, rank IC -0.007304109632968903\n",
      "Epoch 16, step 250, loss 1.0592776536941528, rank IC 0.0019005556823685765\n",
      "Epoch 16, step 300, loss 1.1827424764633179, rank IC -0.0005834711482748389\n",
      "Epoch 16, step 350, loss 1.7563660144805908, rank IC -0.0003905123157892376\n",
      "Epoch 17, step 0, loss 1.02385413646698, rank IC -0.001426722388714552\n",
      "Epoch 17, step 50, loss 1.3897923231124878, rank IC -2.425815910100937e-05\n",
      "Epoch 17, step 100, loss 0.9086033701896667, rank IC 0.000620691105723381\n",
      "Epoch 17, step 150, loss 1.7400354146957397, rank IC 0.003194449469447136\n",
      "Epoch 17, step 200, loss 1.3558069467544556, rank IC -0.007165759336203337\n",
      "Epoch 17, step 250, loss 1.0561256408691406, rank IC 0.0028881626203656197\n",
      "Epoch 17, step 300, loss 1.1822850704193115, rank IC -6.338651292026043e-05\n",
      "Epoch 17, step 350, loss 1.7571799755096436, rank IC -0.0007182558183558285\n",
      "Epoch 18, step 0, loss 1.023695468902588, rank IC -0.00231688073836267\n",
      "Epoch 18, step 50, loss 1.3898365497589111, rank IC -0.0005614906549453735\n",
      "Epoch 18, step 100, loss 0.9084829092025757, rank IC 0.0009156111627817154\n",
      "Epoch 18, step 150, loss 1.7393229007720947, rank IC 0.003432929515838623\n",
      "Epoch 18, step 200, loss 1.3401284217834473, rank IC -0.006529835518449545\n",
      "Epoch 18, step 250, loss 1.0531806945800781, rank IC 0.003086572512984276\n",
      "Epoch 18, step 300, loss 1.1818840503692627, rank IC 6.1507648752012756e-06\n",
      "Epoch 18, step 350, loss 1.7621769905090332, rank IC -0.0010283227311447263\n",
      "Epoch 19, step 0, loss 1.0211188793182373, rank IC -0.002945093670859933\n",
      "Epoch 19, step 50, loss 1.3897544145584106, rank IC -0.00040233414620161057\n",
      "Epoch 19, step 100, loss 0.9059399962425232, rank IC 0.0009546387009322643\n",
      "Epoch 19, step 150, loss 1.7383641004562378, rank IC 0.0037513114511966705\n",
      "Epoch 19, step 200, loss 1.3818143606185913, rank IC -0.006355547811836004\n",
      "Epoch 19, step 250, loss 1.0513159036636353, rank IC 0.005099524278193712\n",
      "Epoch 19, step 300, loss 1.1817017793655396, rank IC 0.00018828967586159706\n",
      "Epoch 19, step 350, loss 1.76043701171875, rank IC -0.001379888504743576\n",
      "Epoch 20, step 0, loss 1.0222420692443848, rank IC -0.00304375565610826\n",
      "Epoch 20, step 50, loss 1.3897229433059692, rank IC -0.00018760934472084045\n",
      "Epoch 20, step 100, loss 0.9071096777915955, rank IC 0.0006585398805327713\n",
      "Epoch 20, step 150, loss 1.7374625205993652, rank IC 0.0038810449186712503\n",
      "Epoch 20, step 200, loss 1.3352339267730713, rank IC -0.005634089931845665\n",
      "Epoch 20, step 250, loss 1.0482450723648071, rank IC 0.005448607262223959\n",
      "Epoch 20, step 300, loss 1.181427240371704, rank IC 0.0003884420730173588\n",
      "Epoch 20, step 350, loss 1.7608914375305176, rank IC -0.00202863453887403\n",
      "Epoch 21, step 0, loss 1.022323727607727, rank IC -0.003856103168800473\n",
      "Epoch 21, step 50, loss 1.3897356986999512, rank IC -0.00044058109051547945\n",
      "Epoch 21, step 100, loss 0.907206118106842, rank IC 0.0010023621143773198\n",
      "Epoch 21, step 150, loss 1.7366787195205688, rank IC 0.00407723244279623\n",
      "Epoch 21, step 200, loss 1.3737581968307495, rank IC -0.005368390586227179\n",
      "Epoch 21, step 250, loss 1.0462092161178589, rank IC 0.006846544798463583\n",
      "Epoch 21, step 300, loss 1.18118417263031, rank IC 0.0009939187439158559\n",
      "Epoch 21, step 350, loss 1.7612351179122925, rank IC -0.0020110325422137976\n",
      "Epoch 22, step 0, loss 1.0222498178482056, rank IC -0.003387153847143054\n",
      "Epoch 22, step 50, loss 1.3900632858276367, rank IC -0.0005037809605710208\n",
      "Epoch 22, step 100, loss 0.9073543548583984, rank IC 0.0008703689090907574\n",
      "Epoch 22, step 150, loss 1.7358800172805786, rank IC 0.004154178313910961\n",
      "Epoch 22, step 200, loss 1.2874846458435059, rank IC -0.004977331962436438\n",
      "Epoch 22, step 250, loss 1.0440552234649658, rank IC 0.006848307326436043\n",
      "Epoch 22, step 300, loss 1.1808974742889404, rank IC 0.0010447081876918674\n",
      "Epoch 22, step 350, loss 1.7613667249679565, rank IC -0.00208736932836473\n",
      "Epoch 23, step 0, loss 1.0222190618515015, rank IC -0.0035635794047266245\n",
      "Epoch 23, step 50, loss 1.3897311687469482, rank IC 0.00012815154332201928\n",
      "Epoch 23, step 100, loss 0.9073105454444885, rank IC 0.00025127254775725305\n",
      "Epoch 23, step 150, loss 1.7355303764343262, rank IC 0.004087295848876238\n",
      "Epoch 23, step 200, loss 1.3650321960449219, rank IC -0.005979178007692099\n",
      "Epoch 23, step 250, loss 1.042443037033081, rank IC 0.006894389633089304\n",
      "Epoch 23, step 300, loss 1.1805980205535889, rank IC 0.0010853279381990433\n",
      "Epoch 23, step 350, loss 1.762123703956604, rank IC -0.002195205772295594\n",
      "Epoch 24, step 0, loss 1.022056221961975, rank IC -0.0035290829837322235\n",
      "Epoch 24, step 50, loss 1.3898519277572632, rank IC 0.000252975762123242\n",
      "Epoch 24, step 100, loss 0.9071422815322876, rank IC 0.00020346883684396744\n",
      "Epoch 24, step 150, loss 1.735056757926941, rank IC 0.004421265330165625\n",
      "Epoch 24, step 200, loss 1.3533350229263306, rank IC -0.005329307168722153\n",
      "Epoch 24, step 250, loss 1.0408393144607544, rank IC 0.00703422911465168\n",
      "Epoch 24, step 300, loss 1.1804827451705933, rank IC 0.001230129855684936\n",
      "Epoch 24, step 350, loss 1.7619436979293823, rank IC -0.002266345778480172\n",
      "Epoch 25, step 0, loss 1.0220859050750732, rank IC -0.0037621569354087114\n",
      "Epoch 25, step 50, loss 1.3895567655563354, rank IC 2.3910775780677795e-05\n",
      "Epoch 25, step 100, loss 0.9071053862571716, rank IC 0.0009073720430023968\n",
      "Epoch 25, step 150, loss 1.7349121570587158, rank IC 0.004681026097387075\n",
      "Epoch 25, step 200, loss 1.3320971727371216, rank IC -0.005053200758993626\n",
      "Epoch 25, step 250, loss 1.0386734008789062, rank IC 0.007472041994333267\n",
      "Epoch 25, step 300, loss 1.1801143884658813, rank IC 0.0012805572478100657\n",
      "Epoch 25, step 350, loss 1.7627403736114502, rank IC -0.00233427039347589\n",
      "Epoch 26, step 0, loss 1.0218228101730347, rank IC -0.0038594547659158707\n",
      "Epoch 26, step 50, loss 1.3896583318710327, rank IC 0.0001204696818604134\n",
      "Epoch 26, step 100, loss 0.9068515300750732, rank IC 0.00040481644100509584\n",
      "Epoch 26, step 150, loss 1.7342371940612793, rank IC 0.0048563722521066666\n",
      "Epoch 26, step 200, loss 1.2930742502212524, rank IC -0.005005464889109135\n",
      "Epoch 26, step 250, loss 1.0376801490783691, rank IC 0.007269054651260376\n",
      "Epoch 26, step 300, loss 1.1799629926681519, rank IC 0.0013409588718786836\n",
      "Epoch 26, step 350, loss 1.7631176710128784, rank IC -0.0024034332018345594\n",
      "Epoch 27, step 0, loss 1.0216028690338135, rank IC -0.0038851189892739058\n",
      "Epoch 27, step 50, loss 1.3893228769302368, rank IC 0.00037087779492139816\n",
      "Epoch 27, step 100, loss 0.9067448973655701, rank IC -8.103127584035974e-06\n",
      "Epoch 27, step 150, loss 1.7348885536193848, rank IC 0.004915055353194475\n",
      "Epoch 27, step 200, loss 1.3371121883392334, rank IC -0.004248677287250757\n",
      "Epoch 27, step 250, loss 1.0366863012313843, rank IC 0.007469164673238993\n",
      "Epoch 27, step 300, loss 1.1795728206634521, rank IC 0.0014327942626550794\n",
      "Epoch 27, step 350, loss 1.7637114524841309, rank IC -0.0024448491167277098\n",
      "Epoch 28, step 0, loss 1.0213546752929688, rank IC -0.004011195618659258\n",
      "Epoch 28, step 50, loss 1.3893452882766724, rank IC 0.0007337763090617955\n",
      "Epoch 28, step 100, loss 0.906565248966217, rank IC -0.0003989093529526144\n",
      "Epoch 28, step 150, loss 1.7343344688415527, rank IC 0.004841081332415342\n",
      "Epoch 28, step 200, loss 1.2940900325775146, rank IC -0.00442046532407403\n",
      "Epoch 28, step 250, loss 1.0352253913879395, rank IC 0.00761224702000618\n",
      "Epoch 28, step 300, loss 1.1793888807296753, rank IC 0.0015622986247763038\n",
      "Epoch 28, step 350, loss 1.7640317678451538, rank IC -0.0025130908470600843\n",
      "Epoch 29, step 0, loss 1.0211617946624756, rank IC -0.0041587031446397305\n",
      "Epoch 29, step 50, loss 1.3890923261642456, rank IC 0.0014175897231325507\n",
      "Epoch 29, step 100, loss 0.9065265655517578, rank IC -0.0005696071311831474\n",
      "Epoch 29, step 150, loss 1.734626293182373, rank IC 0.0050713676027953625\n",
      "Epoch 29, step 200, loss 1.325060486793518, rank IC -0.0033605669159442186\n",
      "Epoch 29, step 250, loss 1.0344929695129395, rank IC 0.007821731269359589\n",
      "Epoch 29, step 300, loss 1.1790733337402344, rank IC 0.0018410246120765805\n",
      "Epoch 29, step 350, loss 1.7646300792694092, rank IC -0.0027252661529928446\n",
      "Epoch 30, step 0, loss 1.0209492444992065, rank IC -0.004301820416003466\n",
      "Epoch 30, step 50, loss 1.3892637491226196, rank IC 0.00157984159886837\n",
      "Epoch 30, step 100, loss 0.9063851833343506, rank IC -0.0007874227012507617\n",
      "Epoch 30, step 150, loss 1.7342714071273804, rank IC 0.004904532339423895\n",
      "Epoch 30, step 200, loss 1.277720332145691, rank IC -0.0029121863190084696\n",
      "Epoch 30, step 250, loss 1.03363835811615, rank IC 0.008059441111981869\n",
      "Epoch 30, step 300, loss 1.1788969039916992, rank IC 0.001834562630392611\n",
      "Epoch 30, step 350, loss 1.7651782035827637, rank IC -0.0028207304421812296\n",
      "Epoch 31, step 0, loss 1.0207586288452148, rank IC -0.004387475084513426\n",
      "Epoch 31, step 50, loss 1.3891808986663818, rank IC 0.0014609592035412788\n",
      "Epoch 31, step 100, loss 0.9063239693641663, rank IC -0.0008283753995783627\n",
      "Epoch 31, step 150, loss 1.7346347570419312, rank IC 0.005013299640268087\n",
      "Epoch 31, step 200, loss 1.3319040536880493, rank IC -0.0038377000018954277\n",
      "Epoch 31, step 250, loss 1.0331215858459473, rank IC 0.008137951605021954\n",
      "Epoch 31, step 300, loss 1.1784943342208862, rank IC 0.0018137427978217602\n",
      "Epoch 31, step 350, loss 1.7654662132263184, rank IC -0.0028361359145492315\n",
      "Epoch 32, step 0, loss 1.020516276359558, rank IC -0.004443473648279905\n",
      "Epoch 32, step 50, loss 1.389046549797058, rank IC 0.0013691760832443833\n",
      "Epoch 32, step 100, loss 0.9061211943626404, rank IC -0.0009109548409469426\n",
      "Epoch 32, step 150, loss 1.7341727018356323, rank IC 0.005085008684545755\n",
      "Epoch 32, step 200, loss 1.3061270713806152, rank IC -0.0034927958622574806\n",
      "Epoch 32, step 250, loss 1.032595157623291, rank IC 0.00818413682281971\n",
      "Epoch 32, step 300, loss 1.1773638725280762, rank IC 0.0018423493020236492\n",
      "Epoch 32, step 350, loss 1.7764664888381958, rank IC -0.0028709955513477325\n",
      "Epoch 33, step 0, loss 1.0152051448822021, rank IC -0.004489447921514511\n",
      "Epoch 33, step 50, loss 1.3888821601867676, rank IC 0.0017300726613029838\n",
      "Epoch 33, step 100, loss 0.9008634686470032, rank IC -0.0012452820083126426\n",
      "Epoch 33, step 150, loss 1.7343796491622925, rank IC 0.00516462093219161\n",
      "Epoch 33, step 200, loss 1.2735148668289185, rank IC -0.0021401308476924896\n",
      "Epoch 33, step 250, loss 1.0344970226287842, rank IC 0.00796679686754942\n",
      "Epoch 33, step 300, loss 1.1777673959732056, rank IC 0.0018518208526074886\n",
      "Epoch 33, step 350, loss 1.7701425552368164, rank IC -0.00292120105586946\n",
      "Epoch 34, step 0, loss 1.0180447101593018, rank IC -0.0045764390379190445\n",
      "Epoch 34, step 50, loss 1.388503074645996, rank IC 0.0017070552567020059\n",
      "Epoch 34, step 100, loss 0.9035017490386963, rank IC -0.001082708709873259\n",
      "Epoch 34, step 150, loss 1.7340049743652344, rank IC 0.0052236649207770824\n",
      "Epoch 34, step 200, loss 1.2778764963150024, rank IC -0.0016183775151148438\n",
      "Epoch 34, step 250, loss 1.0333083868026733, rank IC 0.008117086254060268\n",
      "Epoch 34, step 300, loss 1.1772586107254028, rank IC 0.001917041838169098\n",
      "Epoch 34, step 350, loss 1.7775903940200806, rank IC -0.0029394393786787987\n",
      "Epoch 35, step 0, loss 1.0145177841186523, rank IC -0.004567003343254328\n",
      "Epoch 35, step 50, loss 1.3886641263961792, rank IC 0.001557283685542643\n",
      "Epoch 35, step 100, loss 0.900283694267273, rank IC -0.0011073289206251502\n",
      "Epoch 35, step 150, loss 1.7341270446777344, rank IC 0.005449245218187571\n",
      "Epoch 35, step 200, loss 1.2763689756393433, rank IC -0.001852091052569449\n",
      "Epoch 35, step 250, loss 1.0349524021148682, rank IC 0.008172020316123962\n",
      "Epoch 35, step 300, loss 1.1772558689117432, rank IC 0.0018705412512645125\n",
      "Epoch 35, step 350, loss 1.7730209827423096, rank IC -0.003008320927619934\n",
      "Epoch 36, step 0, loss 1.0166709423065186, rank IC -0.004730572458356619\n",
      "Epoch 36, step 50, loss 1.3881306648254395, rank IC 0.001631130464375019\n",
      "Epoch 36, step 100, loss 0.902184247970581, rank IC -0.0012328317388892174\n",
      "Epoch 36, step 150, loss 1.7335894107818604, rank IC 0.005529948975890875\n",
      "Epoch 36, step 200, loss 1.3230851888656616, rank IC -0.00023740436881780624\n",
      "Epoch 36, step 250, loss 1.0342592000961304, rank IC 0.007895123213529587\n",
      "Epoch 36, step 300, loss 1.1771013736724854, rank IC 0.0020152153447270393\n",
      "Epoch 36, step 350, loss 1.7723857164382935, rank IC -0.003101964481174946\n",
      "Epoch 37, step 0, loss 1.016843557357788, rank IC -0.004736385773867369\n",
      "Epoch 37, step 50, loss 1.3879504203796387, rank IC 0.001487906090915203\n",
      "Epoch 37, step 100, loss 0.9022324085235596, rank IC -0.0010530591243878007\n",
      "Epoch 37, step 150, loss 1.732909917831421, rank IC 0.005489896517246962\n",
      "Epoch 37, step 200, loss 1.3196673393249512, rank IC 0.0005042969132773578\n",
      "Epoch 37, step 250, loss 1.0328251123428345, rank IC 0.008001911453902721\n",
      "Epoch 37, step 300, loss 1.1768008470535278, rank IC 0.002094909781590104\n",
      "Epoch 37, step 350, loss 1.7722772359848022, rank IC -0.003144260495901108\n",
      "Epoch 38, step 0, loss 1.016726016998291, rank IC -0.004846558440476656\n",
      "Epoch 38, step 50, loss 1.3877753019332886, rank IC 0.001749583170749247\n",
      "Epoch 38, step 100, loss 0.9022316336631775, rank IC -0.0013528974959626794\n",
      "Epoch 38, step 150, loss 1.7329245805740356, rank IC 0.005586595740169287\n",
      "Epoch 38, step 200, loss 1.2753078937530518, rank IC 0.000902216590475291\n",
      "Epoch 38, step 250, loss 1.032064437866211, rank IC 0.008078700862824917\n",
      "Epoch 38, step 300, loss 1.17652428150177, rank IC 0.002079383237287402\n",
      "Epoch 38, step 350, loss 1.7724121809005737, rank IC -0.0031615772750228643\n",
      "Epoch 39, step 0, loss 1.016701579093933, rank IC -0.004895559977740049\n",
      "Epoch 39, step 50, loss 1.3877081871032715, rank IC 0.001861241995356977\n",
      "Epoch 39, step 100, loss 0.9020338654518127, rank IC -0.0013113496825098991\n",
      "Epoch 39, step 150, loss 1.7332267761230469, rank IC 0.005665830802172422\n",
      "Epoch 39, step 200, loss 1.2465652227401733, rank IC 0.001228026463650167\n",
      "Epoch 39, step 250, loss 1.0316106081008911, rank IC 0.008168728090822697\n",
      "Epoch 39, step 300, loss 1.176274061203003, rank IC 0.002079900586977601\n",
      "Epoch 39, step 350, loss 1.7731484174728394, rank IC -0.003171762451529503\n",
      "Epoch 40, step 0, loss 1.0162923336029053, rank IC -0.004994422197341919\n",
      "Epoch 40, step 50, loss 1.3876248598098755, rank IC 0.0018346169963479042\n",
      "Epoch 40, step 100, loss 0.9018335342407227, rank IC -0.0012749253073707223\n",
      "Epoch 40, step 150, loss 1.7336640357971191, rank IC 0.005704199429601431\n",
      "Epoch 40, step 200, loss 1.2672162055969238, rank IC 0.0008615730330348015\n",
      "Epoch 40, step 250, loss 1.0315767526626587, rank IC 0.007864153012633324\n",
      "Epoch 40, step 300, loss 1.176008939743042, rank IC 0.002040870487689972\n",
      "Epoch 40, step 350, loss 1.77314031124115, rank IC -0.003175226738676429\n",
      "Epoch 41, step 0, loss 1.0162302255630493, rank IC -0.004976045340299606\n",
      "Epoch 41, step 50, loss 1.3877512216567993, rank IC 0.002006550319492817\n",
      "Epoch 41, step 100, loss 0.9018588662147522, rank IC -0.0012759361416101456\n",
      "Epoch 41, step 150, loss 1.7343908548355103, rank IC 0.005802284926176071\n",
      "Epoch 41, step 200, loss 1.2607793807983398, rank IC 0.0017975220689550042\n",
      "Epoch 41, step 250, loss 1.0310561656951904, rank IC 0.007981093600392342\n",
      "Epoch 41, step 300, loss 1.175644040107727, rank IC 0.002095587085932493\n",
      "Epoch 41, step 350, loss 1.7732383012771606, rank IC -0.003201700747013092\n",
      "Epoch 42, step 0, loss 1.0161192417144775, rank IC -0.0050613656640052795\n",
      "Epoch 42, step 50, loss 1.3875443935394287, rank IC 0.001952766440808773\n",
      "Epoch 42, step 100, loss 0.9017009735107422, rank IC -0.0013503767549991608\n",
      "Epoch 42, step 150, loss 1.7342071533203125, rank IC 0.0057669878005981445\n",
      "Epoch 42, step 200, loss 1.2527320384979248, rank IC 0.0011389218270778656\n",
      "Epoch 42, step 250, loss 1.0307354927062988, rank IC 0.00807539839297533\n",
      "Epoch 42, step 300, loss 1.175439476966858, rank IC 0.002023402601480484\n",
      "Epoch 42, step 350, loss 1.7729811668395996, rank IC -0.0032589968759566545\n",
      "Epoch 43, step 0, loss 1.016032099723816, rank IC -0.00514986552298069\n",
      "Epoch 43, step 50, loss 1.3874545097351074, rank IC 0.0018293311586603522\n",
      "Epoch 43, step 100, loss 0.9016178250312805, rank IC -0.0011641014134511352\n",
      "Epoch 43, step 150, loss 1.7341071367263794, rank IC 0.005797477439045906\n",
      "Epoch 43, step 200, loss 1.2543041706085205, rank IC 0.001423369161784649\n",
      "Epoch 43, step 250, loss 1.03003990650177, rank IC 0.0081072598695755\n",
      "Epoch 43, step 300, loss 1.1752490997314453, rank IC 0.002061455976217985\n",
      "Epoch 43, step 350, loss 1.7732961177825928, rank IC -0.003325246972963214\n",
      "Epoch 44, step 0, loss 1.0159366130828857, rank IC -0.005247128661721945\n",
      "Epoch 44, step 50, loss 1.3872630596160889, rank IC 0.0019411888206377625\n",
      "Epoch 44, step 100, loss 0.9011358618736267, rank IC -0.001269460073672235\n",
      "Epoch 44, step 150, loss 1.7348052263259888, rank IC 0.005873613525182009\n",
      "Epoch 44, step 200, loss 1.240318775177002, rank IC 0.0024824447464197874\n",
      "Epoch 44, step 250, loss 1.0305224657058716, rank IC 0.008247056044638157\n",
      "Epoch 44, step 300, loss 1.1750038862228394, rank IC 0.0020524957217276096\n",
      "Epoch 44, step 350, loss 1.7728403806686401, rank IC -0.003347230376675725\n",
      "Epoch 45, step 0, loss 1.0159401893615723, rank IC -0.00536164129152894\n",
      "Epoch 45, step 50, loss 1.3870937824249268, rank IC 0.002004799200221896\n",
      "Epoch 45, step 100, loss 0.9015382528305054, rank IC -0.0012256782501935959\n",
      "Epoch 45, step 150, loss 1.7346656322479248, rank IC 0.00591672770678997\n",
      "Epoch 45, step 200, loss 1.258646845817566, rank IC 0.002579423598945141\n",
      "Epoch 45, step 250, loss 1.0296250581741333, rank IC 0.008205189369618893\n",
      "Epoch 45, step 300, loss 1.1746984720230103, rank IC 0.002041693776845932\n",
      "Epoch 45, step 350, loss 1.7733917236328125, rank IC -0.003395000472664833\n",
      "Epoch 46, step 0, loss 1.0156474113464355, rank IC -0.005440481007099152\n",
      "Epoch 46, step 50, loss 1.3869024515151978, rank IC 0.0023474462796002626\n",
      "Epoch 46, step 100, loss 0.901208221912384, rank IC -0.001580923213623464\n",
      "Epoch 46, step 150, loss 1.7345433235168457, rank IC 0.005827882792800665\n",
      "Epoch 46, step 200, loss 1.2594491243362427, rank IC 0.003512512193992734\n",
      "Epoch 46, step 250, loss 1.0292497873306274, rank IC 0.00829344429075718\n",
      "Epoch 46, step 300, loss 1.1744847297668457, rank IC 0.002066762186586857\n",
      "Epoch 46, step 350, loss 1.7737541198730469, rank IC -0.0034451789688318968\n",
      "Epoch 47, step 0, loss 1.0154370069503784, rank IC -0.005598863121122122\n",
      "Epoch 47, step 50, loss 1.3867067098617554, rank IC 0.002400805242359638\n",
      "Epoch 47, step 100, loss 0.9004866480827332, rank IC -0.002136239781975746\n",
      "Epoch 47, step 150, loss 1.7346807718276978, rank IC 0.005886423867195845\n",
      "Epoch 47, step 200, loss 1.2689398527145386, rank IC 0.004108445253223181\n",
      "Epoch 47, step 250, loss 1.029740571975708, rank IC 0.008395210839807987\n",
      "Epoch 47, step 300, loss 1.1743029356002808, rank IC 0.0020779368933290243\n",
      "Epoch 47, step 350, loss 1.7735557556152344, rank IC -0.0035682127345353365\n",
      "Epoch 48, step 0, loss 1.0153900384902954, rank IC -0.005683384370058775\n",
      "Epoch 48, step 50, loss 1.3866889476776123, rank IC 0.0023530314210802317\n",
      "Epoch 48, step 100, loss 0.9009364247322083, rank IC -0.001660547568462789\n",
      "Epoch 48, step 150, loss 1.7350507974624634, rank IC 0.00589826749637723\n",
      "Epoch 48, step 200, loss 1.2304294109344482, rank IC 0.004133334383368492\n",
      "Epoch 48, step 250, loss 1.0291063785552979, rank IC 0.008463064208626747\n",
      "Epoch 48, step 300, loss 1.1739535331726074, rank IC 0.002079984173178673\n",
      "Epoch 48, step 350, loss 1.774261474609375, rank IC -0.0035966073628515005\n",
      "Epoch 49, step 0, loss 1.0150666236877441, rank IC -0.0057593523524701595\n",
      "Epoch 49, step 50, loss 1.386500597000122, rank IC 0.00262886262498796\n",
      "Epoch 49, step 100, loss 0.9005950093269348, rank IC -0.001903232536278665\n",
      "Epoch 49, step 150, loss 1.7353758811950684, rank IC 0.005795528646558523\n",
      "Epoch 49, step 200, loss 1.2664897441864014, rank IC 0.004921677988022566\n",
      "Epoch 49, step 250, loss 1.029380440711975, rank IC 0.008532264269888401\n",
      "Epoch 49, step 300, loss 1.1737948656082153, rank IC 0.002074292628094554\n",
      "Epoch 49, step 350, loss 1.7746574878692627, rank IC -0.003621452720835805\n",
      "Epoch 50, step 0, loss 1.01479172706604, rank IC -0.005803199484944344\n",
      "Epoch 50, step 50, loss 1.3862961530685425, rank IC 0.003094030311331153\n",
      "Epoch 50, step 100, loss 0.9001660943031311, rank IC -0.0022838388103991747\n",
      "Epoch 50, step 150, loss 1.7356752157211304, rank IC 0.005764992441982031\n",
      "Epoch 50, step 200, loss 1.250841498374939, rank IC 0.004989448469132185\n",
      "Epoch 50, step 250, loss 1.029424786567688, rank IC 0.008580596186220646\n",
      "Epoch 50, step 300, loss 1.173585057258606, rank IC 0.0020411277655512094\n",
      "Epoch 50, step 350, loss 1.7751023769378662, rank IC -0.0036732982844114304\n",
      "Epoch 51, step 0, loss 1.014569640159607, rank IC -0.00596354715526104\n",
      "Epoch 51, step 50, loss 1.3860923051834106, rank IC 0.003347969613969326\n",
      "Epoch 51, step 100, loss 0.9000797867774963, rank IC -0.0024049319326877594\n",
      "Epoch 51, step 150, loss 1.7354130744934082, rank IC 0.00584622286260128\n",
      "Epoch 51, step 200, loss 1.2551623582839966, rank IC 0.004959468264132738\n",
      "Epoch 51, step 250, loss 1.0295522212982178, rank IC 0.008714541792869568\n",
      "Epoch 51, step 300, loss 1.1733348369598389, rank IC 0.002034640172496438\n",
      "Epoch 51, step 350, loss 1.7752490043640137, rank IC -0.0037906921934336424\n",
      "Epoch 52, step 0, loss 1.0144391059875488, rank IC -0.006003631744533777\n",
      "Epoch 52, step 50, loss 1.3858935832977295, rank IC 0.003433787263929844\n",
      "Epoch 52, step 100, loss 0.8999144434928894, rank IC -0.0024587211664766073\n",
      "Epoch 52, step 150, loss 1.73513925075531, rank IC 0.005912976339459419\n",
      "Epoch 52, step 200, loss 1.234536051750183, rank IC 0.005113487597554922\n",
      "Epoch 52, step 250, loss 1.0289849042892456, rank IC 0.008752684108912945\n",
      "Epoch 52, step 300, loss 1.1730449199676514, rank IC 0.0020276245195418596\n",
      "Epoch 52, step 350, loss 1.7755794525146484, rank IC -0.003727175295352936\n",
      "Epoch 53, step 0, loss 1.0141409635543823, rank IC -0.006074643228203058\n",
      "Epoch 53, step 50, loss 1.3858193159103394, rank IC 0.0036674027796834707\n",
      "Epoch 53, step 100, loss 0.8996654748916626, rank IC -0.0025474342983216047\n",
      "Epoch 53, step 150, loss 1.7357255220413208, rank IC 0.005915060639381409\n",
      "Epoch 53, step 200, loss 1.2867751121520996, rank IC 0.00559915229678154\n",
      "Epoch 53, step 250, loss 1.0293653011322021, rank IC 0.008786280639469624\n",
      "Epoch 53, step 300, loss 1.1728183031082153, rank IC 0.002120004966855049\n",
      "Epoch 53, step 350, loss 1.7789793014526367, rank IC -0.003875873750075698\n",
      "Epoch 54, step 0, loss 1.012394666671753, rank IC -0.006099706050008535\n",
      "Epoch 54, step 50, loss 1.3852839469909668, rank IC 0.0038356368895620108\n",
      "Epoch 54, step 100, loss 0.8977091908454895, rank IC -0.002768952399492264\n",
      "Epoch 54, step 150, loss 1.7362021207809448, rank IC 0.005978438537567854\n",
      "Epoch 54, step 200, loss 1.2661004066467285, rank IC 0.005402900278568268\n",
      "Epoch 54, step 250, loss 1.0303292274475098, rank IC 0.008975807577371597\n",
      "Epoch 54, step 300, loss 1.1724475622177124, rank IC 0.0021327435970306396\n",
      "Epoch 54, step 350, loss 1.7772023677825928, rank IC -0.0041052717715501785\n",
      "Epoch 55, step 0, loss 1.0132527351379395, rank IC -0.0063254632987082005\n",
      "Epoch 55, step 50, loss 1.3850855827331543, rank IC 0.003979347180575132\n",
      "Epoch 55, step 100, loss 0.8985347747802734, rank IC -0.002926870482042432\n",
      "Epoch 55, step 150, loss 1.7357255220413208, rank IC 0.0059204138815402985\n",
      "Epoch 55, step 200, loss 1.2276979684829712, rank IC 0.00576530396938324\n",
      "Epoch 55, step 250, loss 1.0297095775604248, rank IC 0.009076732210814953\n",
      "Epoch 55, step 300, loss 1.1722655296325684, rank IC 0.002124678110703826\n",
      "Epoch 55, step 350, loss 1.7770116329193115, rank IC -0.003995812963694334\n",
      "Epoch 56, step 0, loss 1.0131222009658813, rank IC -0.006264371331781149\n",
      "Epoch 56, step 50, loss 1.3850295543670654, rank IC 0.0040014577098190784\n",
      "Epoch 56, step 100, loss 0.8983195424079895, rank IC -0.0028943244833499193\n",
      "Epoch 56, step 150, loss 1.7356841564178467, rank IC 0.005854420829564333\n",
      "Epoch 56, step 200, loss 1.2485541105270386, rank IC 0.006226330995559692\n",
      "Epoch 56, step 250, loss 1.0294214487075806, rank IC 0.009221640415489674\n",
      "Epoch 56, step 300, loss 1.1720346212387085, rank IC 0.002130101202055812\n",
      "Epoch 56, step 350, loss 1.776710867881775, rank IC -0.004046312067657709\n",
      "Epoch 57, step 0, loss 1.0132203102111816, rank IC -0.006420508027076721\n",
      "Epoch 57, step 50, loss 1.3849072456359863, rank IC 0.003978562541306019\n",
      "Epoch 57, step 100, loss 0.8984233736991882, rank IC -0.0031087419483810663\n",
      "Epoch 57, step 150, loss 1.7360198497772217, rank IC 0.0058775972574949265\n",
      "Epoch 57, step 200, loss 1.2343261241912842, rank IC 0.006382359657436609\n",
      "Epoch 57, step 250, loss 1.028631567955017, rank IC 0.009284541942179203\n",
      "Epoch 57, step 300, loss 1.171843409538269, rank IC 0.002162683056667447\n",
      "Epoch 57, step 350, loss 1.7766940593719482, rank IC -0.004197516944259405\n",
      "Epoch 58, step 0, loss 1.0130908489227295, rank IC -0.006520070135593414\n",
      "Epoch 58, step 50, loss 1.3847424983978271, rank IC 0.004229431506246328\n",
      "Epoch 58, step 100, loss 0.8981294631958008, rank IC -0.0031562596559524536\n",
      "Epoch 58, step 150, loss 1.7365520000457764, rank IC 0.005843108519911766\n",
      "Epoch 58, step 200, loss 1.2683305740356445, rank IC 0.006386765744537115\n",
      "Epoch 58, step 250, loss 1.0288134813308716, rank IC 0.009322515688836575\n",
      "Epoch 58, step 300, loss 1.1715902090072632, rank IC 0.002110587665811181\n",
      "Epoch 58, step 350, loss 1.7765777111053467, rank IC -0.004205342847853899\n",
      "Epoch 59, step 0, loss 1.0130308866500854, rank IC -0.006647860165685415\n",
      "Epoch 59, step 50, loss 1.3845863342285156, rank IC 0.004099295474588871\n",
      "Epoch 59, step 100, loss 0.8980711102485657, rank IC -0.0030921297147870064\n",
      "Epoch 59, step 150, loss 1.7364251613616943, rank IC 0.005901153665035963\n",
      "Epoch 59, step 200, loss 1.2869480848312378, rank IC 0.005972524639219046\n",
      "Epoch 59, step 250, loss 1.0282191038131714, rank IC 0.009408984333276749\n",
      "Epoch 59, step 300, loss 1.1714080572128296, rank IC 0.0020926743745803833\n",
      "Epoch 59, step 350, loss 1.7760897874832153, rank IC -0.004253301303833723\n",
      "Epoch 60, step 0, loss 1.0130184888839722, rank IC -0.006706657353788614\n",
      "Epoch 60, step 50, loss 1.3842823505401611, rank IC 0.00441448949277401\n",
      "Epoch 60, step 100, loss 0.8980472087860107, rank IC -0.003312212647870183\n",
      "Epoch 60, step 150, loss 1.736374855041504, rank IC 0.005875627044588327\n",
      "Epoch 60, step 200, loss 1.2167860269546509, rank IC 0.006413281429558992\n",
      "Epoch 60, step 250, loss 1.0275546312332153, rank IC 0.009493288584053516\n",
      "Epoch 60, step 300, loss 1.1710855960845947, rank IC 0.0020882776007056236\n",
      "Epoch 60, step 350, loss 1.7791732549667358, rank IC -0.004320046398788691\n",
      "Epoch 61, step 0, loss 1.011454463005066, rank IC -0.006812382489442825\n",
      "Epoch 61, step 50, loss 1.3839153051376343, rank IC 0.004500166047364473\n",
      "Epoch 61, step 100, loss 0.8960217833518982, rank IC -0.0036392640322446823\n",
      "Epoch 61, step 150, loss 1.7359864711761475, rank IC 0.006032010540366173\n",
      "Epoch 61, step 200, loss 1.2428467273712158, rank IC 0.006470737978816032\n",
      "Epoch 61, step 250, loss 1.0282567739486694, rank IC 0.009542548097670078\n",
      "Epoch 61, step 300, loss 1.1707826852798462, rank IC 0.0021689666900783777\n",
      "Epoch 61, step 350, loss 1.7771220207214355, rank IC -0.004364732187241316\n",
      "Epoch 62, step 0, loss 1.0122394561767578, rank IC -0.006969798356294632\n",
      "Epoch 62, step 50, loss 1.3838051557540894, rank IC 0.004834902007132769\n",
      "Epoch 62, step 100, loss 0.8970106840133667, rank IC -0.00365541223436594\n",
      "Epoch 62, step 150, loss 1.736323595046997, rank IC 0.006190438289195299\n",
      "Epoch 62, step 200, loss 1.2313804626464844, rank IC 0.006888723000884056\n",
      "Epoch 62, step 250, loss 1.0276896953582764, rank IC 0.009647782891988754\n",
      "Epoch 62, step 300, loss 1.1706969738006592, rank IC 0.002208624966442585\n",
      "Epoch 62, step 350, loss 1.7767338752746582, rank IC -0.004574145190417767\n",
      "Epoch 63, step 0, loss 1.0123783349990845, rank IC -0.007046472281217575\n",
      "Epoch 63, step 50, loss 1.383641242980957, rank IC 0.0048492602072656155\n",
      "Epoch 63, step 100, loss 0.8970094919204712, rank IC -0.003672796068713069\n",
      "Epoch 63, step 150, loss 1.7362686395645142, rank IC 0.006115090101957321\n",
      "Epoch 63, step 200, loss 1.2415214776992798, rank IC 0.007049421314150095\n",
      "Epoch 63, step 250, loss 1.0271745920181274, rank IC 0.009674668312072754\n",
      "Epoch 63, step 300, loss 1.170451283454895, rank IC 0.0021830324549227953\n",
      "Epoch 63, step 350, loss 1.7763698101043701, rank IC -0.004530127160251141\n",
      "Epoch 64, step 0, loss 1.0123634338378906, rank IC -0.0071622468531131744\n",
      "Epoch 64, step 50, loss 1.3834631443023682, rank IC 0.00501791387796402\n",
      "Epoch 64, step 100, loss 0.8969075083732605, rank IC -0.0037826048210263252\n",
      "Epoch 64, step 150, loss 1.7361143827438354, rank IC 0.0061859083361923695\n",
      "Epoch 64, step 200, loss 1.2295622825622559, rank IC 0.007518175523728132\n",
      "Epoch 64, step 250, loss 1.0268208980560303, rank IC 0.00973128154873848\n",
      "Epoch 64, step 300, loss 1.1702501773834229, rank IC 0.0021190147381275892\n",
      "Epoch 64, step 350, loss 1.776403546333313, rank IC -0.004575887694954872\n",
      "Epoch 65, step 0, loss 1.0123000144958496, rank IC -0.007236973848193884\n",
      "Epoch 65, step 50, loss 1.383357286453247, rank IC 0.0051291766576468945\n",
      "Epoch 65, step 100, loss 0.8968137502670288, rank IC -0.0038204912561923265\n",
      "Epoch 65, step 150, loss 1.7367281913757324, rank IC 0.006197003182023764\n",
      "Epoch 65, step 200, loss 1.2380588054656982, rank IC 0.007463503163307905\n",
      "Epoch 65, step 250, loss 1.0266506671905518, rank IC 0.009861170314252377\n",
      "Epoch 65, step 300, loss 1.1700353622436523, rank IC 0.002118823118507862\n",
      "Epoch 65, step 350, loss 1.7760615348815918, rank IC -0.0046100071631371975\n",
      "Epoch 66, step 0, loss 1.0123496055603027, rank IC -0.007327236235141754\n",
      "Epoch 66, step 50, loss 1.3833547830581665, rank IC 0.00543496198952198\n",
      "Epoch 66, step 100, loss 0.8966557383537292, rank IC -0.004019619897007942\n",
      "Epoch 66, step 150, loss 1.7362011671066284, rank IC 0.00626414455473423\n",
      "Epoch 66, step 200, loss 1.3885430097579956, rank IC 0.008035884238779545\n",
      "Epoch 66, step 250, loss 1.026450753211975, rank IC 0.010023035109043121\n",
      "Epoch 66, step 300, loss 1.1699068546295166, rank IC 0.002164553152397275\n",
      "Epoch 66, step 350, loss 1.7756826877593994, rank IC -0.004766630940139294\n",
      "Epoch 67, step 0, loss 1.012446403503418, rank IC -0.007387008052319288\n",
      "Epoch 67, step 50, loss 1.3830646276474, rank IC 0.005504538770765066\n",
      "Epoch 67, step 100, loss 0.8967102766036987, rank IC -0.004123568069189787\n",
      "Epoch 67, step 150, loss 1.7356607913970947, rank IC 0.006283971015363932\n",
      "Epoch 67, step 200, loss 1.243743658065796, rank IC 0.008097822777926922\n",
      "Epoch 67, step 250, loss 1.0257587432861328, rank IC 0.010021698661148548\n",
      "Epoch 67, step 300, loss 1.169414758682251, rank IC 0.002110936911776662\n",
      "Epoch 67, step 350, loss 1.7791228294372559, rank IC -0.0046859788708388805\n",
      "Epoch 68, step 0, loss 1.0105684995651245, rank IC -0.00748573848977685\n",
      "Epoch 68, step 50, loss 1.382555365562439, rank IC 0.005541577935218811\n",
      "Epoch 68, step 100, loss 0.8943920135498047, rank IC -0.0042018042877316475\n",
      "Epoch 68, step 150, loss 1.7359768152236938, rank IC 0.0063171046786010265\n",
      "Epoch 68, step 200, loss 1.2293199300765991, rank IC 0.008081451058387756\n",
      "Epoch 68, step 250, loss 1.0262725353240967, rank IC 0.010017716325819492\n",
      "Epoch 68, step 300, loss 1.1693073511123657, rank IC 0.0019977421034127474\n",
      "Epoch 68, step 350, loss 1.7764602899551392, rank IC -0.004621730651706457\n",
      "Epoch 69, step 0, loss 1.0116040706634521, rank IC -0.007519105914980173\n",
      "Epoch 69, step 50, loss 1.3823941946029663, rank IC 0.00573576008901\n",
      "Epoch 69, step 100, loss 0.8956931829452515, rank IC -0.004281718749552965\n",
      "Epoch 69, step 150, loss 1.7357676029205322, rank IC 0.006411819253116846\n",
      "Epoch 69, step 200, loss 1.2166759967803955, rank IC 0.008179348893463612\n",
      "Epoch 69, step 250, loss 1.0257229804992676, rank IC 0.010097871534526348\n",
      "Epoch 69, step 300, loss 1.1690385341644287, rank IC 0.002055165357887745\n",
      "Epoch 69, step 350, loss 1.7797750234603882, rank IC -0.004739235155284405\n",
      "Epoch 70, step 0, loss 1.0100888013839722, rank IC -0.007638480979949236\n",
      "Epoch 70, step 50, loss 1.3822041749954224, rank IC 0.005766618996858597\n",
      "Epoch 70, step 100, loss 0.8939223289489746, rank IC -0.004225858021527529\n",
      "Epoch 70, step 150, loss 1.7365784645080566, rank IC 0.006501469761133194\n",
      "Epoch 70, step 200, loss 1.2255841493606567, rank IC 0.008298391476273537\n",
      "Epoch 70, step 250, loss 1.0263644456863403, rank IC 0.010162901133298874\n",
      "Epoch 70, step 300, loss 1.1688519716262817, rank IC 0.0020424106623977423\n",
      "Epoch 70, step 350, loss 1.777147889137268, rank IC -0.004801311995834112\n",
      "Epoch 71, step 0, loss 1.0110797882080078, rank IC -0.007687676697969437\n",
      "Epoch 71, step 50, loss 1.3818957805633545, rank IC 0.005906528327614069\n",
      "Epoch 71, step 100, loss 0.8948127627372742, rank IC -0.004374275449663401\n",
      "Epoch 71, step 150, loss 1.736435890197754, rank IC 0.0066355150192976\n",
      "Epoch 71, step 200, loss 1.2189396619796753, rank IC 0.008458628319203854\n",
      "Epoch 71, step 250, loss 1.025894284248352, rank IC 0.010291610844433308\n",
      "Epoch 71, step 300, loss 1.1686999797821045, rank IC 0.0021028744522482157\n",
      "Epoch 71, step 350, loss 1.777079701423645, rank IC -0.004922236315906048\n",
      "Epoch 72, step 0, loss 1.0111339092254639, rank IC -0.007848317734897137\n",
      "Epoch 72, step 50, loss 1.3818912506103516, rank IC 0.006083179730921984\n",
      "Epoch 72, step 100, loss 0.8948314785957336, rank IC -0.004405103623867035\n",
      "Epoch 72, step 150, loss 1.7364643812179565, rank IC 0.00666687311604619\n",
      "Epoch 72, step 200, loss 1.3493947982788086, rank IC 0.008522084914147854\n",
      "Epoch 72, step 250, loss 1.0255353450775146, rank IC 0.010320445522665977\n",
      "Epoch 72, step 300, loss 1.16861093044281, rank IC 0.0020108160097151995\n",
      "Epoch 72, step 350, loss 1.7764005661010742, rank IC -0.004857536870986223\n",
      "Epoch 73, step 0, loss 1.0112868547439575, rank IC -0.007818177342414856\n",
      "Epoch 73, step 50, loss 1.3816285133361816, rank IC 0.00634731026366353\n",
      "Epoch 73, step 100, loss 0.8946940302848816, rank IC -0.004516806919127703\n",
      "Epoch 73, step 150, loss 1.736190915107727, rank IC 0.006688721943646669\n",
      "Epoch 73, step 200, loss 1.2327845096588135, rank IC 0.00874475110322237\n",
      "Epoch 73, step 250, loss 1.0252195596694946, rank IC 0.010491183958947659\n",
      "Epoch 73, step 300, loss 1.1683473587036133, rank IC 0.002043440705165267\n",
      "Epoch 73, step 350, loss 1.7769222259521484, rank IC -0.004898065235465765\n",
      "Epoch 74, step 0, loss 1.0108973979949951, rank IC -0.007996467873454094\n",
      "Epoch 74, step 50, loss 1.3813215494155884, rank IC 0.006478577386587858\n",
      "Epoch 74, step 100, loss 0.8940444588661194, rank IC -0.004712681286036968\n",
      "Epoch 74, step 150, loss 1.7363998889923096, rank IC 0.0067747789435088634\n",
      "Epoch 74, step 200, loss 1.2066982984542847, rank IC 0.00911864172667265\n",
      "Epoch 74, step 250, loss 1.0250076055526733, rank IC 0.010516182519495487\n",
      "Epoch 74, step 300, loss 1.168121337890625, rank IC 0.002018169267103076\n",
      "Epoch 74, step 350, loss 1.7769715785980225, rank IC -0.0049606505781412125\n",
      "Epoch 75, step 0, loss 1.0108941793441772, rank IC -0.007969469763338566\n",
      "Epoch 75, step 50, loss 1.381301999092102, rank IC 0.006701413542032242\n",
      "Epoch 75, step 100, loss 0.894176721572876, rank IC -0.004813490901142359\n",
      "Epoch 75, step 150, loss 1.7364009618759155, rank IC 0.00682263495400548\n",
      "Epoch 75, step 200, loss 1.212960124015808, rank IC 0.00928666815161705\n",
      "Epoch 75, step 250, loss 1.025134801864624, rank IC 0.010601705871522427\n",
      "Epoch 75, step 300, loss 1.1680186986923218, rank IC 0.0020260890014469624\n",
      "Epoch 75, step 350, loss 1.7770518064498901, rank IC -0.005021762568503618\n",
      "Epoch 76, step 0, loss 1.0107684135437012, rank IC -0.00812146533280611\n",
      "Epoch 76, step 50, loss 1.3810282945632935, rank IC 0.006616357248276472\n",
      "Epoch 76, step 100, loss 0.8941804766654968, rank IC -0.0048761828802526\n",
      "Epoch 76, step 150, loss 1.737022042274475, rank IC 0.00670720636844635\n",
      "Epoch 76, step 200, loss 1.2197959423065186, rank IC 0.008876527659595013\n",
      "Epoch 76, step 250, loss 1.0252677202224731, rank IC 0.010653510689735413\n",
      "Epoch 76, step 300, loss 1.167767882347107, rank IC 0.0020708206575363874\n",
      "Epoch 76, step 350, loss 1.7773857116699219, rank IC -0.0050771646201610565\n",
      "Epoch 77, step 0, loss 1.0105822086334229, rank IC -0.008243868127465248\n",
      "Epoch 77, step 50, loss 1.3808515071868896, rank IC 0.006874460726976395\n",
      "Epoch 77, step 100, loss 0.8938887715339661, rank IC -0.004911718424409628\n",
      "Epoch 77, step 150, loss 1.736606240272522, rank IC 0.006758255884051323\n",
      "Epoch 77, step 200, loss 1.2868494987487793, rank IC 0.009557383134961128\n",
      "Epoch 77, step 250, loss 1.0251883268356323, rank IC 0.010749326087534428\n",
      "Epoch 77, step 300, loss 1.167602777481079, rank IC 0.002070815535262227\n",
      "Epoch 77, step 350, loss 1.7770575284957886, rank IC -0.0051348102279007435\n",
      "Epoch 78, step 0, loss 1.0106126070022583, rank IC -0.008203111588954926\n",
      "Epoch 78, step 50, loss 1.3806986808776855, rank IC 0.007185718510299921\n",
      "Epoch 78, step 100, loss 0.8936787843704224, rank IC -0.005039925687015057\n",
      "Epoch 78, step 150, loss 1.7355425357818604, rank IC 0.0068177878856658936\n",
      "Epoch 78, step 200, loss 1.2549179792404175, rank IC 0.009486700408160686\n",
      "Epoch 78, step 250, loss 1.0246723890304565, rank IC 0.010766803286969662\n",
      "Epoch 78, step 300, loss 1.16743803024292, rank IC 0.0020492146722972393\n",
      "Epoch 78, step 350, loss 1.7769911289215088, rank IC -0.00516861118376255\n",
      "Epoch 79, step 0, loss 1.010445475578308, rank IC -0.008314340375363827\n",
      "Epoch 79, step 50, loss 1.3806344270706177, rank IC 0.007102427538484335\n",
      "Epoch 79, step 100, loss 0.8934589624404907, rank IC -0.005043803248554468\n",
      "Epoch 79, step 150, loss 1.7352242469787598, rank IC 0.006879188120365143\n",
      "Epoch 79, step 200, loss 1.2170623540878296, rank IC 0.009532347321510315\n",
      "Epoch 79, step 250, loss 1.0242195129394531, rank IC 0.010839316993951797\n",
      "Epoch 79, step 300, loss 1.167327642440796, rank IC 0.0020916007924824953\n",
      "Epoch 79, step 350, loss 1.7776068449020386, rank IC -0.00519152544438839\n",
      "Epoch 80, step 0, loss 1.0102719068527222, rank IC -0.00845704972743988\n",
      "Epoch 80, step 50, loss 1.3805105686187744, rank IC 0.006987591739743948\n",
      "Epoch 80, step 100, loss 0.8933387994766235, rank IC -0.004892987199127674\n",
      "Epoch 80, step 150, loss 1.7352542877197266, rank IC 0.006897462531924248\n",
      "Epoch 80, step 200, loss 1.20921790599823, rank IC 0.00976730789989233\n",
      "Epoch 80, step 250, loss 1.024139642715454, rank IC 0.010893571190536022\n",
      "Epoch 80, step 300, loss 1.1670502424240112, rank IC 0.0020693582482635975\n",
      "Epoch 80, step 350, loss 1.7801414728164673, rank IC -0.005241408944129944\n",
      "Epoch 81, step 0, loss 1.009119987487793, rank IC -0.008428719826042652\n",
      "Epoch 81, step 50, loss 1.3801517486572266, rank IC 0.007334747817367315\n",
      "Epoch 81, step 100, loss 0.8918682932853699, rank IC -0.005171837750822306\n",
      "Epoch 81, step 150, loss 1.73617684841156, rank IC 0.0068315062671899796\n",
      "Epoch 81, step 200, loss 1.2331894636154175, rank IC 0.00992878433316946\n",
      "Epoch 81, step 250, loss 1.0247526168823242, rank IC 0.010948941111564636\n",
      "Epoch 81, step 300, loss 1.1668192148208618, rank IC 0.0020760882180184126\n",
      "Epoch 81, step 350, loss 1.7777725458145142, rank IC -0.005264739040285349\n",
      "Epoch 82, step 0, loss 1.0098499059677124, rank IC -0.008537378162145615\n",
      "Epoch 82, step 50, loss 1.3800930976867676, rank IC 0.00730602303519845\n",
      "Epoch 82, step 100, loss 0.8926013708114624, rank IC -0.004981324076652527\n",
      "Epoch 82, step 150, loss 1.7358297109603882, rank IC 0.006760576739907265\n",
      "Epoch 82, step 200, loss 1.2244393825531006, rank IC 0.009791144169867039\n",
      "Epoch 82, step 250, loss 1.0241211652755737, rank IC 0.011001475155353546\n",
      "Epoch 82, step 300, loss 1.1667307615280151, rank IC 0.0020598922856152058\n",
      "Epoch 82, step 350, loss 1.7772902250289917, rank IC -0.005270822439342737\n",
      "Epoch 83, step 0, loss 1.0099382400512695, rank IC -0.008690190501511097\n",
      "Epoch 83, step 50, loss 1.379846453666687, rank IC 0.007301851641386747\n",
      "Epoch 83, step 100, loss 0.8926070332527161, rank IC -0.004927103873342276\n",
      "Epoch 83, step 150, loss 1.7354736328125, rank IC 0.006819052156060934\n",
      "Epoch 83, step 200, loss 1.215366244316101, rank IC 0.010015256702899933\n",
      "Epoch 83, step 250, loss 1.023716926574707, rank IC 0.011074786074459553\n",
      "Epoch 83, step 300, loss 1.1665940284729004, rank IC 0.0020698814187198877\n",
      "Epoch 83, step 350, loss 1.7775211334228516, rank IC -0.0053787752985954285\n",
      "Epoch 84, step 0, loss 1.0098661184310913, rank IC -0.008710571564733982\n",
      "Epoch 84, step 50, loss 1.3797249794006348, rank IC 0.007343920413404703\n",
      "Epoch 84, step 100, loss 0.8924903869628906, rank IC -0.004935730714350939\n",
      "Epoch 84, step 150, loss 1.7356247901916504, rank IC 0.006872078869491816\n",
      "Epoch 84, step 200, loss 1.2212239503860474, rank IC 0.010213281027972698\n",
      "Epoch 84, step 250, loss 1.0234755277633667, rank IC 0.011168996803462505\n",
      "Epoch 84, step 300, loss 1.166464924812317, rank IC 0.0021308131981641054\n",
      "Epoch 84, step 350, loss 1.7768547534942627, rank IC -0.005422886926680803\n",
      "Epoch 85, step 0, loss 1.00999116897583, rank IC -0.008874303661286831\n",
      "Epoch 85, step 50, loss 1.379474401473999, rank IC 0.007458896841853857\n",
      "Epoch 85, step 100, loss 0.8924511075019836, rank IC -0.005022576544433832\n",
      "Epoch 85, step 150, loss 1.7354036569595337, rank IC 0.0069296048022806644\n",
      "Epoch 85, step 200, loss 1.2084871530532837, rank IC 0.010293734259903431\n",
      "Epoch 85, step 250, loss 1.0229368209838867, rank IC 0.011222810484468937\n",
      "Epoch 85, step 300, loss 1.1663177013397217, rank IC 0.0021010867785662413\n",
      "Epoch 85, step 350, loss 1.7770435810089111, rank IC -0.005464888643473387\n",
      "Epoch 86, step 0, loss 1.009954810142517, rank IC -0.008865936659276485\n",
      "Epoch 86, step 50, loss 1.3794479370117188, rank IC 0.007620519492775202\n",
      "Epoch 86, step 100, loss 0.892423689365387, rank IC -0.005200392100960016\n",
      "Epoch 86, step 150, loss 1.735586404800415, rank IC 0.006910365540534258\n",
      "Epoch 86, step 200, loss 1.2752116918563843, rank IC 0.010221492499113083\n",
      "Epoch 86, step 250, loss 1.022656798362732, rank IC 0.011270597577095032\n",
      "Epoch 86, step 300, loss 1.166259765625, rank IC 0.002122317673638463\n",
      "Epoch 86, step 350, loss 1.776511311531067, rank IC -0.005473268683999777\n",
      "Epoch 87, step 0, loss 1.010185718536377, rank IC -0.00902168732136488\n",
      "Epoch 87, step 50, loss 1.3791614770889282, rank IC 0.007706876844167709\n",
      "Epoch 87, step 100, loss 0.8925360441207886, rank IC -0.0051854453049600124\n",
      "Epoch 87, step 150, loss 1.7357326745986938, rank IC 0.0069250669330358505\n",
      "Epoch 87, step 200, loss 1.2313554286956787, rank IC 0.010702583938837051\n",
      "Epoch 87, step 250, loss 1.021715521812439, rank IC 0.011338479816913605\n",
      "Epoch 87, step 300, loss 1.166075587272644, rank IC 0.00210427213460207\n",
      "Epoch 87, step 350, loss 1.7761892080307007, rank IC -0.005485864821821451\n",
      "Epoch 88, step 0, loss 1.0100913047790527, rank IC -0.00902437325567007\n",
      "Epoch 88, step 50, loss 1.3790932893753052, rank IC 0.00782910455018282\n",
      "Epoch 88, step 100, loss 0.8922820687294006, rank IC -0.005278805736452341\n",
      "Epoch 88, step 150, loss 1.7354000806808472, rank IC 0.006907628383487463\n",
      "Epoch 88, step 200, loss 1.2043451070785522, rank IC 0.010496078990399837\n",
      "Epoch 88, step 250, loss 1.0214335918426514, rank IC 0.011394514702260494\n",
      "Epoch 88, step 300, loss 1.16594660282135, rank IC 0.00213706330396235\n",
      "Epoch 88, step 350, loss 1.7765090465545654, rank IC -0.005501456093043089\n",
      "Epoch 89, step 0, loss 1.0099666118621826, rank IC -0.009195228107273579\n",
      "Epoch 89, step 50, loss 1.378991961479187, rank IC 0.00773180415853858\n",
      "Epoch 89, step 100, loss 0.8922253847122192, rank IC -0.005176897626370192\n",
      "Epoch 89, step 150, loss 1.735556721687317, rank IC 0.007013897877186537\n",
      "Epoch 89, step 200, loss 1.2283084392547607, rank IC 0.010787745006382465\n",
      "Epoch 89, step 250, loss 1.0214245319366455, rank IC 0.011451554484665394\n",
      "Epoch 89, step 300, loss 1.165771484375, rank IC 0.002142875688150525\n",
      "Epoch 89, step 350, loss 1.7762386798858643, rank IC -0.005561586003750563\n",
      "Epoch 90, step 0, loss 1.0099619626998901, rank IC -0.009159675799310207\n",
      "Epoch 90, step 50, loss 1.3788173198699951, rank IC 0.008044742047786713\n",
      "Epoch 90, step 100, loss 0.8921678066253662, rank IC -0.005436968058347702\n",
      "Epoch 90, step 150, loss 1.7364171743392944, rank IC 0.0069528184831142426\n",
      "Epoch 90, step 200, loss 1.2084990739822388, rank IC 0.010815370827913284\n",
      "Epoch 90, step 250, loss 1.0213273763656616, rank IC 0.011556201614439487\n",
      "Epoch 90, step 300, loss 1.1656742095947266, rank IC 0.0021481590811163187\n",
      "Epoch 90, step 350, loss 1.7762224674224854, rank IC -0.00557279959321022\n",
      "Epoch 91, step 0, loss 1.009944200515747, rank IC -0.0093361372128129\n",
      "Epoch 91, step 50, loss 1.3786853551864624, rank IC 0.008039667271077633\n",
      "Epoch 91, step 100, loss 0.8920472860336304, rank IC -0.00542560825124383\n",
      "Epoch 91, step 150, loss 1.7369669675827026, rank IC 0.006909966468811035\n",
      "Epoch 91, step 200, loss 1.2324894666671753, rank IC 0.01114873867481947\n",
      "Epoch 91, step 250, loss 1.02125084400177, rank IC 0.011613224633038044\n",
      "Epoch 91, step 300, loss 1.1655347347259521, rank IC 0.002146257786080241\n",
      "Epoch 91, step 350, loss 1.7762174606323242, rank IC -0.005634631961584091\n",
      "Epoch 92, step 0, loss 1.0098981857299805, rank IC -0.00933194998651743\n",
      "Epoch 92, step 50, loss 1.3785063028335571, rank IC 0.008076128549873829\n",
      "Epoch 92, step 100, loss 0.8919217586517334, rank IC -0.005385056138038635\n",
      "Epoch 92, step 150, loss 1.7357581853866577, rank IC 0.006860658526420593\n",
      "Epoch 92, step 200, loss 1.2391916513442993, rank IC 0.010790683329105377\n",
      "Epoch 92, step 250, loss 1.0206122398376465, rank IC 0.011685417033731937\n",
      "Epoch 92, step 300, loss 1.1652684211730957, rank IC 0.002165920799598098\n",
      "Epoch 92, step 350, loss 1.7779306173324585, rank IC -0.005652569700032473\n",
      "Epoch 93, step 0, loss 1.0091205835342407, rank IC -0.009450270794332027\n",
      "Epoch 93, step 50, loss 1.378279447555542, rank IC 0.0081227021291852\n",
      "Epoch 93, step 100, loss 0.8908584117889404, rank IC -0.005415572319179773\n",
      "Epoch 93, step 150, loss 1.7347357273101807, rank IC 0.006829482968896627\n",
      "Epoch 93, step 200, loss 1.2027966976165771, rank IC 0.011117521673440933\n",
      "Epoch 93, step 250, loss 1.0207451581954956, rank IC 0.011736061424016953\n",
      "Epoch 93, step 300, loss 1.1651270389556885, rank IC 0.002191602485254407\n",
      "Epoch 93, step 350, loss 1.776718258857727, rank IC -0.005658050533384085\n",
      "Epoch 94, step 0, loss 1.009554386138916, rank IC -0.009496297687292099\n",
      "Epoch 94, step 50, loss 1.3782505989074707, rank IC 0.008311591111123562\n",
      "Epoch 94, step 100, loss 0.891372799873352, rank IC -0.005438054446130991\n",
      "Epoch 94, step 150, loss 1.7351888418197632, rank IC 0.006834279280155897\n",
      "Epoch 94, step 200, loss 1.200073480606079, rank IC 0.011194570921361446\n",
      "Epoch 94, step 250, loss 1.0203883647918701, rank IC 0.011789429001510143\n",
      "Epoch 94, step 300, loss 1.164966344833374, rank IC 0.002220160560682416\n",
      "Epoch 94, step 350, loss 1.776661992073059, rank IC -0.005684088449925184\n",
      "Epoch 95, step 0, loss 1.0095772743225098, rank IC -0.00961169321089983\n",
      "Epoch 95, step 50, loss 1.3780618906021118, rank IC 0.008433143608272076\n",
      "Epoch 95, step 100, loss 0.8913828730583191, rank IC -0.00549365533515811\n",
      "Epoch 95, step 150, loss 1.7353240251541138, rank IC 0.006878940854221582\n",
      "Epoch 95, step 200, loss 1.1973044872283936, rank IC 0.011320866644382477\n",
      "Epoch 95, step 250, loss 1.0202678442001343, rank IC 0.011853066273033619\n",
      "Epoch 95, step 300, loss 1.1649116277694702, rank IC 0.0022259289398789406\n",
      "Epoch 95, step 350, loss 1.7762686014175415, rank IC -0.005704221781343222\n",
      "Epoch 96, step 0, loss 1.0096920728683472, rank IC -0.009709366597235203\n",
      "Epoch 96, step 50, loss 1.377955675125122, rank IC 0.008317135274410248\n",
      "Epoch 96, step 100, loss 0.8914068937301636, rank IC -0.005474269390106201\n",
      "Epoch 96, step 150, loss 1.735748529434204, rank IC 0.0069115678779780865\n",
      "Epoch 96, step 200, loss 1.2041687965393066, rank IC 0.011257312260568142\n",
      "Epoch 96, step 250, loss 1.019928216934204, rank IC 0.01189994066953659\n",
      "Epoch 96, step 300, loss 1.1648263931274414, rank IC 0.0022246961016207933\n",
      "Epoch 96, step 350, loss 1.7759286165237427, rank IC -0.005740823224186897\n",
      "Epoch 97, step 0, loss 1.0096909999847412, rank IC -0.009712611325085163\n",
      "Epoch 97, step 50, loss 1.3777791261672974, rank IC 0.008553252555429935\n",
      "Epoch 97, step 100, loss 0.8914089798927307, rank IC -0.005572997033596039\n",
      "Epoch 97, step 150, loss 1.735650658607483, rank IC 0.0069350674748420715\n",
      "Epoch 97, step 200, loss 1.2262364625930786, rank IC 0.011380385607481003\n",
      "Epoch 97, step 250, loss 1.0195491313934326, rank IC 0.01195074338465929\n",
      "Epoch 97, step 300, loss 1.1646766662597656, rank IC 0.00221902783960104\n",
      "Epoch 97, step 350, loss 1.7753612995147705, rank IC -0.005761806387454271\n",
      "Epoch 98, step 0, loss 1.009892225265503, rank IC -0.00985179003328085\n",
      "Epoch 98, step 50, loss 1.3776136636734009, rank IC 0.008432462811470032\n",
      "Epoch 98, step 100, loss 0.8914377689361572, rank IC -0.005488204304128885\n",
      "Epoch 98, step 150, loss 1.7353777885437012, rank IC 0.007025012280791998\n",
      "Epoch 98, step 200, loss 1.227123737335205, rank IC 0.011641889810562134\n",
      "Epoch 98, step 250, loss 1.0190062522888184, rank IC 0.012039815075695515\n",
      "Epoch 98, step 300, loss 1.1645439863204956, rank IC 0.0022010738030076027\n",
      "Epoch 98, step 350, loss 1.7749876976013184, rank IC -0.005615741014480591\n",
      "Epoch 99, step 0, loss 1.009890079498291, rank IC -0.009931981563568115\n",
      "Epoch 99, step 50, loss 1.3774901628494263, rank IC 0.008298267610371113\n",
      "Epoch 99, step 100, loss 0.8911545872688293, rank IC -0.005405493080615997\n",
      "Epoch 99, step 150, loss 1.7352988719940186, rank IC 0.007057067006826401\n",
      "Epoch 99, step 200, loss 1.2000843286514282, rank IC 0.011782754212617874\n",
      "Epoch 99, step 250, loss 1.0182753801345825, rank IC 0.012081081978976727\n",
      "Epoch 99, step 300, loss 1.1644154787063599, rank IC 0.002212138846516609\n",
      "Epoch 99, step 350, loss 1.77510666847229, rank IC -0.005835025105625391\n",
      "Epoch 100, step 0, loss 1.0099269151687622, rank IC -0.00996219739317894\n",
      "Epoch 100, step 50, loss 1.3773967027664185, rank IC 0.008714738301932812\n",
      "Epoch 100, step 100, loss 0.8912730813026428, rank IC -0.005672903265804052\n",
      "Epoch 100, step 150, loss 1.735776424407959, rank IC 0.00710733188316226\n",
      "Epoch 100, step 200, loss 1.1979960203170776, rank IC 0.011789028532803059\n",
      "Epoch 100, step 250, loss 1.018060564994812, rank IC 0.012137738056480885\n",
      "Epoch 100, step 300, loss 1.1643506288528442, rank IC 0.002199230482801795\n",
      "Epoch 100, step 350, loss 1.7750332355499268, rank IC -0.005846381653100252\n",
      "Epoch 101, step 0, loss 1.009935975074768, rank IC -0.010114026255905628\n",
      "Epoch 101, step 50, loss 1.3773292303085327, rank IC 0.008449180983006954\n",
      "Epoch 101, step 100, loss 0.8912684917449951, rank IC -0.005531450267881155\n",
      "Epoch 101, step 150, loss 1.7359240055084229, rank IC 0.007093115244060755\n",
      "Epoch 101, step 200, loss 1.2044957876205444, rank IC 0.011914260685443878\n",
      "Epoch 101, step 250, loss 1.0177887678146362, rank IC 0.012186896055936813\n",
      "Epoch 101, step 300, loss 1.1642347574234009, rank IC 0.002213028259575367\n",
      "Epoch 101, step 350, loss 1.7745105028152466, rank IC -0.005889161955565214\n",
      "Epoch 102, step 0, loss 1.0100456476211548, rank IC -0.010108414106070995\n",
      "Epoch 102, step 50, loss 1.3772014379501343, rank IC 0.008866909891366959\n",
      "Epoch 102, step 100, loss 0.8912840485572815, rank IC -0.0057398178614676\n",
      "Epoch 102, step 150, loss 1.7359651327133179, rank IC 0.007172224577516317\n",
      "Epoch 102, step 200, loss 1.215014934539795, rank IC 0.011911813169717789\n",
      "Epoch 102, step 250, loss 1.0175490379333496, rank IC 0.012242945842444897\n",
      "Epoch 102, step 300, loss 1.1640912294387817, rank IC 0.002209296217188239\n",
      "Epoch 102, step 350, loss 1.7745689153671265, rank IC -0.005869096610695124\n",
      "Epoch 103, step 0, loss 1.0100449323654175, rank IC -0.010244235396385193\n",
      "Epoch 103, step 50, loss 1.3770583868026733, rank IC 0.00868427287787199\n",
      "Epoch 103, step 100, loss 0.8911458849906921, rank IC -0.005770334508270025\n",
      "Epoch 103, step 150, loss 1.735637903213501, rank IC 0.007185801398009062\n",
      "Epoch 103, step 200, loss 1.2227014303207397, rank IC 0.012211531400680542\n",
      "Epoch 103, step 250, loss 1.0172516107559204, rank IC 0.012286022305488586\n",
      "Epoch 103, step 300, loss 1.1638983488082886, rank IC 0.002216418506577611\n",
      "Epoch 103, step 350, loss 1.7749202251434326, rank IC -0.005920155439525843\n",
      "Epoch 104, step 0, loss 1.0097179412841797, rank IC -0.010216680355370045\n",
      "Epoch 104, step 50, loss 1.3767046928405762, rank IC 0.009046871215105057\n",
      "Epoch 104, step 100, loss 0.8906221389770508, rank IC -0.005915232002735138\n",
      "Epoch 104, step 150, loss 1.735007405281067, rank IC 0.007214140612632036\n",
      "Epoch 104, step 200, loss 1.2118232250213623, rank IC 0.011929560452699661\n",
      "Epoch 104, step 250, loss 1.0168362855911255, rank IC 0.012327228672802448\n",
      "Epoch 104, step 300, loss 1.1638811826705933, rank IC 0.0022128233686089516\n",
      "Epoch 104, step 350, loss 1.7743533849716187, rank IC -0.0059193894267082214\n",
      "Epoch 105, step 0, loss 1.0100278854370117, rank IC -0.010351095348596573\n",
      "Epoch 105, step 50, loss 1.3767496347427368, rank IC 0.008717305958271027\n",
      "Epoch 105, step 100, loss 0.8910108804702759, rank IC -0.00569280656054616\n",
      "Epoch 105, step 150, loss 1.7349573373794556, rank IC 0.007206515874713659\n",
      "Epoch 105, step 200, loss 1.2110780477523804, rank IC 0.012180893681943417\n",
      "Epoch 105, step 250, loss 1.0166311264038086, rank IC 0.012361501343548298\n",
      "Epoch 105, step 300, loss 1.1637564897537231, rank IC 0.002226600656285882\n",
      "Epoch 105, step 350, loss 1.7739750146865845, rank IC -0.00594438286498189\n",
      "Epoch 106, step 0, loss 1.010209083557129, rank IC -0.01035192608833313\n",
      "Epoch 106, step 50, loss 1.3766669034957886, rank IC 0.009108327329158783\n",
      "Epoch 106, step 100, loss 0.8911052346229553, rank IC -0.005808039102703333\n",
      "Epoch 106, step 150, loss 1.7352601289749146, rank IC 0.007223610300570726\n",
      "Epoch 106, step 200, loss 1.2035062313079834, rank IC 0.012168366461992264\n",
      "Epoch 106, step 250, loss 1.0161476135253906, rank IC 0.012408276088535786\n",
      "Epoch 106, step 300, loss 1.1636831760406494, rank IC 0.0022223424166440964\n",
      "Epoch 106, step 350, loss 1.7738373279571533, rank IC -0.005937756970524788\n",
      "Epoch 107, step 0, loss 1.0102165937423706, rank IC -0.010414592921733856\n",
      "Epoch 107, step 50, loss 1.3764927387237549, rank IC 0.009065466932952404\n",
      "Epoch 107, step 100, loss 0.8910754919052124, rank IC -0.005807521287351847\n",
      "Epoch 107, step 150, loss 1.7356055974960327, rank IC 0.007211793214082718\n",
      "Epoch 107, step 200, loss 1.2020440101623535, rank IC 0.012277540750801563\n",
      "Epoch 107, step 250, loss 1.0160939693450928, rank IC 0.012458682991564274\n",
      "Epoch 107, step 300, loss 1.1634776592254639, rank IC 0.002225347561761737\n",
      "Epoch 107, step 350, loss 1.7739014625549316, rank IC -0.005971482489258051\n",
      "Epoch 108, step 0, loss 1.0100291967391968, rank IC -0.010487574152648449\n",
      "Epoch 108, step 50, loss 1.3762706518173218, rank IC 0.009106859564781189\n",
      "Epoch 108, step 100, loss 0.8905912041664124, rank IC -0.005908600520342588\n",
      "Epoch 108, step 150, loss 1.7359446287155151, rank IC 0.007225099951028824\n",
      "Epoch 108, step 200, loss 1.2157679796218872, rank IC 0.01241794228553772\n",
      "Epoch 108, step 250, loss 1.0154495239257812, rank IC 0.012489695101976395\n",
      "Epoch 108, step 300, loss 1.1633899211883545, rank IC 0.002239835448563099\n",
      "Epoch 108, step 350, loss 1.7731678485870361, rank IC -0.005941348150372505\n",
      "Epoch 109, step 0, loss 1.0103062391281128, rank IC -0.01053666789084673\n",
      "Epoch 109, step 50, loss 1.3762673139572144, rank IC 0.009264347143471241\n",
      "Epoch 109, step 100, loss 0.8909063935279846, rank IC -0.0058695729821920395\n",
      "Epoch 109, step 150, loss 1.7359592914581299, rank IC 0.007242519408464432\n",
      "Epoch 109, step 200, loss 1.2081995010375977, rank IC 0.012263829819858074\n",
      "Epoch 109, step 250, loss 1.014741063117981, rank IC 0.012536388821899891\n",
      "Epoch 109, step 300, loss 1.1634010076522827, rank IC 0.0022355227265506983\n",
      "Epoch 109, step 350, loss 1.7730815410614014, rank IC -0.0059655276127159595\n",
      "Epoch 110, step 0, loss 1.0103822946548462, rank IC -0.010684359818696976\n",
      "Epoch 110, step 50, loss 1.3761773109436035, rank IC 0.00900572445243597\n",
      "Epoch 110, step 100, loss 0.8910247087478638, rank IC -0.00573707977309823\n",
      "Epoch 110, step 150, loss 1.735489845275879, rank IC 0.0072410148568451405\n",
      "Epoch 110, step 200, loss 1.2272651195526123, rank IC 0.012643493711948395\n",
      "Epoch 110, step 250, loss 1.0150303840637207, rank IC 0.012592819519340992\n",
      "Epoch 110, step 300, loss 1.163237452507019, rank IC 0.0022488869726657867\n",
      "Epoch 110, step 350, loss 1.772396206855774, rank IC -0.006018815096467733\n",
      "Epoch 111, step 0, loss 1.0105462074279785, rank IC -0.01066864375025034\n",
      "Epoch 111, step 50, loss 1.3760796785354614, rank IC 0.009465661831200123\n",
      "Epoch 111, step 100, loss 0.8910558819770813, rank IC -0.005957653746008873\n",
      "Epoch 111, step 150, loss 1.7348672151565552, rank IC 0.0072168149054050446\n",
      "Epoch 111, step 200, loss 1.2007156610488892, rank IC 0.012433256022632122\n",
      "Epoch 111, step 250, loss 1.0139857530593872, rank IC 0.012639877386391163\n",
      "Epoch 111, step 300, loss 1.1632167100906372, rank IC 0.0022473742719739676\n",
      "Epoch 111, step 350, loss 1.7727224826812744, rank IC -0.006064068526029587\n",
      "Epoch 112, step 0, loss 1.010563850402832, rank IC -0.010793759487569332\n",
      "Epoch 112, step 50, loss 1.375988483428955, rank IC 0.009341644123196602\n",
      "Epoch 112, step 100, loss 0.8912043571472168, rank IC -0.0058776312507689\n",
      "Epoch 112, step 150, loss 1.7350597381591797, rank IC 0.007210139185190201\n",
      "Epoch 112, step 200, loss 1.2065668106079102, rank IC 0.012683499604463577\n",
      "Epoch 112, step 250, loss 1.0140740871429443, rank IC 0.012679246254265308\n",
      "Epoch 112, step 300, loss 1.1631263494491577, rank IC 0.0022537100594490767\n",
      "Epoch 112, step 350, loss 1.7727349996566772, rank IC -0.0060935127548873425\n",
      "Epoch 113, step 0, loss 1.0104118585586548, rank IC -0.010787383653223515\n",
      "Epoch 113, step 50, loss 1.3756269216537476, rank IC 0.009688291698694229\n",
      "Epoch 113, step 100, loss 0.8907362222671509, rank IC -0.006106265354901552\n",
      "Epoch 113, step 150, loss 1.7355443239212036, rank IC 0.007194552570581436\n",
      "Epoch 113, step 200, loss 1.1950194835662842, rank IC 0.012829429470002651\n",
      "Epoch 113, step 250, loss 1.0144695043563843, rank IC 0.01272790227085352\n",
      "Epoch 113, step 300, loss 1.1628820896148682, rank IC 0.00226377765648067\n",
      "Epoch 113, step 350, loss 1.772204875946045, rank IC -0.006161789875477552\n",
      "Epoch 114, step 0, loss 1.010604977607727, rank IC -0.010905520059168339\n",
      "Epoch 114, step 50, loss 1.3757874965667725, rank IC 0.009411314502358437\n",
      "Epoch 114, step 100, loss 0.8909728527069092, rank IC -0.005953504238277674\n",
      "Epoch 114, step 150, loss 1.7359508275985718, rank IC 0.007204938214272261\n",
      "Epoch 114, step 200, loss 1.1932965517044067, rank IC 0.012944765388965607\n",
      "Epoch 114, step 250, loss 1.0134767293930054, rank IC 0.012779194861650467\n",
      "Epoch 114, step 300, loss 1.1629278659820557, rank IC 0.0022500806953758\n",
      "Epoch 114, step 350, loss 1.7719109058380127, rank IC -0.006188197527080774\n",
      "Epoch 115, step 0, loss 1.0106881856918335, rank IC -0.010940399952232838\n",
      "Epoch 115, step 50, loss 1.3756742477416992, rank IC 0.009567930363118649\n",
      "Epoch 115, step 100, loss 0.8909830451011658, rank IC -0.006010034587234259\n",
      "Epoch 115, step 150, loss 1.7366502285003662, rank IC 0.007199427112936974\n",
      "Epoch 115, step 200, loss 1.196534514427185, rank IC 0.012931075878441334\n",
      "Epoch 115, step 250, loss 1.012868881225586, rank IC 0.012837945483624935\n",
      "Epoch 115, step 300, loss 1.1628540754318237, rank IC 0.0022604363039135933\n",
      "Epoch 115, step 350, loss 1.7716537714004517, rank IC -0.006244200747460127\n",
      "Epoch 116, step 0, loss 1.0107810497283936, rank IC -0.01098955050110817\n",
      "Epoch 116, step 50, loss 1.3754774332046509, rank IC 0.00975131243467331\n",
      "Epoch 116, step 100, loss 0.8909114599227905, rank IC -0.006142619997262955\n",
      "Epoch 116, step 150, loss 1.7371141910552979, rank IC 0.007159601431339979\n",
      "Epoch 116, step 200, loss 1.2116256952285767, rank IC 0.013193887658417225\n",
      "Epoch 116, step 250, loss 1.012571096420288, rank IC 0.012889593839645386\n",
      "Epoch 116, step 300, loss 1.1627267599105835, rank IC 0.002250504679977894\n",
      "Epoch 116, step 350, loss 1.771202564239502, rank IC -0.006276685744524002\n",
      "Epoch 117, step 0, loss 1.0109038352966309, rank IC -0.011068304069340229\n",
      "Epoch 117, step 50, loss 1.3752796649932861, rank IC 0.00971903745085001\n",
      "Epoch 117, step 100, loss 0.8907088041305542, rank IC -0.006207600701600313\n",
      "Epoch 117, step 150, loss 1.736061692237854, rank IC 0.007141712587326765\n",
      "Epoch 117, step 200, loss 1.2162470817565918, rank IC 0.013133995234966278\n",
      "Epoch 117, step 250, loss 1.0115290880203247, rank IC 0.012905653566122055\n",
      "Epoch 117, step 300, loss 1.1627631187438965, rank IC 0.002258160850033164\n",
      "Epoch 117, step 350, loss 1.770911693572998, rank IC -0.006291097030043602\n",
      "Epoch 118, step 0, loss 1.0110714435577393, rank IC -0.011099439114332199\n",
      "Epoch 118, step 50, loss 1.3754221200942993, rank IC 0.00986168161034584\n",
      "Epoch 118, step 100, loss 0.8910662531852722, rank IC -0.006122272461652756\n",
      "Epoch 118, step 150, loss 1.7350490093231201, rank IC 0.007160461042076349\n",
      "Epoch 118, step 200, loss 1.23175847530365, rank IC 0.013246986083686352\n",
      "Epoch 118, step 250, loss 1.0108751058578491, rank IC 0.012963704764842987\n",
      "Epoch 118, step 300, loss 1.1627484560012817, rank IC 0.0022850658278912306\n",
      "Epoch 118, step 350, loss 1.7707968950271606, rank IC -0.00630912184715271\n",
      "Epoch 119, step 0, loss 1.0111594200134277, rank IC -0.01114389207214117\n",
      "Epoch 119, step 50, loss 1.3752938508987427, rank IC 0.009945780970156193\n",
      "Epoch 119, step 100, loss 0.8910027146339417, rank IC -0.006142709404230118\n",
      "Epoch 119, step 150, loss 1.7343484163284302, rank IC 0.007128184661269188\n",
      "Epoch 119, step 200, loss 1.2246124744415283, rank IC 0.013433273881673813\n",
      "Epoch 119, step 250, loss 1.0100317001342773, rank IC 0.01300853956490755\n",
      "Epoch 119, step 300, loss 1.162715196609497, rank IC 0.002264393260702491\n",
      "Epoch 119, step 350, loss 1.7711228132247925, rank IC -0.006353175733238459\n",
      "Epoch 120, step 0, loss 1.0112069845199585, rank IC -0.011212936602532864\n",
      "Epoch 120, step 50, loss 1.3751522302627563, rank IC 0.00999889150261879\n",
      "Epoch 120, step 100, loss 0.8913423418998718, rank IC -0.006162905599921942\n",
      "Epoch 120, step 150, loss 1.735612392425537, rank IC 0.007161298766732216\n",
      "Epoch 120, step 200, loss 1.2134126424789429, rank IC 0.013392973691225052\n",
      "Epoch 120, step 250, loss 1.0113157033920288, rank IC 0.013040650635957718\n",
      "Epoch 120, step 300, loss 1.1624788045883179, rank IC 0.0022786057088524103\n",
      "Epoch 120, step 350, loss 1.7707332372665405, rank IC -0.006376327481120825\n",
      "Epoch 121, step 0, loss 1.0111620426177979, rank IC -0.011301551945507526\n",
      "Epoch 121, step 50, loss 1.375145435333252, rank IC 0.009874027222394943\n",
      "Epoch 121, step 100, loss 0.8913266062736511, rank IC -0.006144353654235601\n",
      "Epoch 121, step 150, loss 1.735961675643921, rank IC 0.007165366318076849\n",
      "Epoch 121, step 200, loss 1.200505018234253, rank IC 0.013544216752052307\n",
      "Epoch 121, step 250, loss 1.011791467666626, rank IC 0.013047133572399616\n",
      "Epoch 121, step 300, loss 1.1623775959014893, rank IC 0.0022961862850934267\n",
      "Epoch 121, step 350, loss 1.7703757286071777, rank IC -0.006350076291710138\n",
      "Epoch 122, step 0, loss 1.0111531019210815, rank IC -0.011311172507703304\n",
      "Epoch 122, step 50, loss 1.3750983476638794, rank IC 0.010173081420361996\n",
      "Epoch 122, step 100, loss 0.8913847208023071, rank IC -0.0062383562326431274\n",
      "Epoch 122, step 150, loss 1.7361103296279907, rank IC 0.007189842406660318\n",
      "Epoch 122, step 200, loss 1.1963887214660645, rank IC 0.013560444116592407\n",
      "Epoch 122, step 250, loss 1.0110490322113037, rank IC 0.013108520768582821\n",
      "Epoch 122, step 300, loss 1.1622778177261353, rank IC 0.002280469285324216\n",
      "Epoch 122, step 350, loss 1.7701958417892456, rank IC -0.006433746311813593\n",
      "Epoch 123, step 0, loss 1.0112348794937134, rank IC -0.011373549699783325\n",
      "Epoch 123, step 50, loss 1.3750903606414795, rank IC 0.010107009671628475\n",
      "Epoch 123, step 100, loss 0.8913671374320984, rank IC -0.006195542868226767\n",
      "Epoch 123, step 150, loss 1.7363929748535156, rank IC 0.007196018006652594\n",
      "Epoch 123, step 200, loss 1.1967817544937134, rank IC 0.013639464974403381\n",
      "Epoch 123, step 250, loss 1.011049509048462, rank IC 0.013147321529686451\n",
      "Epoch 123, step 300, loss 1.1621612310409546, rank IC 0.0022825945634394884\n",
      "Epoch 123, step 350, loss 1.7699921131134033, rank IC -0.006470972206443548\n",
      "Epoch 124, step 0, loss 1.0112611055374146, rank IC -0.011423480696976185\n",
      "Epoch 124, step 50, loss 1.374940037727356, rank IC 0.010116931982338428\n",
      "Epoch 124, step 100, loss 0.8911712765693665, rank IC -0.006232077721506357\n",
      "Epoch 124, step 150, loss 1.7367169857025146, rank IC 0.00719546340405941\n",
      "Epoch 124, step 200, loss 1.1903098821640015, rank IC 0.013623648323118687\n",
      "Epoch 124, step 250, loss 1.0104420185089111, rank IC 0.013159561902284622\n",
      "Epoch 124, step 300, loss 1.1621510982513428, rank IC 0.002286006696522236\n",
      "Epoch 124, step 350, loss 1.7697926759719849, rank IC -0.006456772331148386\n",
      "Epoch 125, step 0, loss 1.011360764503479, rank IC -0.011491917073726654\n",
      "Epoch 125, step 50, loss 1.374963641166687, rank IC 0.010079535655677319\n",
      "Epoch 125, step 100, loss 0.891300618648529, rank IC -0.006268616765737534\n",
      "Epoch 125, step 150, loss 1.7373191118240356, rank IC 0.007213978562504053\n",
      "Epoch 125, step 200, loss 1.195111870765686, rank IC 0.013734462670981884\n",
      "Epoch 125, step 250, loss 1.0103144645690918, rank IC 0.013195127248764038\n",
      "Epoch 125, step 300, loss 1.1620941162109375, rank IC 0.0022981110960245132\n",
      "Epoch 125, step 350, loss 1.769411563873291, rank IC -0.0064676194451749325\n",
      "Epoch 126, step 0, loss 1.0115351676940918, rank IC -0.011522449553012848\n",
      "Epoch 126, step 50, loss 1.3749604225158691, rank IC 0.010285595431923866\n",
      "Epoch 126, step 100, loss 0.8915087580680847, rank IC -0.0063658966682851315\n",
      "Epoch 126, step 150, loss 1.737537145614624, rank IC 0.007171595003455877\n",
      "Epoch 126, step 200, loss 1.1917073726654053, rank IC 0.013806206174194813\n",
      "Epoch 126, step 250, loss 1.0096632242202759, rank IC 0.013236583210527897\n",
      "Epoch 126, step 300, loss 1.1620478630065918, rank IC 0.0022915340960025787\n",
      "Epoch 126, step 350, loss 1.7691799402236938, rank IC -0.006496223155409098\n",
      "Epoch 127, step 0, loss 1.011577844619751, rank IC -0.011572226881980896\n",
      "Epoch 127, step 50, loss 1.3748610019683838, rank IC 0.010273255407810211\n",
      "Epoch 127, step 100, loss 0.8915446400642395, rank IC -0.0063690668903291225\n",
      "Epoch 127, step 150, loss 1.7382266521453857, rank IC 0.007213877514004707\n",
      "Epoch 127, step 200, loss 1.2033864259719849, rank IC 0.01393298152834177\n",
      "Epoch 127, step 250, loss 1.0096311569213867, rank IC 0.013318889774382114\n",
      "Epoch 127, step 300, loss 1.1619455814361572, rank IC 0.0023118092212826014\n",
      "Epoch 127, step 350, loss 1.7690351009368896, rank IC -0.006537094712257385\n",
      "Epoch 128, step 0, loss 1.011552095413208, rank IC -0.011628913693130016\n",
      "Epoch 128, step 50, loss 1.3746082782745361, rank IC 0.010516171343624592\n",
      "Epoch 128, step 100, loss 0.8913701772689819, rank IC -0.006547428201884031\n",
      "Epoch 128, step 150, loss 1.7373316287994385, rank IC 0.007174108177423477\n",
      "Epoch 128, step 200, loss 1.1925842761993408, rank IC 0.01397386472672224\n",
      "Epoch 128, step 250, loss 1.0091273784637451, rank IC 0.013319377787411213\n",
      "Epoch 128, step 300, loss 1.1619980335235596, rank IC 0.0023030629381537437\n",
      "Epoch 128, step 350, loss 1.7690678834915161, rank IC -0.006508218590170145\n",
      "Epoch 129, step 0, loss 1.0116301774978638, rank IC -0.011674714274704456\n",
      "Epoch 129, step 50, loss 1.374577283859253, rank IC 0.010318438522517681\n",
      "Epoch 129, step 100, loss 0.8913425207138062, rank IC -0.006507046986371279\n",
      "Epoch 129, step 150, loss 1.7368476390838623, rank IC 0.007197404745966196\n",
      "Epoch 129, step 200, loss 1.2033015489578247, rank IC 0.014179644174873829\n",
      "Epoch 129, step 250, loss 1.0089712142944336, rank IC 0.013361667282879353\n",
      "Epoch 129, step 300, loss 1.1618741750717163, rank IC 0.002304547233507037\n",
      "Epoch 129, step 350, loss 1.7687753438949585, rank IC -0.006570506375283003\n",
      "Epoch 130, step 0, loss 1.0116550922393799, rank IC -0.011736970394849777\n",
      "Epoch 130, step 50, loss 1.374706745147705, rank IC 0.010538950562477112\n",
      "Epoch 130, step 100, loss 0.8914687633514404, rank IC -0.006626909598708153\n",
      "Epoch 130, step 150, loss 1.7361996173858643, rank IC 0.007175156380981207\n",
      "Epoch 130, step 200, loss 1.1918179988861084, rank IC 0.014104320667684078\n",
      "Epoch 130, step 250, loss 1.0079524517059326, rank IC 0.013373148627579212\n",
      "Epoch 130, step 300, loss 1.1618231534957886, rank IC 0.0023083777632564306\n",
      "Epoch 130, step 350, loss 1.7689672708511353, rank IC -0.006517066154628992\n",
      "Epoch 131, step 0, loss 1.0116416215896606, rank IC -0.011764849536120892\n",
      "Epoch 131, step 50, loss 1.3745362758636475, rank IC 0.010595357976853848\n",
      "Epoch 131, step 100, loss 0.8914700150489807, rank IC -0.006667569279670715\n",
      "Epoch 131, step 150, loss 1.7363146543502808, rank IC 0.007196602877229452\n",
      "Epoch 131, step 200, loss 1.2084600925445557, rank IC 0.014161554165184498\n",
      "Epoch 131, step 250, loss 1.0080007314682007, rank IC 0.013398404233157635\n",
      "Epoch 131, step 300, loss 1.1617567539215088, rank IC 0.0023152772337198257\n",
      "Epoch 131, step 350, loss 1.7683576345443726, rank IC -0.006540913134813309\n",
      "Epoch 132, step 0, loss 1.011793613433838, rank IC -0.011826787143945694\n",
      "Epoch 132, step 50, loss 1.374552607536316, rank IC 0.010581434704363346\n",
      "Epoch 132, step 100, loss 0.8914808630943298, rank IC -0.006685750558972359\n",
      "Epoch 132, step 150, loss 1.7363860607147217, rank IC 0.007229426875710487\n",
      "Epoch 132, step 200, loss 1.2327730655670166, rank IC 0.0143093541264534\n",
      "Epoch 132, step 250, loss 1.006545901298523, rank IC 0.013435646891593933\n",
      "Epoch 132, step 300, loss 1.1617993116378784, rank IC 0.002314607612788677\n",
      "Epoch 132, step 350, loss 1.76880943775177, rank IC -0.006492760498076677\n",
      "Epoch 133, step 0, loss 1.0116677284240723, rank IC -0.011867481283843517\n",
      "Epoch 133, step 50, loss 1.3745481967926025, rank IC 0.010564831085503101\n",
      "Epoch 133, step 100, loss 0.8913320302963257, rank IC -0.0066747828386723995\n",
      "Epoch 133, step 150, loss 1.736160159111023, rank IC 0.007215936202555895\n",
      "Epoch 133, step 200, loss 1.2066614627838135, rank IC 0.014214787632226944\n",
      "Epoch 133, step 250, loss 1.0061239004135132, rank IC 0.013483910821378231\n",
      "Epoch 133, step 300, loss 1.1618026494979858, rank IC 0.0023229727521538734\n",
      "Epoch 133, step 350, loss 1.7680578231811523, rank IC -0.0066028437577188015\n",
      "Epoch 134, step 0, loss 1.0120397806167603, rank IC -0.011929054744541645\n",
      "Epoch 134, step 50, loss 1.3744115829467773, rank IC 0.010821075178682804\n",
      "Epoch 134, step 100, loss 0.8918688893318176, rank IC -0.0068272873759269714\n",
      "Epoch 134, step 150, loss 1.7366703748703003, rank IC 0.007196925114840269\n",
      "Epoch 134, step 200, loss 1.202301025390625, rank IC 0.014421850442886353\n",
      "Epoch 134, step 250, loss 1.0071951150894165, rank IC 0.013489662669599056\n",
      "Epoch 134, step 300, loss 1.1617063283920288, rank IC 0.0023326340597122908\n",
      "Epoch 134, step 350, loss 1.7677953243255615, rank IC -0.006475514266639948\n",
      "Epoch 135, step 0, loss 1.0120460987091064, rank IC -0.011984159238636494\n",
      "Epoch 135, step 50, loss 1.3743854761123657, rank IC 0.010623727925121784\n",
      "Epoch 135, step 100, loss 0.8920027017593384, rank IC -0.0067438106052577496\n",
      "Epoch 135, step 150, loss 1.7371340990066528, rank IC 0.0072511644102633\n",
      "Epoch 135, step 200, loss 1.1960432529449463, rank IC 0.014478974044322968\n",
      "Epoch 135, step 250, loss 1.0072904825210571, rank IC 0.013519414700567722\n",
      "Epoch 135, step 300, loss 1.161458134651184, rank IC 0.0023206390906125307\n",
      "Epoch 135, step 350, loss 1.7675727605819702, rank IC -0.006658222991973162\n",
      "Epoch 136, step 0, loss 1.0121468305587769, rank IC -0.011997195892035961\n",
      "Epoch 136, step 50, loss 1.3744066953659058, rank IC 0.010750457644462585\n",
      "Epoch 136, step 100, loss 0.8921215534210205, rank IC -0.006840916816145182\n",
      "Epoch 136, step 150, loss 1.7377904653549194, rank IC 0.007238147314637899\n",
      "Epoch 136, step 200, loss 1.1868125200271606, rank IC 0.014501326717436314\n",
      "Epoch 136, step 250, loss 1.007046103477478, rank IC 0.013535750098526478\n",
      "Epoch 136, step 300, loss 1.1613832712173462, rank IC 0.0023172793444246054\n",
      "Epoch 136, step 350, loss 1.7670013904571533, rank IC -0.006565991789102554\n",
      "Epoch 137, step 0, loss 1.0123211145401, rank IC -0.01206169929355383\n",
      "Epoch 137, step 50, loss 1.374389886856079, rank IC 0.010636051185429096\n",
      "Epoch 137, step 100, loss 0.8921762108802795, rank IC -0.006808709818869829\n",
      "Epoch 137, step 150, loss 1.7378544807434082, rank IC 0.007246359717100859\n",
      "Epoch 137, step 200, loss 1.1899508237838745, rank IC 0.014614317566156387\n",
      "Epoch 137, step 250, loss 1.0067696571350098, rank IC 0.013550180941820145\n",
      "Epoch 137, step 300, loss 1.1613807678222656, rank IC 0.002324722008779645\n",
      "Epoch 137, step 350, loss 1.7667136192321777, rank IC -0.006565046962350607\n",
      "Epoch 138, step 0, loss 1.012394905090332, rank IC -0.012095157988369465\n",
      "Epoch 138, step 50, loss 1.3742759227752686, rank IC 0.010780531913042068\n",
      "Epoch 138, step 100, loss 0.8921536803245544, rank IC -0.006918517407029867\n",
      "Epoch 138, step 150, loss 1.738416314125061, rank IC 0.007269732654094696\n",
      "Epoch 138, step 200, loss 1.1912699937820435, rank IC 0.014645226299762726\n",
      "Epoch 138, step 250, loss 1.0061651468276978, rank IC 0.013586975634098053\n",
      "Epoch 138, step 300, loss 1.1612788438796997, rank IC 0.0023217955604195595\n",
      "Epoch 138, step 350, loss 1.7662756443023682, rank IC -0.006617449689656496\n",
      "Epoch 139, step 0, loss 1.0124653577804565, rank IC -0.012130867689847946\n",
      "Epoch 139, step 50, loss 1.3742612600326538, rank IC 0.010961487889289856\n",
      "Epoch 139, step 100, loss 0.8922274112701416, rank IC -0.0070113446563482285\n",
      "Epoch 139, step 150, loss 1.7380938529968262, rank IC 0.007310735061764717\n",
      "Epoch 139, step 200, loss 1.1839573383331299, rank IC 0.014667931012809277\n",
      "Epoch 139, step 250, loss 1.005547046661377, rank IC 0.01359513122588396\n",
      "Epoch 139, step 300, loss 1.1612874269485474, rank IC 0.002329972805455327\n",
      "Epoch 139, step 350, loss 1.7660691738128662, rank IC -0.006507560610771179\n",
      "Epoch 140, step 0, loss 1.0125552415847778, rank IC -0.012172876857221127\n",
      "Epoch 140, step 50, loss 1.3742015361785889, rank IC 0.010873529128730297\n",
      "Epoch 140, step 100, loss 0.8923379182815552, rank IC -0.007009697612375021\n",
      "Epoch 140, step 150, loss 1.7381950616836548, rank IC 0.007318864110857248\n",
      "Epoch 140, step 200, loss 1.1888058185577393, rank IC 0.01486265379935503\n",
      "Epoch 140, step 250, loss 1.0049207210540771, rank IC 0.013632706366479397\n",
      "Epoch 140, step 300, loss 1.1612467765808105, rank IC 0.0023196025285869837\n",
      "Epoch 140, step 350, loss 1.765721321105957, rank IC -0.006606634706258774\n",
      "Epoch 141, step 0, loss 1.0127097368240356, rank IC -0.012210658751428127\n",
      "Epoch 141, step 50, loss 1.374202847480774, rank IC 0.010994560085237026\n",
      "Epoch 141, step 100, loss 0.8923990726470947, rank IC -0.007082120981067419\n",
      "Epoch 141, step 150, loss 1.7389858961105347, rank IC 0.007326079998165369\n",
      "Epoch 141, step 200, loss 1.1896079778671265, rank IC 0.014861651696264744\n",
      "Epoch 141, step 250, loss 1.0046941041946411, rank IC 0.013646195642650127\n",
      "Epoch 141, step 300, loss 1.1611608266830444, rank IC 0.0023331029806286097\n",
      "Epoch 141, step 350, loss 1.765690565109253, rank IC -0.006509973201900721\n",
      "Epoch 142, step 0, loss 1.0127919912338257, rank IC -0.012238554656505585\n",
      "Epoch 142, step 50, loss 1.3741625547409058, rank IC 0.011078964918851852\n",
      "Epoch 142, step 100, loss 0.8925167322158813, rank IC -0.007105946075171232\n",
      "Epoch 142, step 150, loss 1.7380080223083496, rank IC 0.007354538422077894\n",
      "Epoch 142, step 200, loss 1.192638874053955, rank IC 0.014986683614552021\n",
      "Epoch 142, step 250, loss 1.0041954517364502, rank IC 0.013676412403583527\n",
      "Epoch 142, step 300, loss 1.161088466644287, rank IC 0.002325052162632346\n",
      "Epoch 142, step 350, loss 1.7656112909317017, rank IC -0.006511489395052195\n",
      "Epoch 143, step 0, loss 1.0128105878829956, rank IC -0.012279550544917583\n",
      "Epoch 143, step 50, loss 1.3740273714065552, rank IC 0.01104599516838789\n",
      "Epoch 143, step 100, loss 0.8924095034599304, rank IC -0.007100462913513184\n",
      "Epoch 143, step 150, loss 1.7371803522109985, rank IC 0.007323915604501963\n",
      "Epoch 143, step 200, loss 1.1968547105789185, rank IC 0.014937368221580982\n",
      "Epoch 143, step 250, loss 1.0029441118240356, rank IC 0.013709134422242641\n",
      "Epoch 143, step 300, loss 1.1611571311950684, rank IC 0.0023177454713732004\n",
      "Epoch 143, step 350, loss 1.7655529975891113, rank IC -0.006491081323474646\n",
      "Epoch 144, step 0, loss 1.0128315687179565, rank IC -0.012348848395049572\n",
      "Epoch 144, step 50, loss 1.3739978075027466, rank IC 0.010901891626417637\n",
      "Epoch 144, step 100, loss 0.8923693895339966, rank IC -0.0071191731840372086\n",
      "Epoch 144, step 150, loss 1.7367243766784668, rank IC 0.007350729778409004\n",
      "Epoch 144, step 200, loss 1.21026611328125, rank IC 0.015093070454895496\n",
      "Epoch 144, step 250, loss 1.0028785467147827, rank IC 0.013742104172706604\n",
      "Epoch 144, step 300, loss 1.1611319780349731, rank IC 0.002316105179488659\n",
      "Epoch 144, step 350, loss 1.7652593851089478, rank IC -0.006549668963998556\n",
      "Epoch 145, step 0, loss 1.0128909349441528, rank IC -0.012360711582005024\n",
      "Epoch 145, step 50, loss 1.3739372491836548, rank IC 0.011156427673995495\n",
      "Epoch 145, step 100, loss 0.8922517895698547, rank IC -0.007213294506072998\n",
      "Epoch 145, step 150, loss 1.7362771034240723, rank IC 0.007376406341791153\n",
      "Epoch 145, step 200, loss 1.1951873302459717, rank IC 0.01499821338802576\n",
      "Epoch 145, step 250, loss 1.0019729137420654, rank IC 0.01376891415566206\n",
      "Epoch 145, step 300, loss 1.1610645055770874, rank IC 0.0023264391347765923\n",
      "Epoch 145, step 350, loss 1.7654705047607422, rank IC -0.006543338764458895\n",
      "Epoch 146, step 0, loss 1.0128833055496216, rank IC -0.012410844676196575\n",
      "Epoch 146, step 50, loss 1.374077558517456, rank IC 0.010995899327099323\n",
      "Epoch 146, step 100, loss 0.8925831913948059, rank IC -0.007157770451158285\n",
      "Epoch 146, step 150, loss 1.7368983030319214, rank IC 0.007349468767642975\n",
      "Epoch 146, step 200, loss 1.2138694524765015, rank IC 0.015188314951956272\n",
      "Epoch 146, step 250, loss 1.0030802488327026, rank IC 0.013784688897430897\n",
      "Epoch 146, step 300, loss 1.1609629392623901, rank IC 0.0023197277914732695\n",
      "Epoch 146, step 350, loss 1.765052080154419, rank IC -0.006609503645449877\n",
      "Epoch 147, step 0, loss 1.0129259824752808, rank IC -0.012472830712795258\n",
      "Epoch 147, step 50, loss 1.374027967453003, rank IC 0.01104763150215149\n",
      "Epoch 147, step 100, loss 0.8925716876983643, rank IC -0.0071955290623009205\n",
      "Epoch 147, step 150, loss 1.736261248588562, rank IC 0.007339089643210173\n",
      "Epoch 147, step 200, loss 1.1886980533599854, rank IC 0.015140329487621784\n",
      "Epoch 147, step 250, loss 1.001994013786316, rank IC 0.013813015073537827\n",
      "Epoch 147, step 300, loss 1.1609992980957031, rank IC 0.0023226484190672636\n",
      "Epoch 147, step 350, loss 1.7654578685760498, rank IC -0.006570284720510244\n",
      "Epoch 148, step 0, loss 1.0129319429397583, rank IC -0.01247429195791483\n",
      "Epoch 148, step 50, loss 1.3740154504776, rank IC 0.011188149452209473\n",
      "Epoch 148, step 100, loss 0.8929532170295715, rank IC -0.007277678232640028\n",
      "Epoch 148, step 150, loss 1.7372053861618042, rank IC 0.007326175924390554\n",
      "Epoch 148, step 200, loss 1.1867624521255493, rank IC 0.015283898450434208\n",
      "Epoch 148, step 250, loss 1.0030385255813599, rank IC 0.013822085224092007\n",
      "Epoch 148, step 300, loss 1.16083562374115, rank IC 0.002325989305973053\n",
      "Epoch 148, step 350, loss 1.7652050256729126, rank IC -0.006589250173419714\n",
      "Epoch 149, step 0, loss 1.013003945350647, rank IC -0.01252915058284998\n",
      "Epoch 149, step 50, loss 1.374030351638794, rank IC 0.0111469104886055\n",
      "Epoch 149, step 100, loss 0.892993152141571, rank IC -0.00729771563783288\n",
      "Epoch 149, step 150, loss 1.7384761571884155, rank IC 0.007338316645473242\n",
      "Epoch 149, step 200, loss 1.193184494972229, rank IC 0.015368160791695118\n",
      "Epoch 149, step 250, loss 1.0030463933944702, rank IC 0.013845749199390411\n",
      "Epoch 149, step 300, loss 1.1607234477996826, rank IC 0.002335013123229146\n",
      "Epoch 149, step 350, loss 1.7646863460540771, rank IC -0.006565953139215708\n",
      "Epoch 150, step 0, loss 1.0130856037139893, rank IC -0.012559869326651096\n",
      "Epoch 150, step 50, loss 1.3739792108535767, rank IC 0.011288564652204514\n",
      "Epoch 150, step 100, loss 0.8929763436317444, rank IC -0.007385666016489267\n",
      "Epoch 150, step 150, loss 1.7385683059692383, rank IC 0.007369751110672951\n",
      "Epoch 150, step 200, loss 1.1846312284469604, rank IC 0.015510912984609604\n",
      "Epoch 150, step 250, loss 1.002832055091858, rank IC 0.013882224448025227\n",
      "Epoch 150, step 300, loss 1.1605931520462036, rank IC 0.002318676793947816\n",
      "Epoch 150, step 350, loss 1.7646288871765137, rank IC -0.006572230253368616\n",
      "Epoch 151, step 0, loss 1.013084053993225, rank IC -0.012570132501423359\n",
      "Epoch 151, step 50, loss 1.3739053010940552, rank IC 0.011456561274826527\n",
      "Epoch 151, step 100, loss 0.8928408026695251, rank IC -0.007484229747205973\n",
      "Epoch 151, step 150, loss 1.738911509513855, rank IC 0.007366649806499481\n",
      "Epoch 151, step 200, loss 1.1828558444976807, rank IC 0.015582927502691746\n",
      "Epoch 151, step 250, loss 1.0021604299545288, rank IC 0.013907316140830517\n",
      "Epoch 151, step 300, loss 1.1606725454330444, rank IC 0.0023145556915551424\n",
      "Epoch 151, step 350, loss 1.7643095254898071, rank IC -0.006608170922845602\n",
      "Epoch 152, step 0, loss 1.0131382942199707, rank IC -0.012641253881156445\n",
      "Epoch 152, step 50, loss 1.374032735824585, rank IC 0.011410471983253956\n",
      "Epoch 152, step 100, loss 0.8930308222770691, rank IC -0.00748215988278389\n",
      "Epoch 152, step 150, loss 1.7391568422317505, rank IC 0.007336112204939127\n",
      "Epoch 152, step 200, loss 1.187615156173706, rank IC 0.015643881633877754\n",
      "Epoch 152, step 250, loss 1.0018091201782227, rank IC 0.013947668485343456\n",
      "Epoch 152, step 300, loss 1.16062331199646, rank IC 0.0023121170233935118\n",
      "Epoch 152, step 350, loss 1.7639191150665283, rank IC -0.00662568211555481\n",
      "Epoch 153, step 0, loss 1.013338327407837, rank IC -0.012690789997577667\n",
      "Epoch 153, step 50, loss 1.3739721775054932, rank IC 0.011311224661767483\n",
      "Epoch 153, step 100, loss 0.8931853771209717, rank IC -0.00747024267911911\n",
      "Epoch 153, step 150, loss 1.7383867502212524, rank IC 0.007399393245577812\n",
      "Epoch 153, step 200, loss 1.1885075569152832, rank IC 0.01566663384437561\n",
      "Epoch 153, step 250, loss 1.0012688636779785, rank IC 0.013976707123219967\n",
      "Epoch 153, step 300, loss 1.1606050729751587, rank IC 0.0023174670059233904\n",
      "Epoch 153, step 350, loss 1.7638098001480103, rank IC -0.006627158727496862\n",
      "Epoch 154, step 0, loss 1.0133694410324097, rank IC -0.012684823013842106\n",
      "Epoch 154, step 50, loss 1.3739794492721558, rank IC 0.01175021380186081\n",
      "Epoch 154, step 100, loss 0.8931340575218201, rank IC -0.007645603269338608\n",
      "Epoch 154, step 150, loss 1.7382597923278809, rank IC 0.007362840697169304\n",
      "Epoch 154, step 200, loss 1.1981385946273804, rank IC 0.015718145295977592\n",
      "Epoch 154, step 250, loss 1.0010311603546143, rank IC 0.014018681831657887\n",
      "Epoch 154, step 300, loss 1.1605806350708008, rank IC 0.0023157193791121244\n",
      "Epoch 154, step 350, loss 1.763500690460205, rank IC -0.006663423031568527\n",
      "Epoch 155, step 0, loss 1.0134073495864868, rank IC -0.012736561708152294\n",
      "Epoch 155, step 50, loss 1.3739303350448608, rank IC 0.011496759951114655\n",
      "Epoch 155, step 100, loss 0.8929800391197205, rank IC -0.007595523726195097\n",
      "Epoch 155, step 150, loss 1.7371658086776733, rank IC 0.007349139079451561\n",
      "Epoch 155, step 200, loss 1.20000422000885, rank IC 0.015728378668427467\n",
      "Epoch 155, step 250, loss 0.9995403289794922, rank IC 0.014050960540771484\n",
      "Epoch 155, step 300, loss 1.1606577634811401, rank IC 0.002323652384802699\n",
      "Epoch 155, step 350, loss 1.7637943029403687, rank IC -0.006642078515142202\n",
      "Epoch 156, step 0, loss 1.0133343935012817, rank IC -0.01276653353124857\n",
      "Epoch 156, step 50, loss 1.3739532232284546, rank IC 0.01158630196005106\n",
      "Epoch 156, step 100, loss 0.8929879665374756, rank IC -0.007645652163773775\n",
      "Epoch 156, step 150, loss 1.7380646467208862, rank IC 0.0073871673084795475\n",
      "Epoch 156, step 200, loss 1.1849664449691772, rank IC 0.01577659696340561\n",
      "Epoch 156, step 250, loss 0.9999768733978271, rank IC 0.014061853289604187\n",
      "Epoch 156, step 300, loss 1.160584568977356, rank IC 0.0023285348434001207\n",
      "Epoch 156, step 350, loss 1.7636374235153198, rank IC -0.006659700069576502\n",
      "Epoch 157, step 0, loss 1.013566493988037, rank IC -0.012778257019817829\n",
      "Epoch 157, step 50, loss 1.3739867210388184, rank IC 0.011631409637629986\n",
      "Epoch 157, step 100, loss 0.8934139013290405, rank IC -0.007663471158593893\n",
      "Epoch 157, step 150, loss 1.7383414506912231, rank IC 0.00738333398476243\n",
      "Epoch 157, step 200, loss 1.1874959468841553, rank IC 0.015837496146559715\n",
      "Epoch 157, step 250, loss 0.9999247789382935, rank IC 0.014082484878599644\n",
      "Epoch 157, step 300, loss 1.1603959798812866, rank IC 0.00232211803086102\n",
      "Epoch 157, step 350, loss 1.7634778022766113, rank IC -0.006708326283842325\n",
      "Epoch 158, step 0, loss 1.0136064291000366, rank IC -0.012853189371526241\n",
      "Epoch 158, step 50, loss 1.3739094734191895, rank IC 0.011577248573303223\n",
      "Epoch 158, step 100, loss 0.8934156894683838, rank IC -0.007679809350520372\n",
      "Epoch 158, step 150, loss 1.7387707233428955, rank IC 0.0073965466581285\n",
      "Epoch 158, step 200, loss 1.1882199048995972, rank IC 0.016086136922240257\n",
      "Epoch 158, step 250, loss 0.9997093677520752, rank IC 0.014108152128756046\n",
      "Epoch 158, step 300, loss 1.1604034900665283, rank IC 0.0023179238196462393\n",
      "Epoch 158, step 350, loss 1.7628077268600464, rank IC -0.0066795856691896915\n",
      "Epoch 159, step 0, loss 1.013785481452942, rank IC -0.012821569107472897\n",
      "Epoch 159, step 50, loss 1.3738694190979004, rank IC 0.011804004199802876\n",
      "Epoch 159, step 100, loss 0.8934884667396545, rank IC -0.007807427551597357\n",
      "Epoch 159, step 150, loss 1.7389020919799805, rank IC 0.007341098040342331\n",
      "Epoch 159, step 200, loss 1.1870652437210083, rank IC 0.015956977382302284\n",
      "Epoch 159, step 250, loss 0.9990168809890747, rank IC 0.014120968990027905\n",
      "Epoch 159, step 300, loss 1.1603509187698364, rank IC 0.0023224602919071913\n",
      "Epoch 159, step 350, loss 1.7626926898956299, rank IC -0.006688928231596947\n",
      "Epoch 160, step 0, loss 1.013745665550232, rank IC -0.012878094799816608\n",
      "Epoch 160, step 50, loss 1.3738936185836792, rank IC 0.0117168128490448\n",
      "Epoch 160, step 100, loss 0.8934770822525024, rank IC -0.007774885278195143\n",
      "Epoch 160, step 150, loss 1.7396047115325928, rank IC 0.0073614283464848995\n",
      "Epoch 160, step 200, loss 1.1889225244522095, rank IC 0.016062209382653236\n",
      "Epoch 160, step 250, loss 0.9985307455062866, rank IC 0.014161497354507446\n",
      "Epoch 160, step 300, loss 1.1603206396102905, rank IC 0.0023246612399816513\n",
      "Epoch 160, step 350, loss 1.7623467445373535, rank IC -0.006733407732099295\n",
      "Epoch 161, step 0, loss 1.0138851404190063, rank IC -0.01290261372923851\n",
      "Epoch 161, step 50, loss 1.3738893270492554, rank IC 0.01185938622802496\n",
      "Epoch 161, step 100, loss 0.8936019539833069, rank IC -0.007861677557229996\n",
      "Epoch 161, step 150, loss 1.7388818264007568, rank IC 0.007390464190393686\n",
      "Epoch 161, step 200, loss 1.1940048933029175, rank IC 0.015895891934633255\n",
      "Epoch 161, step 250, loss 0.9979503750801086, rank IC 0.014169170521199703\n",
      "Epoch 161, step 300, loss 1.1602122783660889, rank IC 0.002331475494429469\n",
      "Epoch 161, step 350, loss 1.7621110677719116, rank IC -0.006689717527478933\n",
      "Epoch 162, step 0, loss 1.013981819152832, rank IC -0.01293089147657156\n",
      "Epoch 162, step 50, loss 1.3738843202590942, rank IC 0.011714987456798553\n",
      "Epoch 162, step 100, loss 0.8936026692390442, rank IC -0.007799317594617605\n",
      "Epoch 162, step 150, loss 1.7388310432434082, rank IC 0.007414486724883318\n",
      "Epoch 162, step 200, loss 1.1834626197814941, rank IC 0.016076596453785896\n",
      "Epoch 162, step 250, loss 0.9970406889915466, rank IC 0.014219325967133045\n",
      "Epoch 162, step 300, loss 1.1602038145065308, rank IC 0.0023413763847202063\n",
      "Epoch 162, step 350, loss 1.7620763778686523, rank IC -0.006741509307175875\n",
      "Epoch 163, step 0, loss 1.0141687393188477, rank IC -0.012959414161741734\n",
      "Epoch 163, step 50, loss 1.3738431930541992, rank IC 0.011975757777690887\n",
      "Epoch 163, step 100, loss 0.8940003514289856, rank IC -0.00795290432870388\n",
      "Epoch 163, step 150, loss 1.7383500337600708, rank IC 0.007352443411946297\n",
      "Epoch 163, step 200, loss 1.1865849494934082, rank IC 0.016021331772208214\n",
      "Epoch 163, step 250, loss 0.9972782135009766, rank IC 0.014221496880054474\n",
      "Epoch 163, step 300, loss 1.160117268562317, rank IC 0.0023490425664931536\n",
      "Epoch 163, step 350, loss 1.7614223957061768, rank IC -0.006716940551996231\n",
      "Epoch 164, step 0, loss 1.0142602920532227, rank IC -0.01298986654728651\n",
      "Epoch 164, step 50, loss 1.3738466501235962, rank IC 0.011970124207437038\n",
      "Epoch 164, step 100, loss 0.8940670490264893, rank IC -0.00796432513743639\n",
      "Epoch 164, step 150, loss 1.7379599809646606, rank IC 0.007370117586106062\n",
      "Epoch 164, step 200, loss 1.1817076206207275, rank IC 0.01613294892013073\n",
      "Epoch 164, step 250, loss 0.9964831471443176, rank IC 0.014261799864470959\n",
      "Epoch 164, step 300, loss 1.1601074934005737, rank IC 0.0023464858531951904\n",
      "Epoch 164, step 350, loss 1.7614071369171143, rank IC -0.006762944161891937\n",
      "Epoch 165, step 0, loss 1.0143691301345825, rank IC -0.013021244667470455\n",
      "Epoch 165, step 50, loss 1.3738235235214233, rank IC 0.012051994912326336\n",
      "Epoch 165, step 100, loss 0.8941079378128052, rank IC -0.008022434078156948\n",
      "Epoch 165, step 150, loss 1.739452838897705, rank IC 0.007391065824776888\n",
      "Epoch 165, step 200, loss 1.1908140182495117, rank IC 0.016104206442832947\n",
      "Epoch 165, step 250, loss 0.9967109560966492, rank IC 0.014263630844652653\n",
      "Epoch 165, step 300, loss 1.1600239276885986, rank IC 0.0023622966837137938\n",
      "Epoch 165, step 350, loss 1.76091468334198, rank IC -0.006739484611898661\n",
      "Epoch 166, step 0, loss 1.0144060850143433, rank IC -0.0130666084587574\n",
      "Epoch 166, step 50, loss 1.3739153146743774, rank IC 0.011927294544875622\n",
      "Epoch 166, step 100, loss 0.8942589163780212, rank IC -0.007979268208146095\n",
      "Epoch 166, step 150, loss 1.7386831045150757, rank IC 0.007364422082901001\n",
      "Epoch 166, step 200, loss 1.191148281097412, rank IC 0.016192873939871788\n",
      "Epoch 166, step 250, loss 0.9955199956893921, rank IC 0.014328092336654663\n",
      "Epoch 166, step 300, loss 1.1599806547164917, rank IC 0.0023464311379939318\n",
      "Epoch 166, step 350, loss 1.7609233856201172, rank IC -0.006801404058933258\n",
      "Epoch 167, step 0, loss 1.0145379304885864, rank IC -0.013090919703245163\n",
      "Epoch 167, step 50, loss 1.3738313913345337, rank IC 0.011895999312400818\n",
      "Epoch 167, step 100, loss 0.894303560256958, rank IC -0.00796287041157484\n",
      "Epoch 167, step 150, loss 1.7383885383605957, rank IC 0.007371822372078896\n",
      "Epoch 167, step 200, loss 1.1850258111953735, rank IC 0.016251565888524055\n",
      "Epoch 167, step 250, loss 0.9955751299858093, rank IC 0.014331129379570484\n",
      "Epoch 167, step 300, loss 1.1599889993667603, rank IC 0.002361648716032505\n",
      "Epoch 167, step 350, loss 1.7606232166290283, rank IC -0.0067930263467133045\n",
      "Epoch 168, step 0, loss 1.0147075653076172, rank IC -0.013105959631502628\n",
      "Epoch 168, step 50, loss 1.373847246170044, rank IC 0.012087852694094181\n",
      "Epoch 168, step 100, loss 0.8946388363838196, rank IC -0.008077620528638363\n",
      "Epoch 168, step 150, loss 1.7383149862289429, rank IC 0.007370929699391127\n",
      "Epoch 168, step 200, loss 1.1941678524017334, rank IC 0.016167037189006805\n",
      "Epoch 168, step 250, loss 0.9952831268310547, rank IC 0.014354740269482136\n",
      "Epoch 168, step 300, loss 1.1599675416946411, rank IC 0.002358374185860157\n",
      "Epoch 168, step 350, loss 1.7603085041046143, rank IC -0.006792828906327486\n",
      "Epoch 169, step 0, loss 1.0147216320037842, rank IC -0.013154205866158009\n",
      "Epoch 169, step 50, loss 1.373853325843811, rank IC 0.01203109323978424\n",
      "Epoch 169, step 100, loss 0.8944957256317139, rank IC -0.008070448413491249\n",
      "Epoch 169, step 150, loss 1.7378817796707153, rank IC 0.007368733640760183\n",
      "Epoch 169, step 200, loss 1.179604411125183, rank IC 0.0162497591227293\n",
      "Epoch 169, step 250, loss 0.9947172403335571, rank IC 0.014390966854989529\n",
      "Epoch 169, step 300, loss 1.160008430480957, rank IC 0.0023729100357741117\n"
     ]
    }
   ],
   "source": [
    "x_all=torch.as_tensor(h.fetch(col_set=\"feature\").values).reshape(-1,10,1,158).float()\n",
    "print(x_all.type())\n",
    "y_all=torch.as_tensor(h.fetch(col_set=\"label\").values).reshape(-1,10).float()\n",
    "# x_all, y_all = torch.randn((1000, 10, 1, 158)), torch.randn((1000, 10))#NS表示股票数量，T表示时间刻度\n",
    "# xx=np.array([[[[]]]])\n",
    "# for i in range(20,len(x_all)):\n",
    "#     xx=np.append(xx,np.array(x_all)[i-20:i,:,:,:])\n",
    "# xx=xx.reshape(-1,10,20,158)\n",
    "# y_all=y_all[20:]\n",
    "\n",
    "# x_all=xx\n",
    "train_ds = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_all, y_all), batch_size=3)\n",
    "epochs = 500\n",
    "for e in range(1, epochs+1):\n",
    "    for i, (x, y) in enumerate(train_ds):\n",
    "        x=x.float()\n",
    "        y=y.float()\n",
    "        loss, rank_ic = train_step(x, y)\n",
    "        if i%50 == 0:\n",
    "            print(f'Epoch {e}, step {i}, loss {loss}, rank IC {rank_ic}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "a=y_pred[0].detach().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.04122316, 0.01624   , 0.01270811, 0.01508808, 0.01564417,\n       0.01365395, 0.01512384, 0.01130995, 0.01485159, 0.01293275,\n       0.01620272, 0.01592413, 0.01827147, 0.01304155, 0.01073989,\n       0.01428189, 0.01436793, 0.01151943, 0.0106531 , 0.0105264 ,\n       0.01071329, 0.01054199, 0.01061796, 0.01053172, 0.01076764,\n       0.01065698, 0.01060616, 0.01051606, 0.01059769, 0.0105675 ,\n       0.01052126, 0.01071764, 0.01082299, 0.01067096, 0.01367577,\n       0.01159915, 0.01461557, 0.01490788, 0.01610725, 0.09752707,\n       0.04593089, 0.02867398, 0.01771405, 0.01565305, 0.01262739,\n       0.01447232, 0.0132992 , 0.01331814, 0.02265323, 0.01300141,\n       0.01444576, 0.01679984, 0.01397658, 0.01420742, 0.01078932,\n       0.01290425, 0.01506974, 0.01519204, 0.01702126, 0.01906222,\n       0.01345149, 0.01539662, 0.01425351, 0.00915469, 0.04899049,\n       0.01395542, 0.01550213, 0.01149457, 0.03280352, 0.04588123,\n       0.01561379, 0.01722977, 0.01581881, 0.01189341, 0.01468367,\n       0.01653946, 0.01528825, 0.01781991, 0.01437612, 0.01805031,\n       0.01251712, 0.01608917, 0.0136549 , 0.01371008, 0.01097463,\n       0.01509073, 0.01085512, 0.01064844, 0.01072421, 0.01060534,\n       0.01080316, 0.01068727, 0.01063821, 0.01131694, 0.01082811,\n       0.01160906, 0.01424868, 0.01498854, 0.01275672, 0.0114772 ,\n       0.01210084, 0.01354263, 0.01527438, 0.01606527, 0.01610424,\n       0.01714331, 0.01576143, 0.01280566, 0.0164331 , 0.01417414,\n       0.01494701, 0.0165167 , 0.01577347, 0.01518279, 0.02124271,\n       0.00909759, 0.01481646, 0.01150707, 0.03559332, 0.00720216,\n       0.01639839, 0.06635099, 0.07082963, 0.09590542, 0.0306972 ,\n       0.01325336, 0.01164082, 0.02338223, 0.01230716, 0.0107916 ,\n       0.01115647, 0.04211998, 0.01916029, 0.00918056, 0.01151255,\n       0.01176641, 0.01133369, 0.01097292, 0.01287222, 0.01087726,\n       0.01126026, 0.01482741, 0.01259874, 0.03058149, 0.00867112,\n       0.01383727, 0.01280088, 0.01133849, 0.01221234, 0.01885562,\n       0.01534102, 0.02047301, 0.03895806, 0.01586839, 0.01517571,\n       0.01575969, 0.0126956 , 0.01148832, 0.01094752, 0.01096816,\n       0.01602351, 0.01857531, 0.0113757 , 0.01798976, 0.01580656,\n       0.01243285, 0.01326314, 0.01158355, 0.01748426, 0.01280265,\n       0.01539721, 0.01419443, 0.01573772, 0.01280039, 0.01062368,\n       0.01089242, 0.01062257, 0.01127091, 0.01070527, 0.01612622,\n       0.01186697, 0.01210398, 0.0131915 , 0.01969821, 0.01351991,\n       0.01130783, 0.05174328, 0.01200897, 0.01567686, 0.01003821,\n       0.01465594, 0.0117192 , 0.01119402, 0.01706326, 0.01473121,\n       0.01231939, 0.01851036, 0.06079292, 0.00911774, 0.01207485,\n       0.01521677, 0.01485359, 0.01276376, 0.01435159, 0.01184441,\n       0.01693239, 0.02473164, 0.01105184, 0.01604973, 0.0240366 ,\n       0.00874785, 0.0129025 , 0.01316556, 0.01587843, 0.01268711,\n       0.0153845 , 0.01392885, 0.01468182, 0.01375679, 0.01481298,\n       0.01290757, 0.01669876, 0.01526998, 0.01386735, 0.01544229,\n       0.01739825, 0.0161514 , 0.0140965 , 0.01325525, 0.0134682 ,\n       0.01282   , 0.00771387, 0.01415089, 0.01437803, 0.01370433,\n       0.01811184, 0.01410615, 0.01584769, 0.01554211, 0.01542358,\n       0.01471108, 0.01640593, 0.01060895, 0.01225312, 0.01062615,\n       0.01866598, 0.01449325, 0.01099072, 0.01079252, 0.01086721,\n       0.01059982, 0.01073361, 0.01054804, 0.01063933, 0.01065927,\n       0.01048332, 0.01057585, 0.01409494, 0.01062924, 0.01145318,\n       0.01054885, 0.01060712, 0.01060693, 0.01059134, 0.01063371,\n       0.01075876, 0.01058628, 0.01059875, 0.01059998, 0.01062158,\n       0.01130242, 0.01166048, 0.01209185, 0.01174749, 0.01073194,\n       0.0107475 , 0.01062437, 0.01375204, 0.01574555, 0.01350948,\n       0.01830532, 0.01326314, 0.01502777, 0.01262495, 0.01057685,\n       0.01065676, 0.0161619 , 0.0106205 , 0.01050467, 0.01050766,\n       0.01058135, 0.01065999, 0.01064332, 0.01361167, 0.01061346,\n       0.01059194, 0.01049764, 0.01064759, 0.01105515, 0.01067968,\n       0.01084342, 0.0123056 , 0.01518162, 0.01380173, 0.02437886,\n       0.0096939 , 0.01618009, 0.0195065 , 0.0302512 , 0.08967541,\n       0.02286947, 0.02367887, 0.07683181, 0.08458859, 0.00829743,\n       0.03925125, 0.01136252, 0.01439036, 0.01674933, 0.01538973,\n       0.01272255, 0.0122289 , 0.01270262, 0.0168417 , 0.01338619,\n       0.01239912, 0.01116104, 0.01067303, 0.0105031 , 0.01053043,\n       0.01065339, 0.01227516, 0.01063559, 0.01052455, 0.0104958 ,\n       0.01103261, 0.01064732, 0.01065339, 0.01057461, 0.01048957,\n       0.0105318 , 0.01089176, 0.01067078, 0.01242003, 0.01082032,\n       0.01060403, 0.01061843, 0.01048271, 0.010609  , 0.01065401,\n       0.01108248, 0.01239948, 0.0151143 , 0.01076346, 0.01064883,\n       0.01088254, 0.0106058 , 0.01846529, 0.01329061, 0.01238254,\n       0.01182814, 0.01819649, 0.01124595, 0.02754095, 0.01536785,\n       0.01578606, 0.01811107, 0.01266778, 0.0142115 , 0.01727866,\n       0.01766857, 0.01855118, 0.01385342, 0.01327417, 0.01144567,\n       0.01107044, 0.01103393, 0.01105387, 0.01214179, 0.01231881,\n       0.01246119, 0.01061025, 0.01108977, 0.0106327 , 0.01074537,\n       0.01127536, 0.01479424, 0.01062229, 0.01090796, 0.01161779,\n       0.01075105, 0.01119523, 0.01252762, 0.01260944, 0.01739475,\n       0.01169308, 0.01456739, 0.01135552, 0.01098487, 0.01510531,\n       0.01183134, 0.01130643, 0.01275041, 0.0160625 , 0.01325902,\n       0.01419956, 0.01381203, 0.01324139, 0.0144698 , 0.01263016,\n       0.014067  , 0.01634417, 0.01713943, 0.01381236, 0.01064565,\n       0.01141757, 0.01121568, 0.01178749, 0.01488606, 0.01138475,\n       0.01701147, 0.01460978, 0.01228491, 0.01201163, 0.01786992,\n       0.01671352, 0.01659654, 0.01519854, 0.01152199, 0.01721139,\n       0.01522097, 0.0160496 , 0.01607855, 0.01656668, 0.01486101,\n       0.01169702, 0.01107545, 0.01178744, 0.01402645, 0.01073917,\n       0.01730885, 0.01278401, 0.01228211, 0.01084298, 0.01069502,\n       0.01091352, 0.01187352, 0.0106294 , 0.01066439, 0.01063377,\n       0.01172079, 0.01065524, 0.01061813, 0.01080397, 0.01112316,\n       0.01092603, 0.01081592, 0.0106713 , 0.01829946, 0.01519681,\n       0.01630613, 0.01269794, 0.01127639, 0.01069364, 0.01094864,\n       0.01122503, 0.01446328, 0.08395001, 0.0714782 , 0.03130437,\n       0.0084709 , 0.039132  , 0.07035249, 0.01795566, 0.06452613,\n       0.07332227, 0.0161662 , 0.01477275, 0.01497813, 0.01514724,\n       0.01483138, 0.01434972, 0.01424349, 0.01467346, 0.0139807 ,\n       0.0152963 , 0.01190062, 0.01077273, 0.01105369, 0.01432367,\n       0.01595422, 0.01652734, 0.01685783, 0.01214603, 0.00709882,\n       0.0112681 , 0.03319377, 0.01248411, 0.03442484, 0.04377515,\n       0.02006821, 0.02146836, 0.04207742, 0.00881943, 0.00962041,\n       0.01073243, 0.01335263, 0.0128043 , 0.01348613, 0.012018  ,\n       0.01510958, 0.017229  , 0.0159396 , 0.01457558, 0.01440206,\n       0.01260346, 0.01143881, 0.01087549, 0.01082068, 0.01080913,\n       0.01858152, 0.0154946 , 0.01184422, 0.01087933, 0.01429162,\n       0.01123799, 0.011171  , 0.01094502, 0.01700925, 0.01642582,\n       0.01228838, 0.01104004, 0.01107802, 0.01813104, 0.01749653,\n       0.01333163, 0.01200211, 0.01167326, 0.01323588, 0.01319811,\n       0.01080235, 0.0110882 , 0.01878615, 0.01093433, 0.01064551,\n       0.0106056 , 0.01071476, 0.01117233, 0.0106414 , 0.0116743 ,\n       0.01072798, 0.01087435, 0.01059172, 0.0148482 , 0.01656442,\n       0.01433087, 0.01652223, 0.01340403, 0.01061168, 0.01063206,\n       0.01062117, 0.01151173, 0.01062568, 0.01058314, 0.01260102,\n       0.01063978, 0.01058219, 0.01498371, 0.01199615, 0.01082588,\n       0.01070677, 0.01073233, 0.01224646, 0.01670951, 0.01235837,\n       0.01298711, 0.0111053 , 0.01308439, 0.01719209, 0.01475249,\n       0.01361486, 0.01231307, 0.01309873, 0.01208558, 0.01556985,\n       0.0170277 , 0.0139227 , 0.01076551, 0.01057782, 0.01064656,\n       0.01075133, 0.01068725, 0.01069173, 0.0105781 , 0.01122133,\n       0.01059003, 0.01070991, 0.01088163, 0.01351205, 0.01177865,\n       0.01065461, 0.01092802, 0.01071844, 0.01127346, 0.01119666,\n       0.01682   , 0.01252139, 0.01315479, 0.01239699, 0.01070406,\n       0.01264528, 0.01621035, 0.01286683, 0.01287552, 0.01134659,\n       0.0111173 , 0.01212737, 0.01117712, 0.01102012, 0.01129935,\n       0.0108002 , 0.01080744, 0.01056377, 0.01061674, 0.01061717,\n       0.01054907, 0.01066189, 0.01302129, 0.0121891 , 0.01114129,\n       0.01391105, 0.01119052, 0.01087264, 0.01741783, 0.01273462,\n       0.01701252, 0.01275248, 0.01130049, 0.01072352, 0.01286293,\n       0.01080519, 0.01064814, 0.01229778, 0.01065896, 0.01115537,\n       0.01068005, 0.01126891, 0.01627382, 0.01254501, 0.01073502,\n       0.01088936, 0.01066922, 0.01128101, 0.01493728, 0.01482094,\n       0.0126737 , 0.01402712], dtype=float32)"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a[a[0]<0]=-1\n",
    "a[a[0]>0]=1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot((np.array(y)*a)[:,9].cumsum())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.constant(np.nan_to_num(h.fetch(col_set=\"feature\").values,0).reshape(-1,10,1,158))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=np.array([[[[]]]])\n",
    "for i in range(20,len(x)):\n",
    "    xx=np.append(xx,np.array(x)[i-20:i,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=xx.reshape(-1,10,20,158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "a=[1,2,3,4,5,6]\n",
    "looked=[1 for i in range(len(a))]\n",
    "g=[]\n",
    "def dfs(ii,iin):\n",
    "    for i in range(ii+1,len(a)):\n",
    "        if(a[i]%a[ii]==0 and looked[i]==1):\n",
    "            if(ii==iin):\n",
    "                g.append(1)\n",
    "                looked[i]=0\n",
    "                dfs(ii,iin)\n",
    "            else:\n",
    "                looked[i]=0\n",
    "                g[len(g)-1]=2*(g[len(g)-1]+1)\n",
    "                dfs(ii,iin)\n",
    "\n",
    "for i in range(len(a)):\n",
    "    if(looked[i]==1):\n",
    "        looked[i]=0\n",
    "        dfs(i,i)\n",
    "\n",
    "print(sum(g))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAuthenticationError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/9h/zxjknhr917q09gsp6bbyrkqw0000gn/T/ipykernel_93301/1953526776.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m#openai.organization = \"YOUR_ORG_ID\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mopenai\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapi_key\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetenv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"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\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mopenai\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mModel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_resources/abstract/listable_api_resource.py\u001B[0m in \u001B[0;36mlist\u001B[0;34m(cls, api_key, request_id, api_version, organization, api_base, api_type, **params)\u001B[0m\n\u001B[1;32m     50\u001B[0m         \u001B[0;34m**\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     51\u001B[0m     ):\n\u001B[0;32m---> 52\u001B[0;31m         requestor, url = cls.__prepare_list_requestor(\n\u001B[0m\u001B[1;32m     53\u001B[0m             \u001B[0mapi_key\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     54\u001B[0m             \u001B[0mapi_version\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_resources/abstract/listable_api_resource.py\u001B[0m in \u001B[0;36m__prepare_list_requestor\u001B[0;34m(cls, api_key, api_version, organization, api_base, api_type)\u001B[0m\n\u001B[1;32m     18\u001B[0m         \u001B[0mapi_type\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m     ):\n\u001B[0;32m---> 20\u001B[0;31m         requestor = api_requestor.APIRequestor(\n\u001B[0m\u001B[1;32m     21\u001B[0m             \u001B[0mapi_key\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m             \u001B[0mapi_base\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mapi_base\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapi_base\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, key, api_base, api_type, api_version, organization)\u001B[0m\n\u001B[1;32m    128\u001B[0m     ):\n\u001B[1;32m    129\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapi_base\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mapi_base\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mopenai\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapi_base\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 130\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapi_key\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mutil\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdefault_api_key\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    131\u001B[0m         self.api_type = (\n\u001B[1;32m    132\u001B[0m             \u001B[0mApiType\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_str\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mapi_type\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/util.py\u001B[0m in \u001B[0;36mdefault_api_key\u001B[0;34m()\u001B[0m\n\u001B[1;32m    184\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mopenai\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapi_key\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    185\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 186\u001B[0;31m         raise openai.error.AuthenticationError(\n\u001B[0m\u001B[1;32m    187\u001B[0m             \u001B[0;34m\"No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    188\u001B[0m         )\n",
      "\u001B[0;31mAuthenticationError\u001B[0m: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "#openai.organization = \"YOUR_ORG_ID\"\n",
    "openai.api_key = os.getenv(\"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\")\n",
    "openai.Model.list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "def get_stock_data(ticker, start_date, end_date):\n",
    "    stock = yf.Ticker(ticker)\n",
    "    data = stock.history(start=start_date, end=end_date)\n",
    "    data = data.reset_index()\n",
    "    data = data[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "    data['Date'] = data['Date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "    data = data.rename(columns={'Date': 'day', 'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume': 'volume'})\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- AAPL: No data found for this date range, symbol may be delisted\n"
     ]
    }
   ],
   "source": [
    "data = get_stock_data('AAPL', '2023-01-01', '2023-02-01')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "login success!\n",
      "           date       code     open     high      low    close preclose  \\\n",
      "0    2020-01-02  sh.600000  12.4700  12.6400  12.4500  12.4700  12.3700   \n",
      "1    2020-01-03  sh.600000  12.5700  12.6300  12.4700  12.6000  12.4700   \n",
      "2    2020-01-06  sh.600000  12.5200  12.6500  12.4200  12.4600  12.6000   \n",
      "3    2020-01-07  sh.600000  12.5100  12.6000  12.4600  12.5000  12.4600   \n",
      "4    2020-01-08  sh.600000  12.4100  12.4500  12.2500  12.3200  12.5000   \n",
      "..          ...        ...      ...      ...      ...      ...      ...   \n",
      "238  2020-12-25  sh.600000   9.5600   9.6100   9.5100   9.5800   9.5500   \n",
      "239  2020-12-28  sh.600000   9.5500   9.6300   9.5000   9.5700   9.5800   \n",
      "240  2020-12-29  sh.600000   9.5900   9.6200   9.5200   9.5300   9.5700   \n",
      "241  2020-12-30  sh.600000   9.5200   9.5400   9.4400   9.5000   9.5300   \n",
      "242  2020-12-31  sh.600000   9.5100   9.6900   9.4900   9.6800   9.5000   \n",
      "\n",
      "       volume          amount adjustflag      turn tradestatus     pctChg isST  \n",
      "0    51629079  647446166.0000          3  0.183700           1   0.808400    0  \n",
      "1    38018810  477053357.0000          3  0.135300           1   1.042500    0  \n",
      "2    41001193  514432551.0000          3  0.145900           1  -1.111100    0  \n",
      "3    28421482  355811756.0000          3  0.101100           1   0.321000    0  \n",
      "4    35240536  434980266.0000          3  0.125400           1  -1.440000    0  \n",
      "..        ...             ...        ...       ...         ...        ...  ...  \n",
      "238  22420976  214427367.0900          3  0.076400           1   0.314100    0  \n",
      "239  33765935  322826382.2700          3  0.115000           1  -0.104400    0  \n",
      "240  44150648  421945425.9600          3  0.150400           1  -0.418000    0  \n",
      "241  54885564  519975315.1900          3  0.187000           1  -0.314800    0  \n",
      "242  65992362  635666178.4700          3  0.224800           1   1.894700    0  \n",
      "\n",
      "[243 rows x 14 columns]\n",
      "logout success!\n"
     ]
    },
    {
     "data": {
      "text/plain": "<baostock.data.resultset.ResultData at 0x7feff07314f0>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import baostock as bs\n",
    "import pandas as pd\n",
    "\n",
    "# 登陆系统\n",
    "lg = bs.login()\n",
    "\n",
    "# 获取沪深A股历史K线数据\n",
    "rs = bs.query_history_k_data_plus(\"sh.600000\",\n",
    "    \"date,code,open,high,low,close,preclose,volume,amount,adjustflag,turn,tradestatus,pctChg,isST\",\n",
    "    start_date='2020-01-01', end_date='2020-12-31',\n",
    "    frequency=\"d\", adjustflag=\"3\")\n",
    "# 打印结果集\n",
    "data_list = []\n",
    "while (rs.error_code == '0') & rs.next():\n",
    "    # 获取一条记录，将记录合并在一起\n",
    "    data_list.append(rs.get_row_data())\n",
    "result = pd.DataFrame(data_list, columns=rs.fields)\n",
    "# 结果集输出到csv文件\n",
    "print(result)\n",
    "\n",
    "# 登出系统\n",
    "bs.logout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "login success!\n",
      "           date       code    open    high     low   close preclose    volume  \\\n",
      "0    2020-01-02  sh.600517  8.4200  8.4800  8.1800  8.1900   8.2400  33068491   \n",
      "1    2020-01-03  sh.600517  8.2800  8.3200  8.0200  8.0400   8.1900  29015059   \n",
      "2    2020-01-06  sh.600517  7.8800  8.2100  7.8400  7.9700   8.0400  23737895   \n",
      "3    2020-01-07  sh.600517  7.9800  8.0800  7.9200  8.0000   7.9700  19092463   \n",
      "4    2020-01-08  sh.600517  7.9600  7.9900  7.6600  7.6900   8.0000  25226745   \n",
      "..          ...        ...     ...     ...     ...     ...      ...       ...   \n",
      "238  2020-12-25  sh.600517  6.0100  6.2500  5.9200  6.1300   5.9500  41788970   \n",
      "239  2020-12-28  sh.600517  6.0600  6.1600  5.9200  5.9600   6.1300  33584014   \n",
      "240  2020-12-29  sh.600517  5.9800  6.0600  5.7600  5.8000   5.9600  27984818   \n",
      "241  2020-12-30  sh.600517  5.7900  5.8500  5.6800  5.7400   5.8000  20261105   \n",
      "242  2020-12-31  sh.600517  5.7900  6.2700  5.7700  6.0800   5.7400  41296552   \n",
      "\n",
      "             amount adjustflag      turn tradestatus     pctChg isST  \n",
      "0    274241611.0000          3  2.438400           1  -0.606800    0  \n",
      "1    235758495.0000          3  2.139500           1  -1.831500    0  \n",
      "2    189987615.0000          3  1.750400           1  -0.870600    0  \n",
      "3    152631147.0000          3  1.407800           1   0.376400    0  \n",
      "4    197445032.0000          3  1.860100           1  -3.875000    0  \n",
      "..              ...        ...       ...         ...        ...  ...  \n",
      "238  256316478.3900          3  2.370300           1   3.025200    0  \n",
      "239  201583804.8800          3  1.904900           1  -2.773200    0  \n",
      "240  165330112.9900          3  1.587300           1  -2.684600    0  \n",
      "241  117053785.4200          3  1.149200           1  -1.034500    0  \n",
      "242  250937411.5000          3  2.342400           1   5.923300    0  \n",
      "\n",
      "[243 rows x 14 columns]\n",
      "logout success!\n"
     ]
    },
    {
     "data": {
      "text/plain": "<baostock.data.resultset.ResultData at 0x7feff0710b80>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import baostock as bs\n",
    "import pandas as pd\n",
    "\n",
    "# 登陆系统\n",
    "lg = bs.login()\n",
    "\n",
    "# 获取沪深A股历史K线数据\n",
    "rs = bs.query_history_k_data_plus(\"sh.600517\",\n",
    "                                  \"date,code,open,high,low,close,preclose,volume,amount,adjustflag,turn,tradestatus,pctChg,isST\",\n",
    "                                  start_date='2020-01-01', end_date='2020-12-31',\n",
    "                                  frequency=\"d\", adjustflag=\"3\")\n",
    "# 打印结果集\n",
    "data_list = []\n",
    "while (rs.error_code == '0') & rs.next():\n",
    "    # 获取一条记录，将记录合并在一起\n",
    "    data_list.append(rs.get_row_data())\n",
    "result1 = pd.DataFrame(data_list, columns=rs.fields)\n",
    "# 结果集输出到csv文件\n",
    "print(result1)\n",
    "\n",
    "# 登出系统\n",
    "bs.logout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of ['date'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/9h/zxjknhr917q09gsp6bbyrkqw0000gn/T/ipykernel_2088/2064982992.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mvalues\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'date'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m                 )\n\u001B[0;32m--> 311\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001B[0m in \u001B[0;36mset_index\u001B[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001B[0m\n\u001B[1;32m   5449\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5450\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mmissing\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 5451\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"None of {missing} are in the columns\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   5452\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5453\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0minplace\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: \"None of ['date'] are in the columns\""
     ]
    }
   ],
   "source": [
    "result=result.set_index('date')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "result1=result1.set_index('date')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "v=result.values\n",
    "v1=result1.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "vv=np.array([])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "for i in range(20,len(v)):\n",
    "    vv=np.append(vv,np.append(v[i-20:i],v1[i-20:i]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[['sh.600000', '12.4700', '12.6400', ..., '1', '0.808400', '0'],\n         ['sh.600000', '12.5700', '12.6300', ..., '1', '1.042500', '0'],\n         ['sh.600000', '12.5200', '12.6500', ..., '1', '-1.111100',\n          '0'],\n         ...,\n         ['sh.600000', '10.5100', '10.7400', ..., '1', '1.814700', '0'],\n         ['sh.600000', '10.6900', '10.7800', ..., '1', '0.375200', '0'],\n         ['sh.600000', '10.8100', '10.8600', ..., '1', '0.841100', '0']],\n\n        [['sh.600517', '8.4200', '8.4800', ..., '1', '-0.606800', '0'],\n         ['sh.600517', '8.2800', '8.3200', ..., '1', '-1.831500', '0'],\n         ['sh.600517', '7.8800', '8.2100', ..., '1', '-0.870600', '0'],\n         ...,\n         ['sh.600517', '5.9900', '6.3300', ..., '1', '-6.193400', '0'],\n         ['sh.600517', '6.2300', '6.4700', ..., '1', '0.644100', '0'],\n         ['sh.600517', '6.2700', '6.4100', ..., '1', '2.240000', '0']]],\n\n\n       [[['sh.600000', '12.5700', '12.6300', ..., '1', '1.042500', '0'],\n         ['sh.600000', '12.5200', '12.6500', ..., '1', '-1.111100',\n          '0'],\n         ['sh.600000', '12.5100', '12.6000', ..., '1', '0.321000', '0'],\n         ...,\n         ['sh.600000', '10.6900', '10.7800', ..., '1', '0.375200', '0'],\n         ['sh.600000', '10.8100', '10.8600', ..., '1', '0.841100', '0'],\n         ['sh.600000', '10.7300', '10.9100', ..., '1', '0.648700', '0']],\n\n        [['sh.600517', '8.2800', '8.3200', ..., '1', '-1.831500', '0'],\n         ['sh.600517', '7.8800', '8.2100', ..., '1', '-0.870600', '0'],\n         ['sh.600517', '7.9800', '8.0800', ..., '1', '0.376400', '0'],\n         ...,\n         ['sh.600517', '6.2300', '6.4700', ..., '1', '0.644100', '0'],\n         ['sh.600517', '6.2700', '6.4100', ..., '1', '2.240000', '0'],\n         ['sh.600517', '6.8400', '6.8400', ..., '1', '3.912400', '0']]],\n\n\n       [[['sh.600000', '12.5200', '12.6500', ..., '1', '-1.111100',\n          '0'],\n         ['sh.600000', '12.5100', '12.6000', ..., '1', '0.321000', '0'],\n         ['sh.600000', '12.4100', '12.4500', ..., '1', '-1.440000',\n          '0'],\n         ...,\n         ['sh.600000', '10.8100', '10.8600', ..., '1', '0.841100', '0'],\n         ['sh.600000', '10.7300', '10.9100', ..., '1', '0.648700', '0'],\n         ['sh.600000', '10.7600', '10.8400', ..., '1', '-0.828700',\n          '0']],\n\n        [['sh.600517', '7.8800', '8.2100', ..., '1', '-0.870600', '0'],\n         ['sh.600517', '7.9800', '8.0800', ..., '1', '0.376400', '0'],\n         ['sh.600517', '7.9600', '7.9900', ..., '1', '-3.875000', '0'],\n         ...,\n         ['sh.600517', '6.2700', '6.4100', ..., '1', '2.240000', '0'],\n         ['sh.600517', '6.8400', '6.8400', ..., '1', '3.912400', '0'],\n         ['sh.600517', '6.5700', '6.7300', ..., '1', '0.451800', '0']]],\n\n\n       ...,\n\n\n       [[['sh.600000', '10.0800', '10.3500', ..., '1', '2.385700', '0'],\n         ['sh.600000', '10.2600', '10.3900', ..., '1', '-0.485400',\n          '0'],\n         ['sh.600000', '10.2300', '10.2800', ..., '1', '-0.097600',\n          '0'],\n         ...,\n         ['sh.600000', '9.5500', '9.6600', ..., '1', '0.000000', '0'],\n         ['sh.600000', '9.5600', '9.6100', ..., '1', '0.314100', '0'],\n         ['sh.600000', '9.5500', '9.6300', ..., '1', '-0.104400', '0']],\n\n        [['sh.600517', '6.0800', '6.2600', ..., '1', '0.326300', '0'],\n         ['sh.600517', '6.1400', '6.2200', ..., '1', '0.650400', '0'],\n         ['sh.600517', '6.1500', '6.5500', ..., '1', '1.938600', '0'],\n         ...,\n         ['sh.600517', '6.0100', '6.2700', ..., '1', '-2.298900', '0'],\n         ['sh.600517', '6.0100', '6.2500', ..., '1', '3.025200', '0'],\n         ['sh.600517', '6.0600', '6.1600', ..., '1', '-2.773200', '0']]],\n\n\n       [[['sh.600000', '10.2600', '10.3900', ..., '1', '-0.485400',\n          '0'],\n         ['sh.600000', '10.2300', '10.2800', ..., '1', '-0.097600',\n          '0'],\n         ['sh.600000', '10.2400', '10.2500', ..., '1', '-0.683600',\n          '0'],\n         ...,\n         ['sh.600000', '9.5600', '9.6100', ..., '1', '0.314100', '0'],\n         ['sh.600000', '9.5500', '9.6300', ..., '1', '-0.104400', '0'],\n         ['sh.600000', '9.5900', '9.6200', ..., '1', '-0.418000', '0']],\n\n        [['sh.600517', '6.1400', '6.2200', ..., '1', '0.650400', '0'],\n         ['sh.600517', '6.1500', '6.5500', ..., '1', '1.938600', '0'],\n         ['sh.600517', '6.2800', '6.3800', ..., '1', '0.158500', '0'],\n         ...,\n         ['sh.600517', '6.0100', '6.2500', ..., '1', '3.025200', '0'],\n         ['sh.600517', '6.0600', '6.1600', ..., '1', '-2.773200', '0'],\n         ['sh.600517', '5.9800', '6.0600', ..., '1', '-2.684600', '0']]],\n\n\n       [[['sh.600000', '10.2300', '10.2800', ..., '1', '-0.097600',\n          '0'],\n         ['sh.600000', '10.2400', '10.2500', ..., '1', '-0.683600',\n          '0'],\n         ['sh.600000', '10.1100', '10.1700', ..., '1', '-1.769900',\n          '0'],\n         ...,\n         ['sh.600000', '9.5500', '9.6300', ..., '1', '-0.104400', '0'],\n         ['sh.600000', '9.5900', '9.6200', ..., '1', '-0.418000', '0'],\n         ['sh.600000', '9.5200', '9.5400', ..., '1', '-0.314800', '0']],\n\n        [['sh.600517', '6.1500', '6.5500', ..., '1', '1.938600', '0'],\n         ['sh.600517', '6.2800', '6.3800', ..., '1', '0.158500', '0'],\n         ['sh.600517', '6.3200', '6.3200', ..., '1', '-2.215200', '0'],\n         ...,\n         ['sh.600517', '6.0600', '6.1600', ..., '1', '-2.773200', '0'],\n         ['sh.600517', '5.9800', '6.0600', ..., '1', '-2.684600', '0'],\n         ['sh.600517', '5.7900', '5.8500', ..., '1', '-1.034500', '0']]]],\n      dtype=object)"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vv.reshape([-1,2,20,13])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can't find a freq from [] that can resample to day!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/data.py\u001B[0m in \u001B[0;36mfeatures\u001B[0;34m(self, instruments, fields, start_time, end_time, freq, disk_cache, inst_processors)\u001B[0m\n\u001B[1;32m   1184\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1185\u001B[0;31m             return DatasetD.dataset(\n\u001B[0m\u001B[1;32m   1186\u001B[0m                 \u001B[0minstruments\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfields\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfreq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdisk_cache\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minst_processors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minst_processors\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: dataset() got multiple values for argument 'inst_processors'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/9h/zxjknhr917q09gsp6bbyrkqw0000gn/T/ipykernel_2088/1932623150.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0;31m# \"instruments\": ['SH000923','SH600015','SH600019','SH600028','SH600030','SH600036','SH600048','SH600050','SH600104','SH600519'],\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m }\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0mh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mAlpha158\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mdata_handler_config\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/contrib/data/handler.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, instruments, start_time, end_time, freq, infer_processors, learn_processors, fit_start_time, fit_end_time, process_type, filter_pipe, inst_processor, **kwargs)\u001B[0m\n\u001B[1;32m    169\u001B[0m             },\n\u001B[1;32m    170\u001B[0m         }\n\u001B[0;32m--> 171\u001B[0;31m         super().__init__(\n\u001B[0m\u001B[1;32m    172\u001B[0m             \u001B[0minstruments\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minstruments\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    173\u001B[0m             \u001B[0mstart_time\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstart_time\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/dataset/handler.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, instruments, start_time, end_time, data_loader, infer_processors, learn_processors, shared_processors, process_type, drop_raw, **kwargs)\u001B[0m\n\u001B[1;32m    432\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprocess_type\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprocess_type\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    433\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop_raw\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdrop_raw\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 434\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minstruments\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    435\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    436\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget_all_processors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/dataset/handler.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, instruments, start_time, end_time, data_loader, init_data, fetch_orig)\u001B[0m\n\u001B[1;32m     95\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0minit_data\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     96\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mTimeInspector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Init data\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 97\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msetup_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     98\u001B[0m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/dataset/handler.py\u001B[0m in \u001B[0;36msetup_data\u001B[0;34m(self, init_type, **kwargs)\u001B[0m\n\u001B[1;32m    568\u001B[0m         \"\"\"\n\u001B[1;32m    569\u001B[0m         \u001B[0;31m# init raw data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 570\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msetup_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    571\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    572\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mTimeInspector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"fit & process data\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/dataset/handler.py\u001B[0m in \u001B[0;36msetup_data\u001B[0;34m(self, enable_cache)\u001B[0m\n\u001B[1;32m    139\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mTimeInspector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Loading data\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m             \u001B[0;31m# make sure the fetch method is based on a index-sorted pd.DataFrame\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 141\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlazy_sort_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata_loader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minstruments\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstart_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mend_time\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    142\u001B[0m         \u001B[0;31m# TODO: cache\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/dataset/loader.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(self, instruments, start_time, end_time)\u001B[0m\n\u001B[1;32m    133\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_group\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    134\u001B[0m             df = pd.concat(\n\u001B[0;32m--> 135\u001B[0;31m                 {\n\u001B[0m\u001B[1;32m    136\u001B[0m                     \u001B[0mgrp\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_group_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minstruments\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexprs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnames\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    137\u001B[0m                     \u001B[0;32mfor\u001B[0m \u001B[0mgrp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mexprs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnames\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfields\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/dataset/loader.py\u001B[0m in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    134\u001B[0m             df = pd.concat(\n\u001B[1;32m    135\u001B[0m                 {\n\u001B[0;32m--> 136\u001B[0;31m                     \u001B[0mgrp\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_group_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minstruments\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexprs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnames\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    137\u001B[0m                     \u001B[0;32mfor\u001B[0m \u001B[0mgrp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mexprs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnames\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfields\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    138\u001B[0m                 },\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/dataset/loader.py\u001B[0m in \u001B[0;36mload_group_df\u001B[0;34m(self, instruments, exprs, names, start_time, end_time, gp_name)\u001B[0m\n\u001B[1;32m    209\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    210\u001B[0m         \u001B[0mfreq\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfreq\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mgp_name\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfreq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfreq\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 211\u001B[0;31m         df = D.features(\n\u001B[0m\u001B[1;32m    212\u001B[0m             \u001B[0minstruments\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexprs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfreq\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfreq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minst_processors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minst_processor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgp_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    213\u001B[0m         )\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/data.py\u001B[0m in \u001B[0;36mfeatures\u001B[0;34m(self, instruments, fields, start_time, end_time, freq, disk_cache, inst_processors)\u001B[0m\n\u001B[1;32m   1187\u001B[0m             )\n\u001B[1;32m   1188\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1189\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mDatasetD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minstruments\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfields\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfreq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minst_processors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minst_processors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1190\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1191\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/data.py\u001B[0m in \u001B[0;36mdataset\u001B[0;34m(self, instruments, fields, start_time, end_time, freq, inst_processors)\u001B[0m\n\u001B[1;32m    913\u001B[0m             \u001B[0;31m# NOTE: if the frequency is a fixed value.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    914\u001B[0m             \u001B[0;31m# align the data to fixed calendar point\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 915\u001B[0;31m             \u001B[0mcal\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCal\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcalendar\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstart_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfreq\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    916\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcal\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    917\u001B[0m                 return pd.DataFrame(\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/data.py\u001B[0m in \u001B[0;36mcalendar\u001B[0;34m(self, start_time, end_time, freq, future)\u001B[0m\n\u001B[1;32m     89\u001B[0m             \u001B[0mcalendar\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m         \"\"\"\n\u001B[0;32m---> 91\u001B[0;31m         \u001B[0m_calendar\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_calendar_index\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_calendar\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfreq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfuture\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     92\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mstart_time\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"None\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     93\u001B[0m             \u001B[0mstart_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/data.py\u001B[0m in \u001B[0;36m_get_calendar\u001B[0;34m(self, freq, future)\u001B[0m\n\u001B[1;32m    170\u001B[0m         \u001B[0mflag\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mf\"{freq}_future_{future}\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    171\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mflag\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mH\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"c\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 172\u001B[0;31m             \u001B[0m_calendar\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_calendar\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfreq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfuture\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    173\u001B[0m             \u001B[0m_calendar_index\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_calendar\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m}\u001B[0m  \u001B[0;31m# for fast search\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    174\u001B[0m             \u001B[0mH\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"c\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mflag\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_calendar\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_calendar_index\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/data.py\u001B[0m in \u001B[0;36mload_calendar\u001B[0;34m(self, freq, future)\u001B[0m\n\u001B[1;32m    660\u001B[0m         \"\"\"\n\u001B[1;32m    661\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 662\u001B[0;31m             \u001B[0mbackend_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackend_obj\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfreq\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfreq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfuture\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfuture\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    663\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    664\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mfuture\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/storage/file_storage.py\u001B[0m in \u001B[0;36mdata\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    122\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    123\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mList\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mCalVT\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 124\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcheck\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    125\u001B[0m         \u001B[0;31m# If cache is enabled, then return cache directly\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    126\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menable_read_cache\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/storage/file_storage.py\u001B[0m in \u001B[0;36mcheck\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     70\u001B[0m         \u001B[0mValueError\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     71\u001B[0m         \"\"\"\n\u001B[0;32m---> 72\u001B[0;31m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muri\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     73\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{self.storage_name} not exists: {self.uri}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     74\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/storage/file_storage.py\u001B[0m in \u001B[0;36muri\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0muri\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mPath\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 120\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdpm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_data_uri\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_freq_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoinpath\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{self.storage_name}s\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfile_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    121\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/storage/file_storage.py\u001B[0m in \u001B[0;36m_freq_file\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     99\u001B[0m                 \u001B[0mfreq\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mFreq\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_recent_freq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfreq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msupport_freq\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    100\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mfreq\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 101\u001B[0;31m                     \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"can't find a freq from {self.support_freq} that can resample to {self.freq}!\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    102\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_freq_file_cache\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfreq\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    103\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_freq_file_cache\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: can't find a freq from [] that can resample to day!"
     ]
    }
   ],
   "source": [
    "data_handler_config = {\n",
    "    \"start_time\": \"2020-12-11\",\n",
    "    \"end_time\": \"2021-06-10\",\n",
    "    \"instruments\": ['SH000923']\n",
    "    # \"instruments\": ['SH000923','SH600015','SH600019','SH600028','SH600030','SH600036','SH600048','SH600050','SH600104','SH600519'],\n",
    "}\n",
    "h = Alpha158(**data_handler_config)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2088:MainThread](2023-03-26 16:05:16,100) INFO - qlib.Initialization - [config.py:402] - default_conf: client.\n",
      "[2088:MainThread](2023-03-26 16:05:16,101) WARNING - qlib.Initialization - [__init__.py:64] - auto_path is False, please make sure None is mounted\n",
      "[2088:MainThread](2023-03-26 16:05:16,103) INFO - qlib.Initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\n",
      "[2088:MainThread](2023-03-26 16:05:16,104) INFO - qlib.Initialization - [__init__.py:76] - data_path={'__DEFAULT_FREQ': PosixPath('/Users/linweiqiang/PycharmProjects/myidea/C:\\\\Users\\\\linweiqiang_intern\\\\.qlib\\\\data\\\\cn_data')}\n"
     ]
    }
   ],
   "source": [
    "import qlib\n",
    "from qlib.config import REG_CN\n",
    "from qlib.data.dataset.loader import QlibDataLoader\n",
    "\n",
    "from qlib.contrib.data.handler import Alpha158\n",
    "\n",
    "qlib.init(provider_uri=r'C:\\Users\\linweiqiang_intern\\.qlib\\data\\cn_data', region=REG_CN)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can't find a freq from [] that can resample to day!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/data.py\u001B[0m in \u001B[0;36mfeatures\u001B[0;34m(self, instruments, fields, start_time, end_time, freq, disk_cache, inst_processors)\u001B[0m\n\u001B[1;32m   1184\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1185\u001B[0;31m             return DatasetD.dataset(\n\u001B[0m\u001B[1;32m   1186\u001B[0m                 \u001B[0minstruments\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfields\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfreq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdisk_cache\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minst_processors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minst_processors\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: dataset() got multiple values for argument 'inst_processors'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/9h/zxjknhr917q09gsp6bbyrkqw0000gn/T/ipykernel_2088/504094443.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0;34m\"instruments\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'SH000923'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'SH600015'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'SH600019'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'SH600028'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'SH600030'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'SH600036'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'SH600048'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'SH600050'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'SH600104'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'SH600519'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m }\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0mh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mAlpha158\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mdata_handler_config\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/contrib/data/handler.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, instruments, start_time, end_time, freq, infer_processors, learn_processors, fit_start_time, fit_end_time, process_type, filter_pipe, inst_processor, **kwargs)\u001B[0m\n\u001B[1;32m    169\u001B[0m             },\n\u001B[1;32m    170\u001B[0m         }\n\u001B[0;32m--> 171\u001B[0;31m         super().__init__(\n\u001B[0m\u001B[1;32m    172\u001B[0m             \u001B[0minstruments\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minstruments\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    173\u001B[0m             \u001B[0mstart_time\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstart_time\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/dataset/handler.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, instruments, start_time, end_time, data_loader, infer_processors, learn_processors, shared_processors, process_type, drop_raw, **kwargs)\u001B[0m\n\u001B[1;32m    432\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprocess_type\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprocess_type\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    433\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop_raw\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdrop_raw\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 434\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minstruments\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    435\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    436\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget_all_processors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/dataset/handler.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, instruments, start_time, end_time, data_loader, init_data, fetch_orig)\u001B[0m\n\u001B[1;32m     95\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0minit_data\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     96\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mTimeInspector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Init data\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 97\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msetup_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     98\u001B[0m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/dataset/handler.py\u001B[0m in \u001B[0;36msetup_data\u001B[0;34m(self, init_type, **kwargs)\u001B[0m\n\u001B[1;32m    568\u001B[0m         \"\"\"\n\u001B[1;32m    569\u001B[0m         \u001B[0;31m# init raw data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 570\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msetup_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    571\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    572\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mTimeInspector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"fit & process data\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/dataset/handler.py\u001B[0m in \u001B[0;36msetup_data\u001B[0;34m(self, enable_cache)\u001B[0m\n\u001B[1;32m    139\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mTimeInspector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Loading data\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m             \u001B[0;31m# make sure the fetch method is based on a index-sorted pd.DataFrame\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 141\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlazy_sort_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata_loader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minstruments\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstart_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mend_time\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    142\u001B[0m         \u001B[0;31m# TODO: cache\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/dataset/loader.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(self, instruments, start_time, end_time)\u001B[0m\n\u001B[1;32m    133\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_group\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    134\u001B[0m             df = pd.concat(\n\u001B[0;32m--> 135\u001B[0;31m                 {\n\u001B[0m\u001B[1;32m    136\u001B[0m                     \u001B[0mgrp\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_group_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minstruments\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexprs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnames\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    137\u001B[0m                     \u001B[0;32mfor\u001B[0m \u001B[0mgrp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mexprs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnames\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfields\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/dataset/loader.py\u001B[0m in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    134\u001B[0m             df = pd.concat(\n\u001B[1;32m    135\u001B[0m                 {\n\u001B[0;32m--> 136\u001B[0;31m                     \u001B[0mgrp\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_group_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minstruments\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexprs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnames\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    137\u001B[0m                     \u001B[0;32mfor\u001B[0m \u001B[0mgrp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mexprs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnames\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfields\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    138\u001B[0m                 },\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/dataset/loader.py\u001B[0m in \u001B[0;36mload_group_df\u001B[0;34m(self, instruments, exprs, names, start_time, end_time, gp_name)\u001B[0m\n\u001B[1;32m    209\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    210\u001B[0m         \u001B[0mfreq\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfreq\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mgp_name\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfreq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfreq\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 211\u001B[0;31m         df = D.features(\n\u001B[0m\u001B[1;32m    212\u001B[0m             \u001B[0minstruments\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexprs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfreq\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfreq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minst_processors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minst_processor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgp_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    213\u001B[0m         )\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/data.py\u001B[0m in \u001B[0;36mfeatures\u001B[0;34m(self, instruments, fields, start_time, end_time, freq, disk_cache, inst_processors)\u001B[0m\n\u001B[1;32m   1187\u001B[0m             )\n\u001B[1;32m   1188\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1189\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mDatasetD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minstruments\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfields\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfreq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minst_processors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minst_processors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1190\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1191\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/data.py\u001B[0m in \u001B[0;36mdataset\u001B[0;34m(self, instruments, fields, start_time, end_time, freq, inst_processors)\u001B[0m\n\u001B[1;32m    913\u001B[0m             \u001B[0;31m# NOTE: if the frequency is a fixed value.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    914\u001B[0m             \u001B[0;31m# align the data to fixed calendar point\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 915\u001B[0;31m             \u001B[0mcal\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCal\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcalendar\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstart_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend_time\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfreq\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    916\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcal\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    917\u001B[0m                 return pd.DataFrame(\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/data.py\u001B[0m in \u001B[0;36mcalendar\u001B[0;34m(self, start_time, end_time, freq, future)\u001B[0m\n\u001B[1;32m     89\u001B[0m             \u001B[0mcalendar\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m         \"\"\"\n\u001B[0;32m---> 91\u001B[0;31m         \u001B[0m_calendar\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_calendar_index\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_calendar\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfreq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfuture\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     92\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mstart_time\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"None\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     93\u001B[0m             \u001B[0mstart_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/data.py\u001B[0m in \u001B[0;36m_get_calendar\u001B[0;34m(self, freq, future)\u001B[0m\n\u001B[1;32m    170\u001B[0m         \u001B[0mflag\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mf\"{freq}_future_{future}\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    171\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mflag\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mH\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"c\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 172\u001B[0;31m             \u001B[0m_calendar\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_calendar\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfreq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfuture\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    173\u001B[0m             \u001B[0m_calendar_index\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_calendar\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m}\u001B[0m  \u001B[0;31m# for fast search\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    174\u001B[0m             \u001B[0mH\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"c\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mflag\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_calendar\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_calendar_index\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/data.py\u001B[0m in \u001B[0;36mload_calendar\u001B[0;34m(self, freq, future)\u001B[0m\n\u001B[1;32m    660\u001B[0m         \"\"\"\n\u001B[1;32m    661\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 662\u001B[0;31m             \u001B[0mbackend_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackend_obj\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfreq\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfreq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfuture\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfuture\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    663\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    664\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mfuture\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/storage/file_storage.py\u001B[0m in \u001B[0;36mdata\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    122\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    123\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mList\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mCalVT\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 124\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcheck\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    125\u001B[0m         \u001B[0;31m# If cache is enabled, then return cache directly\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    126\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menable_read_cache\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/storage/file_storage.py\u001B[0m in \u001B[0;36mcheck\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     70\u001B[0m         \u001B[0mValueError\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     71\u001B[0m         \"\"\"\n\u001B[0;32m---> 72\u001B[0;31m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muri\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     73\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{self.storage_name} not exists: {self.uri}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     74\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/storage/file_storage.py\u001B[0m in \u001B[0;36muri\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0muri\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mPath\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 120\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdpm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_data_uri\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_freq_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoinpath\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{self.storage_name}s\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfile_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    121\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyqlib-0.8.5.99-py3.9-macosx-10.9-x86_64.egg/qlib/data/storage/file_storage.py\u001B[0m in \u001B[0;36m_freq_file\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     99\u001B[0m                 \u001B[0mfreq\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mFreq\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_recent_freq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfreq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msupport_freq\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    100\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mfreq\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 101\u001B[0;31m                     \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"can't find a freq from {self.support_freq} that can resample to {self.freq}!\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    102\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_freq_file_cache\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfreq\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    103\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_freq_file_cache\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: can't find a freq from [] that can resample to day!"
     ]
    }
   ],
   "source": [
    "data_handler_config = {\n",
    "    \"start_time\": \"2020-12-11\",\n",
    "    \"end_time\": \"2021-06-10\",\n",
    "    \"instruments\": ['SH000923','SH600015','SH600019','SH600028','SH600030','SH600036','SH600048','SH600050','SH600104','SH600519'],\n",
    "}\n",
    "h = Alpha158(**data_handler_config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python397jvsc74a57bd027bef079abd80a919f70d115b698c3271af40f8d84f609e6e910dd2b9e5f2c56",
   "language": "python",
   "display_name": "Python 3.9.7 ('base')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "deb4792152b8b9767403eeef0a1b0f34b83d442136ccee9184cd7d1131f09aa0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}